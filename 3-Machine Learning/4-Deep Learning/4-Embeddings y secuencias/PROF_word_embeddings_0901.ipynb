{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "IZBRUaiBBEpa"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "YS3NA-i6nAFC"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7SN5USFEIIK3"
   },
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Aojnnc7sXrab"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/text/word_embeddings\">\n",
    "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
    "    View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
    "    Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/word_embeddings.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
    "    View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/text/word_embeddings.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Q6mJg1g3apaz"
   },
   "source": [
    "This tutorial contains an introduction to word embeddings. You will train your own word embeddings using a simple Keras model for a sentiment classification task, and then visualize them in the [Embedding Projector](http://projector.tensorflow.org) (shown in the image below). \n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding.jpg?raw=1\" alt=\"Screenshot of the embedding projector\" width=\"400\"/>\n",
    "\n",
    "## Representing text as numbers\n",
    "\n",
    "Machine learning models take vectors (arrays of numbers) as input. When working with text, the first thing you must do is come up with a strategy to convert strings to numbers (or to \"vectorize\" the text) before feeding it to the model. In this section, you will look at three strategies for doing so.\n",
    "\n",
    "### One-hot encodings\n",
    "\n",
    "As a first idea, you might \"one-hot\" encode each word in your vocabulary. Consider the sentence \"The cat sat on the mat\". The vocabulary (or unique words) in this sentence is (cat, mat, on, sat, the). To represent each word, you will create a zero vector with length equal to the vocabulary, then place a one in the index that corresponds to the word. This approach is shown in the following diagram.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/one-hot.png?raw=1\" alt=\"Diagram of one-hot encodings\" width=\"400\" />\n",
    "\n",
    "To create a vector that contains the encoding of the sentence, you could then concatenate the one-hot vectors for each word.\n",
    "\n",
    "**Key point: This approach is inefficient.** A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.\n",
    "\n",
    "### Encode each word with a unique number\n",
    "\n",
    "A second approach you might try is to encode each word using a **unique number**. Continuing the example above, you could assign 1 to \"cat\", 2 to \"mat\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector like [5, 1, 4, 3, 5, 2]. This appoach is efficient. Instead of a sparse vector, you now have a dense one (where all elements are full).\n",
    "\n",
    "There are two downsides to this approach, however:\n",
    "\n",
    "* The integer-encoding is arbitrary (it does not capture any relationship between words).\n",
    "\n",
    "* An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n",
    "\n",
    "### Word embeddings\n",
    "\n",
    "Word embeddings give us a way to use an efficient, dense representation in which similar words have a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a dense vector of floating point values (the length of the vector is a parameter you specify). Instead of specifying the values for the embedding manually, they are trainable parameters (weights learned by the model during training, in the same way a model learns weights for a dense layer). It is common to see word embeddings that are 8-dimensional (for small datasets), up to 1024-dimensions when working with large datasets. A higher dimensional embedding can capture fine-grained relationships between words, but takes more data to learn.\n",
    "\n",
    "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/images/embedding2.png?raw=1\" alt=\"Diagram of an embedding\" width=\"400\"/>\n",
    "\n",
    "Above is a diagram for a word embedding. Each word is represented as a 4-dimensional vector of floating point values. Another way to think of an embedding is as \"lookup table\". After these weights have been learned, you can encode each word by looking up the dense vector it corresponds to in the table."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SZUQErGewZxE"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "RutaI-Tpev3T"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "SBFctV8-JZOc"
   },
   "source": [
    "### Download the IMDb Dataset\n",
    "You will use the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/) through the tutorial. You will train a sentiment classifier model on this dataset and in the process learn embeddings from scratch. To read more about loading a dataset from scratch, see the [Loading text tutorial](../load_data/text.ipynb).  \n",
    "\n",
    "Download the dataset using Keras file utility and take a look at the directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aPO4_UmfF0KH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCuidado NO subir luego al GitHub\\nAcceder mejor a la URL y descargar directamente en carpeta Descargas\\n\\nurl = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\\n\\ndataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\\n                                  untar=True, cache_dir=\\'.\\',\\n                                  cache_subdir=\\'\\')\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Cuidado NO subir luego al GitHub\n",
    "Acceder mejor a la URL y descargar directamente en carpeta Descargas\n",
    "\n",
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
    "\n",
    "dataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n",
    "                                  untar=True, cache_dir='.',\n",
    "                                  cache_subdir='')\n",
    "'''\n",
    "#dataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')\n",
    "#os.listdir(dataset_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eY6yROZNKvbd"
   },
   "source": [
    "Take a look at the `train/` directory. It has `pos` and `neg` folders with movie reviews labelled as positive and negative respectively. You will use reviews from `pos` and `neg` folders to train a binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9-iOHJGN6SDu"
   },
   "outputs": [],
   "source": [
    "dataset_dir = \"C:\\\\Users\\\\alber\\\\Downloads\\\\aclImdb_v1\\\\aclImdb\\\\train\"\n",
    "#train_dir = os.path.join(dataset_dir, 'train')\n",
    "#os.listdir(train_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "9O59BdioK8jY"
   },
   "source": [
    "The `train` directory also has additional folders which should be removed before creating training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "1_Vfi9oWMSh-"
   },
   "outputs": [],
   "source": [
    "#remove_dir = os.path.join(train_dir, 'unsup')\n",
    "#shutil.rmtree(remove_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oFoJjiEyJz9u"
   },
   "source": [
    "Next, create a `tf.data.Dataset` using `tf.keras.preprocessing.text_dataset_from_directory`. You can read more about using this utility in this [text classification tutorial](https://www.tensorflow.org/tutorials/keras/text_classification). \n",
    "\n",
    "Use the `train` directory to create both train and validation datasets with a split of 20% for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ItYD3TLkCOP1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 75000 files belonging to 3 classes.\n",
      "Using 60000 files for training.\n",
      "Found 75000 files belonging to 3 classes.\n",
      "Using 15000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "seed = 123\n",
    "\n",
    "'''\n",
    "Busca en un directorio todas las carpetas. Cada carpeta es una etiqueta\n",
    "Y cada archivo una review. Podemos especificar si es para el subset\n",
    "de training o validation y cuanto dejamos para validacion.\n",
    "Esto crea un tf.data.Dataset\n",
    "ELIMINAR UNA CARPETA EN TRAIN, QUE SOBRA\n",
    "'''\n",
    "\n",
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='training',\n",
    "    seed=seed)\n",
    "\n",
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    dataset_dir,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.2,\n",
    "    subset='validation', # Esto y la semilla permiten que las muestras con train no se superpongan\n",
    "    seed=seed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eHa6cq0-Ym0g"
   },
   "source": [
    "Take a look at a few movie reviews and their labels `(1: positive, 0: negative)` from the train dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "aTCbSkvkYmTT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b\"Ask yourself where she got the gun? Remember what she was taught about the mark's mindset when the con is over? The gun had blanks and it was provided to her from the very beginning.<br /><br />When the patient comes back at the end she was SUPPOSED to see him drive away in the red convertible and lead her to the gang splitting up her 80 thousand.<br /><br />The patient was in on the con from the beginning.<br /><br />Mantegna does not die in the end - the gun had blanks.<br /><br />There - enough spoilers for you there? This is why people are giving it such high ratings. It's extremely original because of the hidden ending and how it cons MOST of the audience.\"\n",
      "2 b\"For some reason, people seem to have a problem differentiating this movie from Trail of the Pink Panther.<br /><br />At any rate, this work does nothing but serve to remind us how sad the world is without Peter Sellers in it.<br /><br />They brought back the same old favorites from Trail (Dreyfus, Cato, Litton, etc.), but they introduced a misdirected Pratt-fall humorist into a role which was designed to substitute for the missing Clouseau. <br /><br />Dreyfus devises a way to produce the perfect copy of Clouseau via a hard frame computer system which factors the variables and tosses out the name of the most inept idiot in the global law enforcement family. What we got was a poor guy who was obviously overwhelmed by the grand scale of what Blake Edwards proposed he should do, and boy does it show.<br /><br />Ted Wass was amiable as Sergeant Clifton Sleigh, but let's face it...he wasn't Clouseau in any way. I realize that Blake Edwards was losing his greatest cash cow, but to disrespect Sellers' memory like this was just sacrilege. Frankly, I'm glad they have remade the original. I hope it runs a long line of successful ventures for Steve Martin.<br /><br />This dreck rates a 2.0/10 from...<br /><br />the Fiend :.\"\n",
      "2 b\"Jimeoin is a nameless actor who finds himself as the eternal extra\\xc2\\x85 never to play a principle role in a movie. He finds himself caught up with a group of would-be stars all trying to gain a break but none of them are able to do so.<br /><br />I was ready for a good comedy, but was bitterly disappointed. Jimeoin is a great comedian but this smacked of 'try hard' and it just failed. There were a few moments where I laughed out aloud and I recognised several moments of clever humour, but it wasn't enough. I enjoyed spotting the good familiar Aussie actors and scenes around Melbourne. I think you should spend your money seeing something else...\"\n",
      "2 b'\"And I\\'ve had vould have gotten avay vith it, too, if it veren\\'t for you meddling Ritzes! Blah, blah!\" <br /><br />No, not really. Poor Bela was continuing his spiraling descent from the triumph of Dracula to working for Ed Wood. He actually has a half comedic, half heroic role in this movie, but mostly he spends the movie being a distraction from what could laughably be called a plot.<br /><br />One has to wonder if the Scooby-Doo cartoons were inspired by movies like this. YOu had everything you see in your average Scooby cartoon- secret passages, some guy in a costume, ulterior motives (which, unlike a Scooby cartoon, don\\'t actually make sense here.) <br /><br />Okay, the Ritz Brothers. They were a very popular vaudeville act back in the day, but no one remembers them much today. Watching this film, you can see why. They didn\\'t have the comic timing, distinct personalities or perfect slapstick of the Three Stooges or Marx Brothers. They were pretty much interchangeable, with Jimmy and Al mugging while Harry got most of the dialog.<br /><br />There is a bit of interesting Hollywood history that the Ritzes staged a \"walkout\" on this film, to protest the quality of the script. 20th Century Fox should have let them walk and reworked the script. Instead, they finished the movie, and Fox kicked them to the curb.<br /><br />I don\\'t understand the plot. I guess that Atwill was supposed to be the villain, but really the guy who was pretending to be SEC agent was the actual killer, but it was never clear why he was killing people or why he would walk into the trap that Atwill and Lugosi had set for him. The ending makes absolutely no sense.<br /><br />You almost get the impression that there were a lot of b-listers (The Ritzes, Lugosi, Atwill, Patsy Kelly) who were insisting that THEY get more screen time than the others. Other characters, like the \"seaman\" who is found in the closet, are introduced and no explanation is given as to what they were doing there.'\n",
      "1 b'I think if you were to ask most JW\\'s whether they expect a miracle cure because of their faith, you will find they do not. I know I do not. What you will find instead is that they believe the promises Christ made of a resurrection. So, even even if the worst were to happen and we die while holding onto our integrity, Jehovah can, and will correct this.<br /><br />It really gets down to a simple question: is God real to you or is this all just make believe? If he is real, and you trust him, you will follow his directions no matter what the short term outcome may be.<br /><br />I had a heart attack about a year and a half ago. One in my family was horrified when she saw the words \"NO BLOOD\" written in large letters over my chart. I reasoned with her that if I were in a position that only a blood transfusion would save my life, would that be a good time to anger the only one could return me to life when the time came? She didn\\'t get it -- God just isn\\'t real enough to her. Too bad. I wish she could have the comfort a strong faith gives.'\n"
     ]
    }
   ],
   "source": [
    "for text_batch, label_batch in train_ds.take(1): # Cogemos el primer batch\n",
    "  for i in range(5): # 1025 da error xq no hay mas en este batch\n",
    "    print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "FHV2pchDhzDn"
   },
   "source": [
    "### Configure the dataset for performance\n",
    "\n",
    "These are two important methods you should use when loading data to make sure that I/O does not become blocking.\n",
    "\n",
    "`.cache()` keeps data in memory after it's loaded off disk. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache, which is more efficient to read than many small files.\n",
    "\n",
    "`.prefetch()` overlaps data preprocessing and model execution while training. \n",
    "\n",
    "You can learn more about both methods, as well as how to cache data to disk in the [data performance guide](https://www.tensorflow.org/guide/data_performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Oz6k1IW7h1TO"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "eqBazMiVQkj1"
   },
   "source": [
    "## Using the Embedding layer\n",
    "\n",
    "Keras makes it easy to use word embeddings. Take a look at the [Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer.\n",
    "\n",
    "The Embedding layer can be understood as a lookup table that maps from integer indices (which stand for specific words) to dense vectors (their embeddings). The dimensionality (or width) of the embedding is a parameter you can experiment with to see what works well for your problem, much in the same way you would experiment with the number of neurons in a Dense layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-OjxLVrMvWUE"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Embedd a 1,000 word vocabulary into 5 dimensions.\n",
    "Razonar cantidad del vocabulario y embeddings con el cap de \"AI and ML for coders\"\n",
    "https://learning.oreilly.com/library/view/ai-and-machine/9781492078180/ch06.html\n",
    "\n",
    "La capa de embedding funciona como un diccionario al que asigna cada palabra con su vector de embedding\n",
    "Estos vectores se entrenan con backpropagation\n",
    "\n",
    "Basicamente embeddings funciona bien como la raiz cuarta del vocabulario\n",
    "Y el vocabulario esta bien tener palabras con al menos 20 como frecuencia\n",
    "de aparición\n",
    "'''\n",
    "embedding_layer = tf.keras.layers.Embedding(1000, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "2dKKV1L2Rk7e"
   },
   "source": [
    "When you create an Embedding layer, the weights for the embedding are randomly initialized (just like any other layer). During training, they are gradually adjusted via backpropagation. Once trained, the learned word embeddings will roughly encode similarities between words (as they were learned for the specific problem your model is trained on).\n",
    "\n",
    "If you pass an integer to an embedding layer, the result replaces each integer with the vector from the embedding table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([  1,   2, 999])>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([1, 2, 999])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0YUjPgP7w0PO"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03042907, -0.00772778,  0.03136179,  0.02621094,  0.0139096 ],\n",
       "       [-0.00522513, -0.01752614, -0.02593738,  0.00014941, -0.03875433],\n",
       "       [ 0.01231589,  0.0234954 , -0.02520455,  0.00919622,  0.00337904]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "No puede hacer mas de la palabra 1000\n",
    "Se inicializa el embedding con pesos aleatorios\n",
    "1000 daria error ya que no hay tanto vocab\n",
    "5 valores para cada \"palabra\" numérica\n",
    "'''\n",
    "result = embedding_layer(tf.constant([1, 2, 999]))\n",
    "result.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'embedding/embeddings:0' shape=(1000, 5) dtype=float32, numpy=\n",
       "array([[ 0.02576594, -0.0064076 ,  0.02969674, -0.0350096 ,  0.01755558],\n",
       "       [ 0.03042907, -0.00772778,  0.03136179,  0.02621094,  0.0139096 ],\n",
       "       [-0.00522513, -0.01752614, -0.02593738,  0.00014941, -0.03875433],\n",
       "       ...,\n",
       "       [ 0.00392149,  0.03900056, -0.03911978,  0.03802966, -0.00448322],\n",
       "       [ 0.00909553, -0.02326909,  0.0353284 ,  0.04282594, -0.0194979 ],\n",
       "       [ 0.01231589,  0.0234954 , -0.02520455,  0.00919622,  0.00337904]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embedding_layer.embeddings.shape)\n",
    "embedding_layer.embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O4PC4QzsxTGx"
   },
   "source": [
    "For text or sequence problems, the Embedding layer takes a 2D tensor of integers, of shape `(samples, sequence_length)`, where each entry is a sequence of integers. It can embed sequences of variable lengths. You could feed into the embedding layer above batches with shapes `(32, 10)` (batch of 32 sequences of length 10) or `(64, 15)` (batch of 64 sequences of length 15).\n",
    "\n",
    "The returned tensor has one more axis than the input, the embedding vectors are aligned along the new last axis. Pass it a `(2, 3)` input batch and the output is `(2, 3, N)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vwSYepRjyRGy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[ 0.01236947, -0.01822526,  0.00336619,  0.01731164,\n",
       "         -0.02586402],\n",
       "        [ 0.00603884, -0.03774611, -0.04411054,  0.03116176,\n",
       "          0.02258295],\n",
       "        [-0.03226171, -0.0435437 ,  0.03335794, -0.0366528 ,\n",
       "          0.02722783]],\n",
       "\n",
       "       [[-0.0183346 , -0.01161654,  0.00477195,  0.03667328,\n",
       "          0.01383214],\n",
       "        [-0.0063717 ,  0.04064864, -0.00227129,  0.00501939,\n",
       "         -0.00971254],\n",
       "        [ 0.04645033, -0.046709  ,  0.02918023, -0.00826325,\n",
       "         -0.04302992]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = embedding_layer(tf.constant([[0, 1, 2],\n",
    "                                      [3, 4, 5]]))\n",
    "print(result.shape)\n",
    "result.numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "WGQp2N92yOyB"
   },
   "source": [
    "When given a batch of sequences as input, an embedding layer returns a 3D floating point tensor, of shape `(samples, sequence_length, embedding_dimensionality)`. To convert from this sequence of variable length to a fixed representation there are a variety of standard approaches. You could use an RNN, Attention, or pooling layer before passing it to a Dense layer. This tutorial uses pooling because it's the simplest. The [Text Classification with an RNN](text_classification_rnn.ipynb) tutorial is a good next step."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "aGicgV5qT0wh"
   },
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "N6NZSqIIoU0Y"
   },
   "source": [
    "Next, define the dataset preprocessing steps required for your sentiment classification model. Initialize a TextVectorization layer with the desired parameters to vectorize movie reviews. You can learn more about using this layer in the [Text Classification](https://www.tensorflow.org/tutorials/keras/text_classification) tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2MlsXzo-ZlfK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\alber\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "# Create a custom standardization function to strip HTML break tags '<br />'.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  stripped_html = tf.strings.regex_replace(lowercase, '<br />', ' ')\n",
    "  return tf.strings.regex_replace(stripped_html,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Vocabulary size and number of words in a sequence.\n",
    "vocab_size = 10000 # Ojo, al final es 10k y no 1k\n",
    "sequence_length = 100\n",
    "\n",
    "# Use the text vectorization layer to normalize, split, and map strings to\n",
    "# integers. Note that the layer uses the custom standardization defined above.\n",
    "# Set maximum_sequence length as all samples are not of the same length.\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "# Make a text-only dataset (no labels) and call adapt to build the vocabulary.\n",
    "# Nos quitamos los labels de esta manera, que van en conjunto con features en train_ds\n",
    "text_ds = train_ds.map(lambda x, y: x)\n",
    "vectorize_layer.adapt(text_ds)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9_wLIiWO8Z"
   },
   "source": [
    "## Create a classification model\n",
    "\n",
    "Use the [Keras Sequential API](https://www.tensorflow.org/guide/keras/sequential_model) to define the sentiment classification model. In this case it is a \"Continuous bag of words\" style model.\n",
    "* The [`TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing/TextVectorization) layer transforms strings into vocabulary indices. You have already initialized `vectorize_layer` as a TextVectorization layer and built it's vocabulary by calling `adapt` on `text_ds`. Now vectorize_layer can be used as the first layer of your end-to-end classification model, feeding transformed strings into the Embedding layer.\n",
    "* The [`Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) layer takes the integer-encoded vocabulary and looks up the embedding vector for each word-index. These vectors are learned as the model trains. The vectors add a dimension to the output array. The resulting dimensions are: `(batch, sequence, embedding)`.\n",
    "\n",
    "* The [`GlobalAveragePooling1D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling1D) layer returns a fixed-length output vector for each example by averaging over the sequence dimension. This allows the model to handle input of variable length, in the simplest way possible.\n",
    "\n",
    "* The fixed-length output vector is piped through a fully-connected ([`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)) layer with 16 hidden units.\n",
    "\n",
    "* The last layer is densely connected with a single output node. \n",
    "\n",
    "Caution: This model doesn't use masking, so the zero-padding is used as part of the input and hence the padding length may affect the output.  To fix this, see the [masking and padding guide](https://www.tensorflow.org/guide/keras/masking_and_padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "pHLcFtn5Wsqj"
   },
   "outputs": [],
   "source": [
    "embedding_dim=16\n",
    "\n",
    "'''\n",
    "GlobalAveragePooling1D\n",
    "Cada palabra tiene asociado un embedding. El ouput es la media de cada\n",
    "coordenada del embedding, por tanto, si hay 16 embeddings, hará un \n",
    "flatten a 16, siendo cada valor la media de la coordenada de ese \n",
    "embedding para todas las palabras de la review\n",
    "'''\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer, # 100 [1, 3, 4, 4, 90, ...]\n",
    "  Embedding(vocab_size, embedding_dim, name=\"embedding\"), # 10.000 x 16 --> [[], [], [] ...] 100x16\n",
    "  GlobalAveragePooling1D(), # [] 100\n",
    "  Dense(16, activation='relu'), # \n",
    "  Dense(1, activation='sigmoid') # originalmente no tiene activacion\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JjLNgKO7W2fe"
   },
   "source": [
    "## Compile and train the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jpX9etB6IOQd"
   },
   "source": [
    "You will use [TensorBoard](https://www.tensorflow.org/tensorboard) to visualize metrics including loss and accuracy. Create a `tf.keras.callbacks.TensorBoard`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "W4Hg3IHFt4Px"
   },
   "outputs": [],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "7OrKAKAKIbuH"
   },
   "source": [
    "Compile and train the model using the `Adam` optimizer and `BinaryCrossentropy` loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "lCUgdP69Wzix"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              # binary_crossentropy\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "5mQehiQyv8rP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "20/20 [==============================] - 38s 2s/step - loss: 0.6926 - accuracy: 0.5495 - val_loss: 0.6901 - val_accuracy: 0.6618\n",
      "Epoch 2/15\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.6887 - accuracy: 0.6917 - val_loss: 0.6839 - val_accuracy: 0.7052\n",
      "Epoch 3/15\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.6814 - accuracy: 0.7035 - val_loss: 0.6732 - val_accuracy: 0.7128\n",
      "Epoch 4/15\n",
      "20/20 [==============================] - 3s 137ms/step - loss: 0.6691 - accuracy: 0.7172 - val_loss: 0.6567 - val_accuracy: 0.7264\n",
      "Epoch 5/15\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.6496 - accuracy: 0.7363 - val_loss: 0.6334 - val_accuracy: 0.7406\n",
      "Epoch 6/15\n",
      "20/20 [==============================] - 3s 133ms/step - loss: 0.6227 - accuracy: 0.7552 - val_loss: 0.6050 - val_accuracy: 0.7532\n",
      "Epoch 7/15\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.5900 - accuracy: 0.7773 - val_loss: 0.5737 - val_accuracy: 0.7676\n",
      "Epoch 8/15\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.5538 - accuracy: 0.7977 - val_loss: 0.5420 - val_accuracy: 0.7816\n",
      "Epoch 9/15\n",
      "20/20 [==============================] - 3s 135ms/step - loss: 0.5169 - accuracy: 0.8106 - val_loss: 0.5123 - val_accuracy: 0.7924\n",
      "Epoch 10/15\n",
      "20/20 [==============================] - 3s 138ms/step - loss: 0.4817 - accuracy: 0.8263 - val_loss: 0.4862 - val_accuracy: 0.8028\n",
      "Epoch 11/15\n",
      "20/20 [==============================] - 3s 136ms/step - loss: 0.4499 - accuracy: 0.8342 - val_loss: 0.4642 - val_accuracy: 0.8074\n",
      "Epoch 12/15\n",
      "20/20 [==============================] - 3s 141ms/step - loss: 0.4219 - accuracy: 0.8413 - val_loss: 0.4459 - val_accuracy: 0.8142\n",
      "Epoch 13/15\n",
      "20/20 [==============================] - 3s 134ms/step - loss: 0.3975 - accuracy: 0.8496 - val_loss: 0.4310 - val_accuracy: 0.8184\n",
      "Epoch 14/15\n",
      "20/20 [==============================] - 3s 137ms/step - loss: 0.3762 - accuracy: 0.8569 - val_loss: 0.4188 - val_accuracy: 0.8204\n",
      "Epoch 15/15\n",
      "20/20 [==============================] - 3s 138ms/step - loss: 0.3574 - accuracy: 0.8628 - val_loss: 0.4089 - val_accuracy: 0.8236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1cf944dc5e0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=15,\n",
    "    callbacks=[tensorboard_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "1wYnVedSPfmX"
   },
   "source": [
    "With this approach the model reaches a validation accuracy of around 84% (note that the model is overfitting since training accuracy is higher).\n",
    "\n",
    "Note: Your results may be a bit different, depending on how weights were randomly initialized before training the embedding layer. \n",
    "\n",
    "You can look into the model summary to learn more about each layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "mDCgjWyq_0dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization (TextVect (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 100, 16)           160000    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 160,289\n",
      "Trainable params: 160,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "hiQbOJZ2WBFY"
   },
   "source": [
    "Visualize the model metrics in TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_Uanp2YH8RzU"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1764), started 1 day, 22:35:36 ago. (Use '!kill 1764' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1340c7e3c9b32495\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1340c7e3c9b32495\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#docs_infra: no_execute\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QvURkGVpXDOy"
   },
   "source": [
    "![embeddings_classifier_accuracy.png](images/embeddings_classifier_accuracy.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KCoA6qwqP836"
   },
   "source": [
    "## Retrieve the trained word embeddings and save them to disk\n",
    "\n",
    "Next, retrieve the word embeddings learned during training. The embeddings are weights of the Embedding layer in the model. The weights matrix is of shape `(vocab_size, embedding_dimension)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Zp5rv01WG2YA"
   },
   "source": [
    "Obtain the weights from the model using `get_layer()` and `get_weights()`. The `get_vocabulary()` function provides the vocabulary to build a metadata file with one token per line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_Uamp1YH8RzU"
   },
   "outputs": [],
   "source": [
    "weights = model.get_layer('embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "J8MiCA77X8B8"
   },
   "source": [
    "Write the weights to disk. To use the [Embedding Projector](http://projector.tensorflow.org), you will upload two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "VLIahl9s53XT"
   },
   "outputs": [],
   "source": [
    "out_v = io.open('vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "JQyMZWyxYjMr"
   },
   "source": [
    "If you are running this tutorial in [Colaboratory](https://colab.research.google.com), you can use the following snippet to download these files to your local machine (or use the file browser, *View -> Table of contents -> File browser*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lUsjQOKMIV2z"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  from google.colab import files\n",
    "  files.download('vectors.tsv')\n",
    "  files.download('metadata.tsv')\n",
    "except Exception:\n",
    "  pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXLfFA54Yz-o"
   },
   "source": [
    "## Visualize the embeddings\n",
    "\n",
    "To visualize the embeddings, upload them to the embedding projector.\n",
    "\n",
    "Open the [Embedding Projector](http://projector.tensorflow.org/) (this can also run in a local TensorBoard instance).\n",
    "\n",
    "* Click on \"Load data\".\n",
    "\n",
    "* Upload the two files you created above: `vecs.tsv` and `meta.tsv`.\n",
    "\n",
    "The embeddings you have trained will now be displayed. You can search for words to find their closest neighbors. For example, try searching for \"beautiful\". You may see neighbors like \"wonderful\". \n",
    "\n",
    "Note: Experimentally, you may be able to produce more interpretable embeddings by using a simpler model. Try deleting the `Dense(16)` layer, retraining the model, and visualizing the embeddings again.\n",
    "\n",
    "Note: Typically, a much larger dataset is needed to train more interpretable word embeddings. This tutorial uses a small IMDb dataset for the purpose of demonstration.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wvKiEHjramNh"
   },
   "source": [
    "## Next Steps"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "BSgAZpwF5xF_"
   },
   "source": [
    "This tutorial has shown you how to train and visualize word embeddings from scratch on a small dataset.\n",
    "\n",
    "* To train word embeddings using Word2Vec algorithm, try the [Word2Vec](https://www.tensorflow.org/tutorials/text/word2vec) tutorial. \n",
    "\n",
    "* To learn more about advanced text processing, read the [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "word_embeddings.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7600a12950a547366bb7a6732117e300ffd26224351912980486e1126c5d0f9a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
