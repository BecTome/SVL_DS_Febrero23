{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis del sentimiento en IMDB\n",
    "\n",
    "Los datos se dividen en partes iguales: 25.000 reseñas para el entrenamiento y 25.000 para probar el clasificador. Además, cada conjunto tiene 12,5k críticas positivas y 12,5k negativas.\n",
    "\n",
    "IMDb permite a los usuarios puntuar las películas en una escala del 1 al 10. Para etiquetar estas reseñas, el encargado de los datos etiquetó todo lo que tuviera ≤ 4 estrellas como negativo y todo lo que tuviera ≥ 7 estrellas como positivo. Las reseñas con 5 o 6 estrellas se omitieron.\n",
    "\n",
    "**Importar las bibliotecas necesarias**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_train = []\n",
    "for line in open(os.getcwd() + '/data/imbd_train.txt', 'r', encoding='latin1'):\n",
    "    \n",
    "    reviews_train.append(line.strip())\n",
    "    \n",
    "reviews_test = []\n",
    "for line in open(os.getcwd() + '/data/imbd_test.txt', 'r', encoding='latin1'):\n",
    "    \n",
    "    reviews_test.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "####################\n",
      "Homelessness (or Houselessness as George Carlin stated) has been an issue for years but never a plan to help those on the street that were once considered human who did everything from going to school, work, or vote for the matter. Most people think of the homeless as just a lost cause while worrying about things such as racism, the war on Iraq, pressuring kids to succeed, technology, the elections, inflation, or worrying if they'll be next to end up on the streets.<br /><br />But what if you were given a bet to live on the streets for a month without the luxuries you once had from a home, the entertainment sets, a bathroom, pictures on the wall, a computer, and everything you once treasure to see what it's like to be homeless? That is Goddard Bolt's lesson.<br /><br />Mel Brooks (who directs) who stars as Bolt plays a rich man who has everything in the world until deciding to make a bet with a sissy rival (Jeffery Tambor) to see if he can live in the streets for thirty days without the luxuries; if Bolt succeeds, he can do what he wants with a future project of making more buildings. The bet's on where Bolt is thrown on the street with a bracelet on his leg to monitor his every move where he can't step off the sidewalk. He's given the nickname Pepto by a vagrant after it's written on his forehead where Bolt meets other characters including a woman by the name of Molly (Lesley Ann Warren) an ex-dancer who got divorce before losing her home, and her pals Sailor (Howard Morris) and Fumes (Teddy Wilson) who are already used to the streets. They're survivors. Bolt isn't. He's not used to reaching mutual agreements like he once did when being rich where it's fight or flight, kill or be killed.<br /><br />While the love connection between Molly and Bolt wasn't necessary to plot, I found \"Life Stinks\" to be one of Mel Brooks' observant films where prior to being a comedy, it shows a tender side compared to his slapstick work such as Blazing Saddles, Young Frankenstein, or Spaceballs for the matter, to show what it's like having something valuable before losing it the next day or on the other hand making a stupid bet like all rich people do when they don't know what to do with their money. Maybe they should give it to the homeless instead of using it like Monopoly money.<br /><br />Or maybe this film will inspire you to help others.\n",
      "####################\n",
      "Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print('####################')\n",
    "    print(reviews_train[i])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ver uno de los elementos de la lista**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"This isn't the comedic Robin Williams, nor is it the quirky/insane Robin Williams of recent thriller fame. This is a hybrid of the classic drama without over-dramatization, mixed with Robin's new love of the thriller. But this isn't a thriller, per se. This is more a mystery/suspense vehicle through which Williams attempts to locate a sick boy and his keeper.<br /><br />Also starring Sandra Oh and Rory Culkin, this Suspense Drama plays pretty much like a news report, until William's character gets close to achieving his goal.<br /><br />I must say that I was highly entertained, though this movie fails to teach, guide, inspect, or amuse. It felt more like I was watching a guy (Williams), as he was actually performing the actions, from a third person perspective. In other words, it felt real, and I was able to subscribe to the premise of the story.<br /><br />All in all, it's worth a watch, though it's definitely not Friday/Saturday night fare.<br /><br />It rates a 7.7/10 from...<br /><br />the Fiend :.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(reviews_train))\n",
    "print(len(reviews_test))\n",
    "reviews_train[5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El texto en bruto es bastante desordenado para hacer una revisión de esta manera, por lo que antes de que podamos hacer cualquier análisis tenemos que limpiar las cosas\n",
    "\n",
    "\n",
    "**Utilizar expresiones regulares para eliminar los caracteres no textuales y las etiquetas html**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Remover todos los signos de puntuación, exclamaciones...\n",
    "# Tb pasamos a minuscula y nos cargamos etiquetas HTML\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "NO_SPACE = \"\"\n",
    "SPACE = \" \"\n",
    "\n",
    "def preprocess_reviews(reviews):\n",
    "    \n",
    "    # Para todas las reviews en minuscula, sustituye algunas cosas por espacio y otras por vacio.\n",
    "    reviews = [REPLACE_NO_SPACE.sub(NO_SPACE, line.lower()) for line in reviews]\n",
    "    reviews = [REPLACE_WITH_SPACE.sub(SPACE, line) for line in reviews]\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# Reviews tras aplicar la limpieza\n",
    "reviews_train_clean = preprocess_reviews(reviews_train)\n",
    "reviews_test_clean = preprocess_reviews(reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"this isn't the comedic robin williams nor is it the quirky insane robin williams of recent thriller fame this is a hybrid of the classic drama without over dramatization mixed with robin's new love of the thriller but this isn't a thriller per se this is more a mystery suspense vehicle through which williams attempts to locate a sick boy and his keeper also starring sandra oh and rory culkin this suspense drama plays pretty much like a news report until william's character gets close to achieving his goal i must say that i was highly entertained though this movie fails to teach guide inspect or amuse it felt more like i was watching a guy williams as he was actually performing the actions from a third person perspective in other words it felt real and i was able to subscribe to the premise of the story all in all it's worth a watch though it's definitely not friday saturday night fare it rates a   from the fiend \""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_train_clean[5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "Para que estos datos tengan sentido para nuestro algoritmo de aprendizaje automático, tendremos que convertir cada opinión en una representación numérica, lo que llamamos vectorización.\n",
    "\n",
    "La forma más sencilla de hacerlo es crear una matriz muy grande con una columna para cada palabra única del corpus (en nuestro caso, el corpus son las 50.000 reseñas). A continuación, transformamos cada opinión en una fila que contiene 0 y 1, donde 1 significa que la palabra del corpus correspondiente a esa columna aparece en esa opinión. Dicho esto, cada fila de la matriz será muy escasa (mayoritariamente ceros). Este proceso también se conoce como codificación en caliente. Utilice el método *CountVectorizer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "     'This is the first document.',\n",
    "     'This document is the second document.',\n",
    "     'And this is the third one.',\n",
    "     'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si aparece una palabra en una review, le pone un 1. Da igual que aparezca 100 veces, no cuenta. Xq binary=True\n",
    "# Solo pone 1s cuando detecta una palabra en una review\n",
    "baseline_vectorizer = CountVectorizer(binary=True)\n",
    "baseline_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "# Reviews en formato vector de palabras. El mismo vectorizador a test, tiene que mantener la estructura\n",
    "X_baseline = baseline_vectorizer.transform(reviews_train_clean)\n",
    "X_test_baseline = baseline_vectorizer.transform(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x87063 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3410713 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bromwell': 9819,\n",
       " 'high': 35211,\n",
       " 'is': 39472,\n",
       " 'cartoon': 11686,\n",
       " 'comedy': 14754,\n",
       " 'it': 39642,\n",
       " 'ran': 61772,\n",
       " 'at': 4537,\n",
       " 'the': 76725,\n",
       " 'same': 66138,\n",
       " 'time': 77626,\n",
       " 'as': 4211,\n",
       " 'some': 71188,\n",
       " 'other': 54861,\n",
       " 'programs': 60156,\n",
       " 'about': 284,\n",
       " 'school': 67025,\n",
       " 'life': 44297,\n",
       " 'such': 74177,\n",
       " 'teachers': 75997,\n",
       " 'my': 51490,\n",
       " 'years': 86260,\n",
       " 'in': 37733,\n",
       " 'teaching': 76000,\n",
       " 'profession': 60088,\n",
       " 'lead': 43605,\n",
       " 'me': 47976,\n",
       " 'to': 77922,\n",
       " 'believe': 6894,\n",
       " 'that': 76671,\n",
       " 'satire': 66462,\n",
       " 'much': 51018,\n",
       " 'closer': 14125,\n",
       " 'reality': 62242,\n",
       " 'than': 76643,\n",
       " 'scramble': 67253,\n",
       " 'survive': 74812,\n",
       " 'financially': 27904,\n",
       " 'insightful': 38615,\n",
       " 'students': 73768,\n",
       " 'who': 84627,\n",
       " 'can': 11132,\n",
       " 'see': 67695,\n",
       " 'right': 64476,\n",
       " 'through': 77369,\n",
       " 'their': 76762,\n",
       " 'pathetic': 56362,\n",
       " 'pomp': 58787,\n",
       " 'pettiness': 57341,\n",
       " 'of': 53843,\n",
       " 'whole': 84639,\n",
       " 'situation': 69959,\n",
       " 'all': 2038,\n",
       " 'remind': 63323,\n",
       " 'schools': 67049,\n",
       " 'knew': 42226,\n",
       " 'and': 2762,\n",
       " 'when': 84450,\n",
       " 'saw': 66588,\n",
       " 'episode': 24853,\n",
       " 'which': 84476,\n",
       " 'student': 73764,\n",
       " 'repeatedly': 63494,\n",
       " 'tried': 79132,\n",
       " 'burn': 10435,\n",
       " 'down': 22050,\n",
       " 'immediately': 37436,\n",
       " 'recalled': 62427,\n",
       " 'classic': 13840,\n",
       " 'line': 44572,\n",
       " 'inspector': 38661,\n",
       " 'here': 34881,\n",
       " 'sack': 65826,\n",
       " 'one': 54223,\n",
       " 'your': 86541,\n",
       " 'welcome': 84135,\n",
       " 'expect': 25952,\n",
       " 'many': 46877,\n",
       " 'adults': 1131,\n",
       " 'age': 1445,\n",
       " 'think': 77056,\n",
       " 'far': 26802,\n",
       " 'fetched': 27446,\n",
       " 'what': 84379,\n",
       " 'pity': 58038,\n",
       " 'isn': 39565,\n",
       " 'homelessness': 35859,\n",
       " 'or': 54556,\n",
       " 'houselessness': 36368,\n",
       " 'george': 30901,\n",
       " 'carlin': 11519,\n",
       " 'stated': 72756,\n",
       " 'has': 34120,\n",
       " 'been': 6660,\n",
       " 'an': 2670,\n",
       " 'issue': 39614,\n",
       " 'for': 28846,\n",
       " 'but': 10574,\n",
       " 'never': 52363,\n",
       " 'plan': 58141,\n",
       " 'help': 34743,\n",
       " 'those': 77231,\n",
       " 'on': 54196,\n",
       " 'street': 73546,\n",
       " 'were': 84257,\n",
       " 'once': 54207,\n",
       " 'considered': 15688,\n",
       " 'human': 36593,\n",
       " 'did': 20215,\n",
       " 'everything': 25523,\n",
       " 'from': 29698,\n",
       " 'going': 31740,\n",
       " 'work': 85510,\n",
       " 'vote': 83129,\n",
       " 'matter': 47565,\n",
       " 'most': 50516,\n",
       " 'people': 56856,\n",
       " 'homeless': 35858,\n",
       " 'just': 41023,\n",
       " 'lost': 45327,\n",
       " 'cause': 12043,\n",
       " 'while': 84492,\n",
       " 'worrying': 85625,\n",
       " 'things': 77032,\n",
       " 'racism': 61457,\n",
       " 'war': 83515,\n",
       " 'iraq': 39359,\n",
       " 'pressuring': 59700,\n",
       " 'kids': 41828,\n",
       " 'succeed': 74135,\n",
       " 'technology': 76091,\n",
       " 'elections': 23680,\n",
       " 'inflation': 38309,\n",
       " 'if': 37183,\n",
       " 'they': 76956,\n",
       " 'll': 44861,\n",
       " 'be': 6399,\n",
       " 'next': 52455,\n",
       " 'end': 24262,\n",
       " 'up': 81499,\n",
       " 'streets': 73557,\n",
       " 'you': 86483,\n",
       " 'given': 31356,\n",
       " 'bet': 7290,\n",
       " 'live': 44794,\n",
       " 'month': 50173,\n",
       " 'without': 85171,\n",
       " 'luxuries': 45842,\n",
       " 'had': 33377,\n",
       " 'home': 35844,\n",
       " 'entertainment': 24667,\n",
       " 'sets': 68267,\n",
       " 'bathroom': 6268,\n",
       " 'pictures': 57717,\n",
       " 'wall': 83402,\n",
       " 'computer': 15211,\n",
       " 'treasure': 78953,\n",
       " 'like': 44416,\n",
       " 'goddard': 31665,\n",
       " 'bolt': 8627,\n",
       " 'lesson': 44051,\n",
       " 'mel': 48272,\n",
       " 'brooks': 9853,\n",
       " 'directs': 20600,\n",
       " 'stars': 72697,\n",
       " 'plays': 58278,\n",
       " 'rich': 64333,\n",
       " 'man': 46595,\n",
       " 'world': 85574,\n",
       " 'until': 81403,\n",
       " 'deciding': 18739,\n",
       " 'make': 46398,\n",
       " 'with': 85140,\n",
       " 'sissy': 69915,\n",
       " 'rival': 64672,\n",
       " 'jeffery': 40268,\n",
       " 'tambor': 75653,\n",
       " 'he': 34360,\n",
       " 'thirty': 77120,\n",
       " 'days': 18432,\n",
       " 'succeeds': 74139,\n",
       " 'do': 21408,\n",
       " 'wants': 83513,\n",
       " 'future': 30031,\n",
       " 'project': 60181,\n",
       " 'making': 46429,\n",
       " 'more': 50319,\n",
       " 'buildings': 10203,\n",
       " 'where': 84457,\n",
       " 'thrown': 77400,\n",
       " 'bracelet': 9221,\n",
       " 'his': 35480,\n",
       " 'leg': 43791,\n",
       " 'monitor': 50057,\n",
       " 'every': 25510,\n",
       " 'move': 50682,\n",
       " 'step': 72949,\n",
       " 'off': 53853,\n",
       " 'sidewalk': 69518,\n",
       " 'nickname': 52543,\n",
       " 'pepto': 56913,\n",
       " 'by': 10691,\n",
       " 'vagrant': 81877,\n",
       " 'after': 1360,\n",
       " 'written': 85840,\n",
       " 'forehead': 28906,\n",
       " 'meets': 48197,\n",
       " 'characters': 12612,\n",
       " 'including': 37879,\n",
       " 'woman': 85298,\n",
       " 'name': 51704,\n",
       " 'molly': 49931,\n",
       " 'lesley': 44036,\n",
       " 'ann': 3025,\n",
       " 'warren': 83612,\n",
       " 'ex': 25627,\n",
       " 'dancer': 18094,\n",
       " 'got': 32084,\n",
       " 'divorce': 21349,\n",
       " 'before': 6701,\n",
       " 'losing': 45322,\n",
       " 'her': 34853,\n",
       " 'pals': 55743,\n",
       " 'sailor': 65978,\n",
       " 'howard': 36404,\n",
       " 'morris': 50429,\n",
       " 'fumes': 29866,\n",
       " 'teddy': 76097,\n",
       " 'wilson': 84909,\n",
       " 'are': 3794,\n",
       " 'already': 2256,\n",
       " 'used': 81711,\n",
       " 're': 62137,\n",
       " 'survivors': 74826,\n",
       " 'not': 53085,\n",
       " 'reaching': 62149,\n",
       " 'mutual': 51469,\n",
       " 'agreements': 1561,\n",
       " 'being': 6826,\n",
       " 'fight': 27612,\n",
       " 'flight': 28404,\n",
       " 'kill': 41873,\n",
       " 'killed': 41878,\n",
       " 'love': 45429,\n",
       " 'connection': 15579,\n",
       " 'between': 7365,\n",
       " 'wasn': 83673,\n",
       " 'necessary': 52066,\n",
       " 'plot': 58391,\n",
       " 'found': 29165,\n",
       " 'stinks': 73159,\n",
       " 'observant': 53634,\n",
       " 'films': 27798,\n",
       " 'prior': 59895,\n",
       " 'shows': 69264,\n",
       " 'tender': 76323,\n",
       " 'side': 69480,\n",
       " 'compared': 15003,\n",
       " 'slapstick': 70221,\n",
       " 'blazing': 8028,\n",
       " 'saddles': 65866,\n",
       " 'young': 86514,\n",
       " 'frankenstein': 29331,\n",
       " 'spaceballs': 71597,\n",
       " 'show': 69221,\n",
       " 'having': 34273,\n",
       " 'something': 71222,\n",
       " 'valuable': 81966,\n",
       " 'day': 18410,\n",
       " 'hand': 33699,\n",
       " 'stupid': 73847,\n",
       " 'don': 21708,\n",
       " 'know': 42286,\n",
       " 'money': 50010,\n",
       " 'maybe': 47676,\n",
       " 'should': 69189,\n",
       " 'give': 31353,\n",
       " 'instead': 38702,\n",
       " 'using': 81743,\n",
       " 'monopoly': 50093,\n",
       " 'this': 77124,\n",
       " 'film': 27688,\n",
       " 'will': 84854,\n",
       " 'inspire': 38671,\n",
       " 'others': 54871,\n",
       " 'brilliant': 9653,\n",
       " 'over': 55140,\n",
       " 'acting': 701,\n",
       " 'best': 7257,\n",
       " 'dramatic': 22219,\n",
       " 'hobo': 35654,\n",
       " 'lady': 42885,\n",
       " 'have': 34253,\n",
       " 'ever': 25473,\n",
       " 'seen': 67745,\n",
       " 'scenes': 66851,\n",
       " 'clothes': 14144,\n",
       " 'warehouse': 83541,\n",
       " 'second': 67602,\n",
       " 'none': 52917,\n",
       " 'corn': 16276,\n",
       " 'face': 26376,\n",
       " 'good': 31862,\n",
       " 'anything': 3323,\n",
       " 'take': 75524,\n",
       " 'lawyers': 43547,\n",
       " 'also': 2267,\n",
       " 'superb': 74496,\n",
       " 'accused': 573,\n",
       " 'turncoat': 79650,\n",
       " 'selling': 67880,\n",
       " 'out': 54934,\n",
       " 'boss': 9016,\n",
       " 'dishonest': 20910,\n",
       " 'lawyer': 43544,\n",
       " 'shrugs': 69345,\n",
       " 'indifferently': 38087,\n",
       " 'says': 66648,\n",
       " 'three': 77306,\n",
       " 'funny': 29934,\n",
       " 'words': 85490,\n",
       " 'jeffrey': 40270,\n",
       " 'favorite': 27073,\n",
       " 'later': 43335,\n",
       " 'larry': 43248,\n",
       " 'sanders': 66221,\n",
       " 'fantastic': 26765,\n",
       " 'too': 78178,\n",
       " 'mad': 46082,\n",
       " 'millionaire': 49123,\n",
       " 'crush': 17438,\n",
       " 'ghetto': 31039,\n",
       " 'character': 12575,\n",
       " 'malevolent': 46488,\n",
       " 'usual': 81760,\n",
       " 'hospital': 36253,\n",
       " 'scene': 66831,\n",
       " 'invade': 39197,\n",
       " 'demolition': 19284,\n",
       " 'site': 69944,\n",
       " 'classics': 13852,\n",
       " 'look': 45171,\n",
       " 'legs': 43844,\n",
       " 'two': 79829,\n",
       " 'big': 7529,\n",
       " 'diggers': 20340,\n",
       " 'fighting': 27621,\n",
       " 'bleeds': 8055,\n",
       " 'movie': 50708,\n",
       " 'gets': 31005,\n",
       " 'better': 7327,\n",
       " 'each': 23040,\n",
       " 'quite': 61361,\n",
       " 'often': 53959,\n",
       " 'easily': 23134,\n",
       " 'underrated': 80490,\n",
       " 'inn': 38496,\n",
       " 'cannon': 11230,\n",
       " 'sure': 74701,\n",
       " 'its': 39760,\n",
       " 'flawed': 28310,\n",
       " 'does': 21526,\n",
       " 'realistic': 62227,\n",
       " 'view': 82635,\n",
       " 'unlike': 80984,\n",
       " 'say': 66614,\n",
       " 'how': 36403,\n",
       " 'citizen': 13703,\n",
       " 'kane': 41231,\n",
       " 'gave': 30600,\n",
       " 'lounge': 45400,\n",
       " 'singers': 69816,\n",
       " 'titanic': 77850,\n",
       " 'italians': 39652,\n",
       " 'idiots': 37148,\n",
       " 'jokes': 40668,\n",
       " 'fall': 26577,\n",
       " 'flat': 28259,\n",
       " 'still': 73100,\n",
       " 'very': 82434,\n",
       " 'lovable': 45426,\n",
       " 'way': 83855,\n",
       " 'comedies': 14748,\n",
       " 'pull': 60756,\n",
       " 'story': 73341,\n",
       " 'traditionally': 78617,\n",
       " 'reviled': 64143,\n",
       " 'members': 48361,\n",
       " 'society': 70981,\n",
       " 'truly': 79392,\n",
       " 'impressive': 37672,\n",
       " 'fisher': 28086,\n",
       " 'king': 41986,\n",
       " 'crap': 16866,\n",
       " 'either': 23591,\n",
       " 'only': 54317,\n",
       " 'complaint': 15087,\n",
       " 'cast': 11820,\n",
       " 'someone': 71200,\n",
       " 'else': 23861,\n",
       " 'director': 20579,\n",
       " 'writer': 85815,\n",
       " 'so': 70920,\n",
       " 'typical': 79884,\n",
       " 'was': 83638,\n",
       " 'less': 44043,\n",
       " 'movies': 50829,\n",
       " 'actually': 855,\n",
       " 'followable': 28733,\n",
       " 'leslie': 44037,\n",
       " 'made': 46110,\n",
       " 'she': 68705,\n",
       " 'under': 80396,\n",
       " 'rated': 61957,\n",
       " 'actress': 833,\n",
       " 'there': 76890,\n",
       " 'moments': 49956,\n",
       " 'could': 16521,\n",
       " 'fleshed': 28358,\n",
       " 'bit': 7772,\n",
       " 'probably': 59962,\n",
       " 'cut': 17774,\n",
       " 'room': 65130,\n",
       " 'worth': 85666,\n",
       " 'price': 59798,\n",
       " 'rent': 63439,\n",
       " 'overall': 55154,\n",
       " 'himself': 35370,\n",
       " 'job': 40537,\n",
       " 'characteristic': 12595,\n",
       " 'speaking': 71740,\n",
       " 'directly': 20577,\n",
       " 'audience': 4783,\n",
       " 'again': 1399,\n",
       " 'actor': 780,\n",
       " 'fume': 29865,\n",
       " 'both': 9037,\n",
       " 'played': 58246,\n",
       " 'parts': 56219,\n",
       " 'well': 84151,\n",
       " 'comedic': 14744,\n",
       " 'robin': 64814,\n",
       " 'williams': 84873,\n",
       " 'nor': 52989,\n",
       " 'quirky': 61355,\n",
       " 'insane': 38570,\n",
       " 'recent': 62460,\n",
       " 'thriller': 77328,\n",
       " 'fame': 26628,\n",
       " 'hybrid': 36886,\n",
       " 'drama': 22201,\n",
       " 'dramatization': 22232,\n",
       " 'mixed': 49686,\n",
       " 'new': 52377,\n",
       " 'per': 56917,\n",
       " 'se': 67482,\n",
       " 'mystery': 51547,\n",
       " 'suspense': 74862,\n",
       " 'vehicle': 82215,\n",
       " 'attempts': 4687,\n",
       " 'locate': 44940,\n",
       " 'sick': 69453,\n",
       " 'boy': 9173,\n",
       " 'keeper': 41529,\n",
       " 'starring': 72693,\n",
       " 'sandra': 66238,\n",
       " 'oh': 53985,\n",
       " 'rory': 65179,\n",
       " 'culkin': 17571,\n",
       " 'pretty': 59753,\n",
       " 'news': 52419,\n",
       " 'report': 63566,\n",
       " 'william': 84871,\n",
       " 'close': 14119,\n",
       " 'achieving': 607,\n",
       " 'goal': 31618,\n",
       " 'must': 51411,\n",
       " 'highly': 35233,\n",
       " 'entertained': 24647,\n",
       " 'though': 77238,\n",
       " 'fails': 26490,\n",
       " 'teach': 75993,\n",
       " 'guide': 33006,\n",
       " 'inspect': 38657,\n",
       " 'amuse': 2656,\n",
       " 'felt': 27310,\n",
       " 'watching': 83742,\n",
       " 'guy': 33230,\n",
       " 'performing': 57030,\n",
       " 'actions': 757,\n",
       " 'third': 77100,\n",
       " 'person': 57183,\n",
       " 'perspective': 57215,\n",
       " 'real': 62209,\n",
       " 'able': 235,\n",
       " 'subscribe': 74033,\n",
       " 'premise': 59569,\n",
       " 'watch': 83719,\n",
       " 'definitely': 18994,\n",
       " 'friday': 29572,\n",
       " 'saturday': 66505,\n",
       " 'night': 52612,\n",
       " 'fare': 26820,\n",
       " 'rates': 61962,\n",
       " 'fiend': 27579,\n",
       " 'yes': 86332,\n",
       " 'art': 4097,\n",
       " 'successfully': 74149,\n",
       " 'slow': 70470,\n",
       " 'paced': 55502,\n",
       " 'unfolds': 80731,\n",
       " 'nice': 52498,\n",
       " 'volumes': 83084,\n",
       " 'even': 25439,\n",
       " 'notice': 53128,\n",
       " 'happening': 33861,\n",
       " 'fine': 27921,\n",
       " 'performance': 56992,\n",
       " 'sexuality': 68378,\n",
       " 'angles': 2926,\n",
       " 'seem': 67728,\n",
       " 'unnecessary': 81064,\n",
       " 'affect': 1257,\n",
       " 'enjoy': 24488,\n",
       " 'however': 36415,\n",
       " 'core': 16248,\n",
       " 'engaging': 24414,\n",
       " 'doesn': 21532,\n",
       " 'rush': 65646,\n",
       " 'onto': 54355,\n",
       " 'grips': 32693,\n",
       " 'enough': 24547,\n",
       " 'keep': 41527,\n",
       " 'wondering': 85359,\n",
       " 'direction': 20560,\n",
       " 'use': 81708,\n",
       " 'lights': 44400,\n",
       " 'achieve': 598,\n",
       " 'desired': 19699,\n",
       " 'affects': 1270,\n",
       " 'unexpectedness': 80672,\n",
       " 'looking': 45182,\n",
       " 'lay': 43553,\n",
       " 'back': 5359,\n",
       " 'hear': 34444,\n",
       " 'thrilling': 77337,\n",
       " 'short': 69132,\n",
       " 'critically': 17210,\n",
       " 'acclaimed': 495,\n",
       " 'psychological': 60623,\n",
       " 'based': 6143,\n",
       " 'true': 79365,\n",
       " 'events': 25456,\n",
       " 'gabriel': 30096,\n",
       " 'celebrated': 12164,\n",
       " 'late': 43323,\n",
       " 'talk': 75597,\n",
       " 'host': 36267,\n",
       " 'becomes': 6593,\n",
       " 'captivated': 11371,\n",
       " 'harrowing': 34081,\n",
       " 'listener': 44714,\n",
       " 'adoptive': 1077,\n",
       " 'mother': 50538,\n",
       " 'toni': 78155,\n",
       " 'collette': 14580,\n",
       " 'troubling': 79314,\n",
       " 'questions': 61267,\n",
       " 'arise': 3879,\n",
       " 'finds': 27919,\n",
       " 'drawn': 22275,\n",
       " 'into': 39115,\n",
       " 'widening': 84736,\n",
       " 'hides': 35190,\n",
       " 'deadly': 18510,\n",
       " 'secretâ': 67641,\n",
       " 'according': 530,\n",
       " 'official': 53900,\n",
       " 'synopsis': 75320,\n",
       " 'really': 62265,\n",
       " 'stop': 73281,\n",
       " 'reading': 62185,\n",
       " 'these': 76937,\n",
       " 'comments': 14879,\n",
       " 'now': 53226,\n",
       " 'lose': 45314,\n",
       " 'ending': 24303,\n",
       " 'ms': 50987,\n",
       " 'planning': 58166,\n",
       " 'chopped': 13293,\n",
       " 'sent': 68019,\n",
       " 'deleted': 19128,\n",
       " 'land': 43073,\n",
       " 'overkill': 55263,\n",
       " 'nature': 51920,\n",
       " 'physical': 57624,\n",
       " 'mental': 48474,\n",
       " 'ailments': 1646,\n",
       " 'obvious': 53691,\n",
       " 'mr': 50948,\n",
       " 'returns': 64042,\n",
       " 'york': 86457,\n",
       " 'possibly': 59098,\n",
       " 'blindness': 8110,\n",
       " 'question': 61253,\n",
       " 'revelation': 64076,\n",
       " 'certain': 12301,\n",
       " 'highway': 35251,\n",
       " 'video': 82575,\n",
       " 'tape': 75755,\n",
       " 'would': 85679,\n",
       " 'benefit': 7031,\n",
       " 'editing': 23318,\n",
       " 'bobby': 8437,\n",
       " 'cannavale': 11210,\n",
       " 'jess': 40372,\n",
       " 'initially': 38453,\n",
       " 'believable': 6887,\n",
       " 'couple': 16620,\n",
       " 'establishing': 25212,\n",
       " 'relationship': 63138,\n",
       " 'might': 48976,\n",
       " 'helped': 34744,\n",
       " 'set': 68256,\n",
       " 'stage': 72472,\n",
       " 'otherwise': 54887,\n",
       " 'exemplary': 25833,\n",
       " 'offers': 53885,\n",
       " 'exceptionally': 25720,\n",
       " 'strong': 73682,\n",
       " 'characterization': 12598,\n",
       " 'gay': 30616,\n",
       " 'impersonation': 37559,\n",
       " 'anna': 3026,\n",
       " 'joe': 40583,\n",
       " 'morton': 50477,\n",
       " 'ashe': 4251,\n",
       " 'pete': 57300,\n",
       " 'logand': 45009,\n",
       " 'perfect': 56960,\n",
       " 'donna': 21755,\n",
       " 'belongs': 6958,\n",
       " 'creepy': 17065,\n",
       " 'hall': 33549,\n",
       " 'correct': 16342,\n",
       " 'saying': 66629,\n",
       " 'psycho': 60606,\n",
       " 'several': 68313,\n",
       " 'organizations': 54663,\n",
       " 'giving': 31364,\n",
       " 'awards': 5121,\n",
       " 'seemed': 67732,\n",
       " 'reach': 62145,\n",
       " 'women': 85324,\n",
       " 'due': 22653,\n",
       " 'slighter': 70362,\n",
       " 'dispersion': 21061,\n",
       " 'roles': 64981,\n",
       " 'certainly': 12302,\n",
       " 'noticed': 53132,\n",
       " 'award': 5117,\n",
       " 'consideration': 15685,\n",
       " 'patrick': 56410,\n",
       " 'stettner': 73022,\n",
       " 'evokes': 25584,\n",
       " 'hitchcock': 35526,\n",
       " 'makes': 46416,\n",
       " 'getting': 31012,\n",
       " 'sandwich': 66251,\n",
       " 'vending': 82254,\n",
       " 'machine': 46013,\n",
       " 'suspenseful': 74868,\n",
       " 'finally': 27894,\n",
       " 'writers': 85819,\n",
       " 'armistead': 3951,\n",
       " 'maupin': 47621,\n",
       " 'terry': 76519,\n",
       " 'anderson': 2788,\n",
       " 'deserve': 19663,\n",
       " 'gratitude': 32392,\n",
       " 'attendants': 4693,\n",
       " 'everywhere': 25539,\n",
       " 'john': 40618,\n",
       " 'cullum': 17580,\n",
       " 'lisa': 44694,\n",
       " 'emery': 24033,\n",
       " 'becky': 6587,\n",
       " 'baker': 5609,\n",
       " 'dir': 20546,\n",
       " 'hitchcockian': 35528,\n",
       " 'suspenser': 74874,\n",
       " 'gives': 31362,\n",
       " 'stand': 72580,\n",
       " 'low': 45508,\n",
       " 'key': 41690,\n",
       " 'celebrities': 12172,\n",
       " 'fans': 26731,\n",
       " 'near': 52032,\n",
       " 'paranoia': 55977,\n",
       " 'associates': 4435,\n",
       " 'why': 84699,\n",
       " 'almost': 2189,\n",
       " 'norm': 53009,\n",
       " 'latest': 43352,\n",
       " 'derange': 19552,\n",
       " 'fan': 26677,\n",
       " 'scenario': 66823,\n",
       " 'no': 52802,\n",
       " 'radio': 61513,\n",
       " 'personality': 57191,\n",
       " 'named': 51706,\n",
       " 'reads': 62194,\n",
       " 'stories': 73317,\n",
       " 'penned': 56813,\n",
       " 'airwaves': 1726,\n",
       " 'accumulated': 556,\n",
       " 'interesting': 38921,\n",
       " 'form': 29012,\n",
       " 'submitted': 74015,\n",
       " 'manuscript': 46869,\n",
       " 'travails': 78905,\n",
       " 'troubled': 79306,\n",
       " 'youth': 86562,\n",
       " 'editor': 23326,\n",
       " 'read': 62171,\n",
       " 'naturally': 51917,\n",
       " 'disturbed': 21265,\n",
       " 'ultimately': 80048,\n",
       " 'intrigued': 39146,\n",
       " 'nightmarish': 52635,\n",
       " 'existence': 25891,\n",
       " 'abducted': 182,\n",
       " 'sexually': 68381,\n",
       " 'abused': 396,\n",
       " 'rescued': 63693,\n",
       " 'nurse': 53411,\n",
       " 'excellent': 25704,\n",
       " 'adopted': 1071,\n",
       " 'correspondence': 16367,\n",
       " 'reveals': 64072,\n",
       " 'dying': 22996,\n",
       " 'aids': 1630,\n",
       " 'meet': 48193,\n",
       " 'suddenly': 74216,\n",
       " 'doubt': 21986,\n",
       " 'devious': 19982,\n",
       " 'ulterior': 80046,\n",
       " 'motives': 50579,\n",
       " 'seed': 67700,\n",
       " 'planted': 58178,\n",
       " 'estranged': 25257,\n",
       " 'lover': 45470,\n",
       " 'whose': 84687,\n",
       " 'sudden': 74215,\n",
       " 'departure': 19459,\n",
       " 'city': 13712,\n",
       " 'apartment': 3390,\n",
       " 'emotional': 24092,\n",
       " 'tailspin': 75492,\n",
       " 'grown': 32841,\n",
       " 'tempest': 76272,\n",
       " 'teacup': 76004,\n",
       " 'decides': 18738,\n",
       " 'investigating': 39248,\n",
       " 'backgrounds': 5388,\n",
       " 'discovering': 20806,\n",
       " 'truths': 79431,\n",
       " 'didn': 20233,\n",
       " 'anticipate': 3200,\n",
       " 'co': 14252,\n",
       " 'wrote': 85871,\n",
       " 'screenplay': 67319,\n",
       " 'former': 29035,\n",
       " 'novice': 53219,\n",
       " 'hoax': 35630,\n",
       " 'run': 65599,\n",
       " 'full': 29845,\n",
       " 'tilt': 77608,\n",
       " 'any': 3294,\n",
       " 'old': 54090,\n",
       " 'fashioned': 26929,\n",
       " 'pot': 59149,\n",
       " 'boiler': 8562,\n",
       " 'helps': 34758,\n",
       " 'conflicted': 15468,\n",
       " 'hearted': 34473,\n",
       " 'genuinely': 30869,\n",
       " 'number': 53362,\n",
       " 'fact': 26418,\n",
       " 'him': 35332,\n",
       " 'thing': 77008,\n",
       " 'escaped': 25094,\n",
       " 'own': 55440,\n",
       " 'unsettling': 81295,\n",
       " 'dreadful': 22285,\n",
       " 'trait': 78683,\n",
       " 'leave': 43698,\n",
       " 'unmentioned': 81035,\n",
       " 'underlines': 80458,\n",
       " 'desperation': 19741,\n",
       " 'rattle': 62010,\n",
       " 'runs': 65618,\n",
       " 'gas': 30515,\n",
       " 'eventually': 25466,\n",
       " 'repetitive': 63524,\n",
       " 'predictable': 59457,\n",
       " 'despite': 19755,\n",
       " 'finely': 27926,\n",
       " 'directed': 20549,\n",
       " 'piece': 57740,\n",
       " 'hoodwink': 36012,\n",
       " 'pays': 56546,\n",
       " 'listen': 44709,\n",
       " 'inner': 38507,\n",
       " 'voice': 83029,\n",
       " 'careful': 11456,\n",
       " 'hope': 36069,\n",
       " 'god': 31648,\n",
       " 'bless': 8078,\n",
       " 'constantly': 15741,\n",
       " 'shooting': 69096,\n",
       " 'foot': 28803,\n",
       " 'lately': 43330,\n",
       " 'dumb': 22725,\n",
       " 'done': 21731,\n",
       " 'decade': 18672,\n",
       " 'perhaps': 57038,\n",
       " 'exception': 25718,\n",
       " 'death': 18575,\n",
       " 'smoochy': 70667,\n",
       " 'bombed': 8654,\n",
       " 'came': 11029,\n",
       " 'cult': 17593,\n",
       " 'dramas': 22214,\n",
       " 'especially': 25159,\n",
       " 'insomnia': 38649,\n",
       " 'hour': 36337,\n",
       " 'photo': 57572,\n",
       " 'mediocre': 48148,\n",
       " 'reviews': 64137,\n",
       " 'quick': 61287,\n",
       " 'dvd': 22928,\n",
       " 'release': 63167,\n",
       " 'among': 2589,\n",
       " 'period': 57050,\n",
       " 'chilling': 13146,\n",
       " 'include': 37874,\n",
       " 'serial': 68161,\n",
       " 'killer': 41885,\n",
       " 'anyone': 3315,\n",
       " 'physically': 57628,\n",
       " 'dangerous': 18132,\n",
       " 'concept': 15267,\n",
       " 'actual': 849,\n",
       " 'case': 11735,\n",
       " 'fraud': 29381,\n",
       " 'yet': 86357,\n",
       " 'officially': 53903,\n",
       " 'confirmed': 15456,\n",
       " 'autobiography': 4952,\n",
       " 'child': 13096,\n",
       " 'anthony': 3178,\n",
       " 'godby': 31657,\n",
       " 'johnson': 40633,\n",
       " 'suffered': 74242,\n",
       " 'horrific': 36168,\n",
       " 'abuse': 394,\n",
       " 'contracted': 15934,\n",
       " 'result': 63915,\n",
       " 'moved': 50686,\n",
       " 'reports': 63578,\n",
       " 'online': 54311,\n",
       " 'may': 47670,\n",
       " 'exist': 25883,\n",
       " 'confused': 15501,\n",
       " 'feelings': 27230,\n",
       " 'brilliantly': 9661,\n",
       " 'portrayed': 59020,\n",
       " 'resurfaced': 63930,\n",
       " 'mind': 49177,\n",
       " 'sociopathic': 70993,\n",
       " 'caretaker': 11474,\n",
       " 'role': 64969,\n",
       " 'cry': 17457,\n",
       " 'little': 44780,\n",
       " 'miss': 49561,\n",
       " 'sunshine': 74481,\n",
       " 'times': 77686,\n",
       " 'looked': 45175,\n",
       " 'camera': 11039,\n",
       " 'thought': 77250,\n",
       " 'staring': 72668,\n",
       " 'takes': 75543,\n",
       " 'play': 58237,\n",
       " 'sort': 71432,\n",
       " 'understated': 80513,\n",
       " 'reviewed': 64130,\n",
       " 'actresses': 837,\n",
       " 'generation': 30764,\n",
       " 'nominated': 52888,\n",
       " 'academy': 428,\n",
       " 'incredible': 37968,\n",
       " 'least': 43674,\n",
       " 'scary': 66784,\n",
       " 'dark': 18225,\n",
       " 'recommend': 62546,\n",
       " 'prepared': 59594,\n",
       " 'unsettled': 81292,\n",
       " 'because': 6560,\n",
       " 'leaves': 43704,\n",
       " 'strange': 73467,\n",
       " 'feeling': 27227,\n",
       " 'first': 28060,\n",
       " 'maupins': 47622,\n",
       " 'taken': 75530,\n",
       " 'displayed': 21070,\n",
       " 'cares': 11468,\n",
       " 'loves': 45480,\n",
       " 'said': 65957,\n",
       " 'we': 83914,\n",
       " 'version': 82401,\n",
       " 'expected': 25964,\n",
       " 'past': 56302,\n",
       " 'gloss': 31549,\n",
       " 'hollywood': 35790,\n",
       " 'succeeded': 74136,\n",
       " 'amount': 2604,\n",
       " 'restraint': 63896,\n",
       " 'captures': 11382,\n",
       " 'fragile': 29241,\n",
       " 'essence': 25199,\n",
       " 'lets': 44077,\n",
       " 'us': 81693,\n",
       " 'struggle': 73715,\n",
       " 'issues': 39619,\n",
       " 'trust': 79410,\n",
       " 'personnel': 57212,\n",
       " 'lifejess': 44320,\n",
       " 'around': 4008,\n",
       " 'himdonna': 35340,\n",
       " 'introduced': 39159,\n",
       " 'players': 58253,\n",
       " 'reminded': 63324,\n",
       " 'nothing': 53115,\n",
       " 'seems': 67739,\n",
       " 'smallest': 70549,\n",
       " 'event': 25449,\n",
       " 'change': 12491,\n",
       " 'our': 54924,\n",
       " 'lives': 44816,\n",
       " 'irrevocably': 39449,\n",
       " 'request': 63664,\n",
       " 'review': 64127,\n",
       " 'book': 8778,\n",
       " 'turns': 79662,\n",
       " 'changing': 12507,\n",
       " 'find': 27910,\n",
       " 'strength': 73570,\n",
       " 'within': 85167,\n",
       " 'carry': 11656,\n",
       " 'forward': 29124,\n",
       " 'bad': 5454,\n",
       " 'avoid': 5075,\n",
       " 'average': 5035,\n",
       " 'american': 2512,\n",
       " 'serious': 68186,\n",
       " 'please': 58315,\n",
       " 'chance': 12456,\n",
       " 'touches': 78441,\n",
       " 'darkness': 18246,\n",
       " 'go': 31613,\n",
       " 'ourselves': 54928,\n",
       " 'stepped': 72974,\n",
       " 'another': 3113,\n",
       " 'quality': 61144,\n",
       " 'forget': 28972,\n",
       " 'steals': 72855,\n",
       " 'leading': 43617,\n",
       " 'looks': 45194,\n",
       " 'screen': 67300,\n",
       " 'presence': 59640,\n",
       " 'hacks': 33375,\n",
       " 'opinion': 54463,\n",
       " 'liked': 44424,\n",
       " 'action': 741,\n",
       " 'tense': 76369,\n",
       " 'opening': 54422,\n",
       " 'semi': 67908,\n",
       " 'truck': 79345,\n",
       " 'transitional': 78780,\n",
       " 'filmed': 27725,\n",
       " 'ways': 83885,\n",
       " 'lapse': 43197,\n",
       " 'photography': 57589,\n",
       " 'unusual': 81438,\n",
       " 'colors': 14657,\n",
       " 'evil': 25559,\n",
       " 'illnesses': 37291,\n",
       " 'born': 8972,\n",
       " 'modern': 49804,\n",
       " ...}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "\n",
    "# Asigna un numero según el orden de aparicion\n",
    "baseline_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_c = CountVectorizer()\n",
    "vectorizer_c.fit(reviews_train_clean)\n",
    "\n",
    "# Ya no es binaria la aparicion, sino un conteo por palabra\n",
    "X_baseline_c = vectorizer_c.transform(reviews_train_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "87063\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline_c.shape)\n",
    "print(len(vectorizer_c.get_feature_names())) # Las mismas\n",
    "# X_baseline_c.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<25000x87063 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3410713 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matriz demasiado grande como para que numpy la imprima por pantalla\n",
    "X_baseline_c"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Baseline Model\n",
    "\n",
    "Entrenar un modelo de Regresión Logística después de transformar los datos con CountVectorized\n",
    "\n",
    "* Son fáciles de interpretar\n",
    "* Los modelos lineales tienden a funcionar bien en conjuntos de datos dispersos como éste.\n",
    "* Aprenden muy rápido en comparación con otros algoritmos.\n",
    "\n",
    "Probar modelos con valores de C de [0.01, 0.05, 0.25, 0.5, 1] y ver cual es el mejor valor para C, y calcular la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Los comentarios vienen ordenados. Los primeros 12,5k son positivos\n",
    "# A test le ocurre lo mismo\n",
    "target = [1 if i < 12500 else 0 for i in range(25000)]\n",
    "\n",
    "# target = []\n",
    "# for i in range(25000):\n",
    "#     if i < 12500: \n",
    "#         target.append(1)\n",
    "#     else:\n",
    "#         target.append(0)\n",
    "\n",
    "def train_model(X_TRAIN, X_TEST):\n",
    "    \n",
    "    lr = LogisticRegression()\n",
    "    \n",
    "    params = {\n",
    "        'C': [0.01, 0.05, 0.25, 0.5, 1]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(lr, params, cv=5)\n",
    "    grid.fit(X_TRAIN, target)\n",
    "\n",
    "    print (\"Final Accuracy: %s\" % accuracy_score(target, grid.best_estimator_.predict(X_TEST)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.88188\n"
     ]
    }
   ],
   "source": [
    "train_model(X_baseline, X_test_baseline)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eliminar las Stop Words\n",
    "\n",
    "Las stop words son palabras muy comunes como \"si\", \"pero\", \"nosotros\", \"él\", \"ella\" y \"ellos\". Normalmente podemos eliminar estas palabras sin cambiar la semántica de un texto y hacerlo a menudo (aunque no siempre) mejora el rendimiento de un modelo. La eliminación de estas palabras de parada resulta mucho más útil cuando empezamos a utilizar secuencias de palabras más largas como características del modelo (véanse los n-gramas más adelante).\n",
    "\n",
    "Antes de aplicar el CountVectorized, vamos a eliminar las stopwords, incluidas en nltk.corpus\n",
    "\n",
    "A continuación, aplica el CountVectorizer y entrena el modelo de regresión logística para obtener la precisión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Fernando\n",
      "[nltk_data]     Carrasco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hay que bajarse las stopwords de nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para visualizar los stopwords de inglés\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['de', 'la', 'que', 'el', 'en', 'y', 'a', 'los', 'del', 'se']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para visualizarlas en español\n",
    "stopwords.words('spanish')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['一', '一下', '一些', '一切', '一则', '一天', '一定', '一方面', '一旦', '一时']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para visualizarlas en español\n",
    "stopwords.words('chinese')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'abans',\n",
       " 'ací',\n",
       " 'ah',\n",
       " 'així',\n",
       " 'això',\n",
       " 'al',\n",
       " 'aleshores',\n",
       " 'algun',\n",
       " 'alguna']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('catalan')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ahala',\n",
       " 'aitzitik',\n",
       " 'al',\n",
       " 'ala ',\n",
       " 'alabadere',\n",
       " 'alabaina',\n",
       " 'alabaina',\n",
       " 'aldiz ',\n",
       " 'alta',\n",
       " 'amaitu']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('basque')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words('spanish'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Aplicamos la eliminacion de las palabras directamente sobre las reviews\n",
    "# Demasiado manual. Mejor sobre el CountVectorizer (ver abajo)\n",
    "english_stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stop_words(corpus):\n",
    "    removed_stop_words = []\n",
    "    for review in corpus:\n",
    "        \n",
    "        # Para cada review elimina las stopwords, y separa todas las palabras por espacio\n",
    "        removed_stop_words.append(\n",
    "            ' '.join([word for word in review.split() if word not in english_stop_words])\n",
    "        )\n",
    "        \n",
    "    return removed_stop_words\n",
    "\n",
    "# Se lo aplicamos antes de vectorizar\n",
    "no_stop_words_train = remove_stop_words(reviews_train_clean)\n",
    "no_stop_words_test = remove_stop_words(reviews_test_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Vectorizamos tras eliminar las stop words\n",
    "Ver docu, tiene cosas interesantes como lowercase=True. Lo hace antes de vectorizar, \n",
    "o el argumento stopwords\n",
    "'''\n",
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(no_stop_words_train)\n",
    "\n",
    "X = cv.transform(no_stop_words_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87046)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87936\n"
     ]
    }
   ],
   "source": [
    "# Se aplica el mismo a test\n",
    "X_test = cv.transform(no_stop_words_test)\n",
    "\n",
    "# Y entrenamos\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 87046)\n",
      "Stop words eliminadas: 17\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X_baseline tras aplicar el vectorizador tal cual en los datos\n",
    "X tras aplicar el vectorizador despues de eliminar las stop words. No se carga muchas\n",
    "'''\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87976\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "El resultado de este codigo es practicamente igual que el anterior, pero elimina más stopwords\n",
    "'''\n",
    "\n",
    "cv = CountVectorizer(binary=True,\n",
    "                     stop_words = english_stop_words)\n",
    "\n",
    "cv.fit(reviews_train_clean)\n",
    "\n",
    "X = cv.transform(reviews_train_clean)\n",
    "X_test = cv.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 86918)\n",
      "Stop words eliminadas: 145\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "X_baseline tras aplicar el vectorizador tal cual en los datos\n",
    "X tras aplicar el vectorizador despus de eliminar las stop words\n",
    "En este caso elimina más, el countvectorizer tokeniza mejor las palabras\n",
    "de lo que lo hemos hecho nosotros en la funcion remove_stop_words. Por ejemplo \"it's\" serian dos palabras\n",
    "'''\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Stop words eliminadas:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota:** En la práctica, una manera más fácil de eliminar las stop words es simplemente utilizar el argumento stop_words con cualquiera de las clases 'Vectorizer' de scikit-learn. Si quieres usar la lista completa de stop words de NLTK puedes usar stop_words='english'. En la práctica he encontrado que el uso de la lista de NLTK en realidad disminuye mi rendimiento porque es demasiado amplia, por lo que normalmente proporciono mi propia lista de palabras. Por ejemplo, stop_words=['in','of','at','a','the'] ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un paso habitual en el preprocesamiento de textos es normalizar las palabras del corpus intentando convertir todas las formas de una palabra en una sola. Para ello existen dos métodos: el Stemming y la Lemmatization.\n",
    "\n",
    "# Stemming\n",
    "\n",
    "El \"stemming\" se considera el método de normalización más tosco o de fuerza bruta (aunque esto no significa necesariamente que vaya a dar peores resultados). Hay varios algoritmos, pero en general todos utilizan reglas básicas para cortar los extremos de las palabras.\n",
    "\n",
    "NLTK tiene varias implementaciones de algoritmos de stemming. Nosotros usaremos el stemmer Porter. Los más usados:\n",
    "* PorterStemmer\n",
    "* SnowballStemmer\n",
    "\n",
    "Aplicar un PoterStemmer, vectorizar, y entrenar el modelo de nuevo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli fli flight flown die die mule deni deni die agre own humbl size meet state siez item sensat tradit refer colon colon plot\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "El stemmer se aplica sobre cada palabra. Las recorta eliminando plurales y tiempos verbales\n",
    "Modifica muy poco cada palabra\n",
    "'''\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "plurals = ['caresses', 'flies', 'fly','flight', 'flown', 'dies', 'die', 'mules', 'denied', 'deny',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer', 'colonizing',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fli fli flight die mule deni deni die agre own humbl size meet state siez item sensat tradit refer colon plot\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "plurals = ['caresses', 'flies', 'fly','flight', 'dies', 'mules', 'denied', 'deny',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recorr corr correl corr cas caser cas play vol vol volv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "plurals = ['recorrer', 'corriendo', 'correlación', 'correré', 'casas', 'casero', 'caso', 'playa', 'volando', 'volar', 'volveré']\n",
    "singles = [stemmer.stem(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicamos a mano. Los stemmers no eliminan palabras, solo quitan sufijos, y ahora habrá más palabras que sean iguales\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def get_stemmed_text(corpus):\n",
    "    stemmer = PorterStemmer()\n",
    "    return [' '.join([stemmer.stem(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "stemmed_reviews_train = get_stemmed_text(reviews_train_clean)\n",
    "stemmed_reviews_test = get_stemmed_text(reviews_test_clean)\n",
    "\n",
    "cv = CountVectorizer(binary=True, stop_words = english_stop_words)\n",
    "cv.fit(stemmed_reviews_train)\n",
    "\n",
    "X_stem = cv.transform(stemmed_reviews_train)\n",
    "X_test = cv.transform(stemmed_reviews_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87656\n"
     ]
    }
   ],
   "source": [
    "train_model(X_stem, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 66715)\n",
      "Diff X normal y X tras stemmer y vectorización: 20348\n"
     ]
    }
   ],
   "source": [
    "# No elimina palabras. Solo recorta sufijos y agrupa tipos de palabras.\n",
    "# Como resultado dará menos palabras debido al agrupado. Se carga unas cuantas letras de las palabras\n",
    "print(X_baseline.shape)\n",
    "print(X_stem.shape)\n",
    "print(\"Diff X normal y X tras stemmer y vectorización:\", X_baseline.shape[1] - X_stem.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "La Lemmatization consiste en identificar la parte del discurso de una palabra determinada y, a continuación, aplicar reglas más complejas para transformar la palabra en su verdadera raíz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Fernando\n",
      "[nltk_data]     Carrasco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caress fly fly flight dy mule study died agreed owned humbled sized meeting stating siezing itemization sensational traditional reference colonizer plotted\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "La diferencia con el stemming es que la lematización tiene en cuenta la morfología\n",
    "de la palabra, sustituyendola por la raiz, no recortándola. Y no es tan restrictivo como el stemming.\n",
    "Necesita un buen diccionario con mapeos, como wordnet\n",
    "\n",
    "En nltk no hay lematizadores en español. Habria que bajarse algun paquete como pip install es-lemmatizer\n",
    "'''\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "plurals = ['caresses', 'flies','fly','flight', 'dies', 'mules', 'studies',\n",
    "            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "            'meeting', 'stating', 'siezing', 'itemization',\n",
    "            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "            'plotted']\n",
    "singles = [lemmatizer.lemmatize(plural) for plural in plurals]\n",
    "\n",
    "print(' '.join(singles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.87812\n"
     ]
    }
   ],
   "source": [
    "def get_lemmatized_text(corpus):\n",
    "    \n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [' '.join([lemmatizer.lemmatize(word) for word in review.split()]) for review in corpus]\n",
    "\n",
    "# Lematizamos las reviews\n",
    "lemmatized_reviews_train = get_lemmatized_text(reviews_train_clean)\n",
    "lemmatized_reviews_test = get_lemmatized_text(reviews_test_clean)\n",
    "\n",
    "# Vectorizamos con conteo tras lematizar\n",
    "cv = CountVectorizer(binary=True, stop_words = english_stop_words)\n",
    "cv.fit(lemmatized_reviews_train)\n",
    "\n",
    "X = cv.transform(lemmatized_reviews_train)\n",
    "X_test = cv.transform(lemmatized_reviews_test)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 80181)\n",
      "Diff X normal y X tras lematizador y vectorización: 6882\n"
     ]
    }
   ],
   "source": [
    "# Elimina menos que con el stemmer. Normal, el stemmer recorta mucho del sufijo\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-grams\n",
    "\n",
    "Podemos añadir potencialmente más poder predictivo a nuestro modelo añadiendo también secuencias de dos o tres palabras (bigramas o trigramas). Por ejemplo, si una crítica tuviera la secuencia de tres palabras \"no me gustó la película\", sólo consideraríamos estas palabras individualmente con un modelo de unigramas y probablemente no captaríamos que se trata de un sentimiento negativo, porque la palabra \"me gustó\" por sí sola va a estar muy correlacionada con una crítica positiva.\n",
    "\n",
    "La biblioteca scikit-learn hace que sea muy fácil jugar con esto. Sólo tiene que utilizar el argumento ngram_range con cualquiera de las clases 'Vectorizer'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"didn't\",)\n",
      "('love',)\n",
      "('music',)\n",
      "('at',)\n",
      "('all',)\n",
      "('my',)\n",
      "('love',)\n",
      "(\"didn't\", 'love')\n",
      "('love', 'music')\n",
      "('music', 'at')\n",
      "('at', 'all')\n",
      "('all', 'my')\n",
      "('my', 'love')\n",
      "###############\n",
      "(\"didn't\", 'love', 'music')\n",
      "('love', 'music', 'at')\n",
      "('music', 'at', 'all')\n",
      "('at', 'all', 'my')\n",
      "('all', 'my', 'love')\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = \"didn't love music at all my love\"\n",
    "\n",
    "one = ngrams(sentence.split(), 1)\n",
    "two = ngrams(sentence.split(), 2)\n",
    "three = ngrams(sentence.split(), 3)\n",
    "\n",
    "for grams in one:\n",
    "  print(grams)\n",
    "\n",
    "for grams in two:\n",
    "  print(grams)\n",
    "print('###############')\n",
    "for grams in three:\n",
    "  print(grams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30444\\153384340.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mOjo\u001b[0m \u001b[0mque\u001b[0m \u001b[0mesto\u001b[0m \u001b[0maumenta\u001b[0m \u001b[0mmuchisimo\u001b[0m \u001b[0mel\u001b[0m \u001b[0mespacio\u001b[0m \u001b[0mde\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m '''\n\u001b[1;32m----> 7\u001b[1;33m ngram_vectorizer = CountVectorizer(binary=True,\n\u001b[0m\u001b[0;32m      8\u001b[0m                                    ngram_range=(1, 2))\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Puede ser bigramas si ngram_range=(2,2), o trigramas (3,3)...\n",
    "Algunas palabras las elimina, como 'a'. Cuidado con eso a la hora de hacer el conteo\n",
    "ngram_range=(1, 3) significa las palabras por separado, los bigramas y los trigramas\n",
    "Ojo que esto aumenta muchisimo el espacio de features\n",
    "'''\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                   ngram_range=(1, 2))\n",
    "\n",
    "vector = ngram_vectorizer.fit_transform([sentence]).toarray()\n",
    "print(vector)\n",
    "print(len(vector[0])+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.889\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Va como argumento del CountVectorizer\n",
    "'''\n",
    "ngram_vectorizer = CountVectorizer(binary=True, stop_words = english_stop_words,\n",
    "                                   ngram_range=(1, 2))\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 1865232)\n",
      "Diff X normal y X tras lematizador y vectorización: -1778169\n"
     ]
    }
   ],
   "source": [
    "# Añade 1448047 n gramas. Cuanto mas ngramas, mayor será el espacio de features\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 1778314)\n",
      "Diff X normal y X tras lematizador y vectorización: -1691251\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Va como argumento del CountVectorizer\n",
    "'''\n",
    "ngram_vectorizer = CountVectorizer(binary=True, stop_words = english_stop_words,\n",
    "                                   ngram_range=(2, 2))\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])\n",
    "\n",
    "train_model(X, X_test)\n",
    "# Esto da 0.84056, como podemos ver la predicción por bigramas es menos potente"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "Otra forma habitual de representar cada documento de un corpus es utilizar el estadístico tf-idf (frecuencia de términos-frecuencia inversa de documentos) para cada palabra, que es un factor de ponderación que podemos utilizar en lugar de las representaciones binarias o de recuento de palabras.\n",
    "\n",
    "Hay varias formas de realizar la transformación tf-idf, pero en pocas palabras, **tf-idf pretende representar el número de veces que una palabra dada aparece en un documento (una crítica de cine en nuestro caso) en relación con el número de documentos del corpus en los que aparece la palabra**.\n",
    "\n",
    "**Nota: Ahora que ya hemos hablado de los n-gramas, cuando hablo de \"palabras\" me refiero a cualquier n-grama (secuencia de palabras) si el modelo utiliza un n mayor que uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2876820724517808"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "ln(N + 1 / count + 1) + 1\n",
    "Cuanto más común, menor es el TDFIDF. Cuanto más rara, mayor es el valor\n",
    "'''\n",
    "# Numero de documentos\n",
    "N = 3\n",
    "\n",
    "# Numero de veces que aparece\n",
    "count = 2\n",
    "\n",
    "1 + np.log((N + 1)/(count + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.69314718 1.28768207 1.69314718 1.69314718 1.        ]\n",
      "['fat', 'is', 'my', 'name', 'ralph']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# TfidfTransformer\n",
    "'''\n",
    "Cuanto más común, más bajo es el TfidfVectorizer\n",
    "'''\n",
    "sent1 = 'My name is Ralph'\n",
    "sent2 = 'Ralph is fat'\n",
    "sent3 = 'Ralph'\n",
    "\n",
    "test = TfidfVectorizer()\n",
    "test.fit_transform([sent1, sent2, sent3])\n",
    "print(test.idf_)\n",
    "print(test.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_30444\\3626992197.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 1 + ln(N + 1 / count + 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# 1 + ln(N + 1 / count + 1)\n",
    "1 + np.log((3 + 1)/(1 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.8822\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_vectorizer.fit(reviews_train_clean)\n",
    "X = tfidf_vectorizer.transform(reviews_train_clean)\n",
    "X_test = tfidf_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model(X, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 87063)\n",
      "(25000, 87063)\n",
      "Diff X normal y X tras lematizador y vectorización: 0\n"
     ]
    }
   ],
   "source": [
    "print(X_baseline.shape)\n",
    "print(X.shape)\n",
    "print(\"Diff X normal y X tras lematizador y vectorización:\", X_baseline.shape[1] - X.shape[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)\n",
    "\n",
    "Recordemos que los clasificadores lineales tienden a funcionar bien en conjuntos de datos muy dispersos (como el que tenemos). Otro algoritmo que puede producir grandes resultados con un tiempo de entrenamiento rápido son las máquinas de vectores de soporte con un núcleo lineal.\n",
    "\n",
    "Construya un modelo con un rango de n-gramas de 1 a 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.89768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# SVM con bigramas\n",
    "ngram_vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "\n",
    "\n",
    "def train_model_svm(X_TRAIN, X_TEST):\n",
    "    \n",
    "    svm = LinearSVC()\n",
    "    \n",
    "    params = {\n",
    "        'C': [0.01, 0.05, 0.25, 0.5, 1]\n",
    "    }\n",
    "    \n",
    "    grid = GridSearchCV(svm, params, cv=5)\n",
    "    grid.fit(X_TRAIN, target)\n",
    "\n",
    "    print (\"Final Accuracy: %s\" % accuracy_score(target, grid.best_estimator_.predict(X_TEST)))\n",
    "    \n",
    "\n",
    "train_model_svm(X, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo final\n",
    "\n",
    "La eliminación de un pequeño conjunto de palabras vacías junto con un rango de n-gramas de 1 a 3 y un clasificador lineal de vectores de soporte muestra los mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.89924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "\n",
    "stop_words = ['in', 'of', 'at', 'a', 'the']\n",
    "ngram_vectorizer = CountVectorizer(binary=True,\n",
    "                                   ngram_range=(1, 2),\n",
    "                                   stop_words=stop_words)\n",
    "\n",
    "ngram_vectorizer.fit(reviews_train_clean)\n",
    "X = ngram_vectorizer.transform(reviews_train_clean)\n",
    "X_test = ngram_vectorizer.transform(reviews_test_clean)\n",
    "\n",
    "train_model_svm(X, X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principales features positivas y negativas\n",
    "\n",
    "Obtener las características más importantes del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87063\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "cv.fit(reviews_train_clean)\n",
    "X = cv.transform(reviews_train_clean)\n",
    "\n",
    "log_reg = LogisticRegression(C=0.5)\n",
    "log_reg.fit(X, target)\n",
    "\n",
    "# Importancia de los coeficientes. En total, todas las palabras vectorizadas\n",
    "print(len(log_reg.coef_[0]))\n",
    "\n",
    "# Cada coeficiente va asociado a una palabra\n",
    "cv.get_feature_names()\n",
    "\n",
    "# Montamos un diccionario con palabra -> coeficiente\n",
    "feature_to_coef = {\n",
    "    word: coef for word, coef in zip(\n",
    "        cv.get_feature_names(), log_reg.coef_[0]\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.predict(cv.transform(['This movie is horrible']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg.predict(cv.transform(['This movie is incredible']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('excellent', 1.3792554670979043)\n",
      "('refreshing', 1.2812203080059015)\n",
      "('perfect', 1.200895573911889)\n",
      "('superb', 1.1359361912434265)\n",
      "('appreciated', 1.1341593639571255)\n",
      "################################\n",
      "('worst', -2.0614270242430086)\n",
      "('waste', -1.9173223247656896)\n",
      "('disappointment', -1.676682059289742)\n",
      "('poorly', -1.6573838956395175)\n",
      "('awful', -1.5339854389777068)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for best_positive in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True)[:5]:\n",
    "    print(best_positive)\n",
    "    \n",
    "print('################################')\n",
    "for best_negative in sorted(\n",
    "    feature_to_coef.items(), \n",
    "    key=lambda x: x[1])[:5]:\n",
    "    print(best_negative)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ec2a379ed5c25334a484232182c9d38ef8bd9861e2542d0c517568c4f99a9a7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
