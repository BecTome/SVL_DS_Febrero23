{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc683fd1-c23d-418a-afd8-9eff488e4df1",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8fa386-51c8-4328-aac5-6751d1168c67",
   "metadata": {},
   "source": [
    "Seguramente hayais oído hablar de Inteligencia Artificial jugando juegos de ordenador por si solos,   \n",
    "un ejemplo muy popular el de Deepmind. Deepmind alcanzó la fama cuando su programita AlphaGo venció  \n",
    "al campeón del mundo del Go en 2016. Ha habido muchos intentos exitosos en el pasado de desarrollar  \n",
    "agentes con el objetivo de jugar juegos de Atari como Breakout, Pong y Space Invaders.  \n",
    "https://www.youtube.com/watch?v=rOL6QJdAlm8&ab_channel=SciNews\n",
    "\n",
    "Cada uno de estos programas sigue un paradigma de Machine learning conocido como Reinforcement Learning,  \n",
    "si nunca has oído hablar de este término, la siguiente analogía es muy práctica y sencilla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b0410-72e2-4721-ba00-a8edb4bf716d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d30df280-973a-4d2a-8acb-b41c5212fd21",
   "metadata": {},
   "source": [
    "## Reinforcement Learning Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9624c80-e9cc-4043-b87f-543fec42cfc0",
   "metadata": {},
   "source": [
    "Considera el escenario de enseñar a un perro nuevos trucos. El perro no entiende nuestro idioma, no le podemos decir   \n",
    "simplemente que lo haga, sin embargo, iniciamos una estrategia. Emulamos una situación, y el perro intenta responder   \n",
    "de distintas formas. Si el perro responde de la forma adecuada, le recompensamos con chuches. Adivina que, la próxima  \n",
    "vez que el perro se exponga a la misma situación, el perro ejecutará una acción similar con entusiasmo esperando más   \n",
    "chuches. Ese es el aprendizaje de refuerzo positivo. De forma similar, el perro aprenderá a que no hacer si se enfrenta  \n",
    "a experiencias negativas.\n",
    "\n",
    "Esto es exactamente como funciona Reinforcement Learning a un alto nivel:\n",
    "\n",
    "- Tu perro es un 'agente' que se expone al **ambiente**. El ambiente puede ser tu casa, contigo\n",
    "\n",
    "- Las situaciones a las que se encuentran es el análogo al **estado**. Un ejemplo de estado podría ser tu perro sentado y tu \n",
    "  usando una palabra o un cierto tono en el salon de tu casa.\n",
    "  \n",
    "- Nuestros agentes reaccionan realizando una **acción** para pasar de un 'estado' a otro 'estado', tu perro pasa de estar de pie,\n",
    "  a estar sentado, por ejemplo.\n",
    "  \n",
    "- Tras la transición entre estados, ellos pueden recibir una **recompensa** o una **penalización**. Chuches o un NO!\n",
    "\n",
    "- La **póliza** es la estrategia de elegir la acción dado un estado que espere el mejor de los resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8acaef9-d859-4326-ad11-4b784649902e",
   "metadata": {},
   "source": [
    "Reinforcement Learning lies between the spectrum of Supervised Learning and Unsupervised Learning, and there's a few important things to note:\n",
    "\n",
    "1. **Being greedy doesn't always work**  \n",
    "    There are things that are easy to do for instant gratification, and there's things that provide long term rewards The goal is to not be greedy by looking for the quick immediate rewards, but instead to optimize for maximum rewards over the whole training.\n",
    "\n",
    "2. **Sequence matters in Reinforcement Learning**  \n",
    "    The reward agent does not just depend on the current state, but the entire history of states. Unlike supervised and unsupervised learning, time is important here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e807167-503f-48cb-94ea-12ce4ae43c48",
   "metadata": {},
   "source": [
    "In a way, Reinforcement Learning is the science of making optimal decisions using experiences. Breaking it down, the process of Reinforcement Learning involves these simple steps:\n",
    "\n",
    "1. Observation of the environment\n",
    "2. Deciding how to act using some strategy\n",
    "3. Acting accordingly\n",
    "4. Receiving a reward or penalty\n",
    "5. Learning from the experiences and refining our strategy\n",
    "6. Iterate until an optimal strategy is found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cc6220-55ef-4cc4-b189-66598d0b2a6b",
   "metadata": {},
   "source": [
    "# Example Design: Self-Driving Cab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9ff221-9dcb-483e-9664-274af8a2fd60",
   "metadata": {},
   "source": [
    "Let's design a simulation of a self-driving cab. The major goal is to demonstrate, in a simplified environment, how you can use RL techniques to develop an efficient and safe approach for tackling this problem.\n",
    "\n",
    "The Smartcab's job is to pick up the passenger at one location and drop them off in another. Here are a few things that we'd love our Smartcab to take care of:\n",
    "\n",
    "- Drop off the passenger to the right location.  \n",
    "- Save passenger's time by taking minimum time possible to drop off  \n",
    "- Take care of passenger's safety and traffic rules  \n",
    "\n",
    "\n",
    "There are different aspects that need to be considered here while modeling an RL solution to this problem: rewards, states, and actions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32039d28-90e0-4897-bd8b-386958f1e03d",
   "metadata": {},
   "source": [
    "# 1. Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e330ead6-f6f0-47bb-9bbc-593919152b9f",
   "metadata": {},
   "source": [
    "Since the agent (the imaginary driver) is reward-motivated and is going to learn how to control the cab by trial experiences in the environment, we need to decide the **rewards** and/or **penalties** and their magnitude accordingly. Here a few points to consider:\n",
    "\n",
    "- The agent should receive a high positive reward for a successful dropoff because this behavior is highly desired\n",
    "- The agent should be penalized if it tries to drop off a passenger in wrong locations\n",
    "- The agent should get a slight negative reward for not making it to the destination after every time-step. \"Slight\" negative    because we would prefer our agent to reach late instead of making wrong moves trying to reach to the destination as fast as possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced78a04-e10f-4d01-b7db-d9d1e51126f8",
   "metadata": {},
   "source": [
    "# 2. State Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e696965-2730-456b-bbb3-03f3600e3a92",
   "metadata": {},
   "source": [
    "In Reinforcement Learning, the agent encounters a state, and then takes action according to the state it's in.\n",
    "\n",
    "The State Space is the set of all possible situations our taxi could inhabit. The state should contain useful information the agent needs to make the right action.\n",
    "\n",
    "Let's say we have a training area for our Smartcab where we are teaching it to transport people in a parking lot to four different locations (R, G, Y, B):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff54f537-361a-481e-91f4-04d5422c9018",
   "metadata": {},
   "source": [
    "<img src=\"yellowcabs.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25922d8b-dfd3-4468-be4b-3bcff32f93c1",
   "metadata": {},
   "source": [
    "Let's assume Smartcab is the only vehicle in this parking lot. We can break up the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. These 25 locations are one part of our state space. Notice the current location state of our taxi is coordinate (3, 1).\n",
    "\n",
    "You'll also notice there are four (4) locations that we can pick up and drop off a passenger: R, G, Y, B or [(0,0), (0,4), (4,0), (4,3)] in (row, col) coordinates. Our illustrated passenger is in location Y and they wish to go to location R.\n",
    "\n",
    "When we also account for one (1) additional passenger state of being inside the taxi, we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; there's four (4) destinations and five (4 + 1) passenger locations.\n",
    "\n",
    "So, our taxi environment has 5 x 5 x 5 x 4 = 500 total possible states.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b617ff3a-bf3c-4e55-8147-6536565f177f",
   "metadata": {},
   "source": [
    "# 3. Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13504141-381e-42f2-a7a8-228197132e6d",
   "metadata": {},
   "source": [
    "The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.\n",
    "\n",
    "In other words, we have six possible actions:\n",
    "\n",
    "- south\n",
    "- north\n",
    "- east\n",
    "- west\n",
    "- pickup\n",
    "- dropoff  \n",
    "\n",
    "\n",
    "This is the action space: the set of all the actions that our agent can take in a given state.\n",
    "\n",
    "You'll notice in the illustration above, that the taxi cannot perform certain actions in certain states due to walls. In environment's code, we will simply provide a -1 penalty for every wall hit and the taxi won't move anywhere. This will just rack up penalties causing the taxi to consider going around the wall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e5235e-65e0-49b5-8307-e8c8c5b76ea1",
   "metadata": {},
   "source": [
    "# Implementation with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1654b8c0-fc2b-4ef6-99b1-464fa4bb8a1c",
   "metadata": {},
   "source": [
    "Fortunately, OpenAI Gym has this exact environment already built for us.\n",
    "\n",
    "Gym provides different game environments which we can plug into our code and test an agent. The library takes care of API for providing all the information that our agent would require, like possible actions, score, and current state. We just need to focus just on the algorithm part for our agent.\n",
    "\n",
    "We'll be using the Gym environment called Taxi-V2, which all of the details explained above were pulled from. The objectives, rewards, and actions are all the same.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36176bd7-929d-427a-bd3e-86540982af0f",
   "metadata": {},
   "source": [
    "# Gym's interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c8a007-2ab3-49a1-b6f2-4d4342bf81b0",
   "metadata": {},
   "source": [
    "We need to install gym first. Executing the following in a Jupyter notebook should work:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83635bb6-ea05-485d-9373-141d99311e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting cmake\n",
      "  Downloading cmake-3.22.5-py2.py3-none-macosx_10_10_universal2.macosx_10_10_x86_64.macosx_11_0_arm64.macosx_11_0_universal2.whl (75.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 75.2 MB 39.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting gym[atari]\n",
      "  Downloading gym-0.24.1.tar.gz (696 kB)\n",
      "\u001b[K     |████████████████████████████████| 696 kB 44.2 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/anaconda3/lib/python3.8/site-packages (1.6.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym[atari]) (1.20.1)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Downloading gym_notices-0.0.7-py3-none-any.whl (2.7 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.8/site-packages (from gym[atari]) (1.6.0)\n",
      "Collecting importlib-metadata>=4.8.0\n",
      "  Downloading importlib_metadata-4.11.4-py3-none-any.whl (18 kB)\n",
      "Collecting ale-py~=0.7.5\n",
      "  Downloading ale_py-0.7.5-cp38-cp38-macosx_10_15_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 51.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting importlib-resources\n",
      "  Downloading importlib_resources-5.8.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym[atari]) (3.4.1)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.24.1-py3-none-any.whl size=793142 sha256=29cd3ccd5f4c4713ccd7071ab29b361a2756ab38c51ca6585dd72520ba38ecae\n",
      "  Stored in directory: /Users/juliobravo/Library/Caches/pip/wheels/5a/e9/0b/5536e77ed2edbbf067ecff287ec039633d40daee4d8dac7716\n",
      "Successfully built gym\n",
      "Installing collected packages: importlib-resources, importlib-metadata, gym-notices, gym, ale-py, cmake\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 3.10.0\n",
      "    Uninstalling importlib-metadata-3.10.0:\n",
      "      Successfully uninstalled importlib-metadata-3.10.0\n",
      "Successfully installed ale-py-0.7.5 cmake-3.22.5 gym-0.24.1 gym-notices-0.0.7 importlib-metadata-4.11.4 importlib-resources-5.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install cmake 'gym[atari]' scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc838097-f9dc-4bf4-af8d-258d6f717ff6",
   "metadata": {},
   "source": [
    "Once installed, we can load the game environment and render what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "affa5e1b-fce4-4534-af79-45f768a24b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py:564: UserWarning: \u001b[33mWARN: The environment Taxi-v2 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "DeprecatedEnv",
     "evalue": "Environment version v2 for `Taxi` is deprecated. Please use `Taxi-v3` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeprecatedEnv\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-903c850ea7d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Taxi-v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, max_episode_steps, autoreset, disable_env_checker, **kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mspec_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0m_check_version_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No registered env with id: {id}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_check_version_exists\u001b[0;34m(ns, name, version)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlatest_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlatest_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         raise error.DeprecatedEnv(\n\u001b[0m\u001b[1;32m    242\u001b[0m             \u001b[0;34mf\"Environment version v{version} for `{get_env_id(ns, name, None)}` is deprecated. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0;34mf\"Please use `{latest_spec.id}` instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDeprecatedEnv\u001b[0m: Environment version v2 for `Taxi` is deprecated. Please use `Taxi-v3` instead."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f75294-02e8-43dd-abdb-26538de2bdc7",
   "metadata": {},
   "source": [
    "# Intro to Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b00cc0-a6ac-491e-be5f-e2be51bb3ba3",
   "metadata": {},
   "source": [
    "Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
    "\n",
    "In our Taxi environment, we have the reward table, P, that the agent will learn from. It does thing by looking receiving a reward for taking an action in the current state, then updating a Q-value to remember if that action was beneficial.\n",
    "\n",
    "The values store in the Q-table are called a Q-values, and they map to a (state, action) combination.\n",
    "\n",
    "A Q-value for a particular state-action combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.\n",
    "\n",
    "For example, if the taxi is faced with a state that includes a passenger at its current location, it is highly likely that the Q-value for pickup is higher when compared to other actions, like dropoff or north.\n",
    "\n",
    "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5eacd85-680e-45e0-822f-d444191eedc5",
   "metadata": {},
   "source": [
    "<math xmlns=\"http://www.w3.org/1998/Math/MathML\" display=\"block\">\n",
    "  <mi>Q</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mstyle mathsize=\"0.85em\">\n",
    "      <mi>state</mi>\n",
    "    </mstyle>\n",
    "  </mrow>\n",
    "  <mo>,</mo>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mstyle mathsize=\"0.85em\">\n",
    "      <mi>action</mi>\n",
    "    </mstyle>\n",
    "  </mrow>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo stretchy=\"false\">&#x2190;</mo>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mn>1</mn>\n",
    "  <mo>&#x2212;</mo>\n",
    "  <mi>&#x3B1;</mi>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mi>Q</mi>α γ\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mstyle mathsize=\"0.85em\">\n",
    "      <mi>state</mi>\n",
    "    </mstyle>\n",
    "  </mrow>\n",
    "  <mo>,</mo>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mstyle mathsize=\"0.85em\">\n",
    "      <mi>action</mi>\n",
    "    </mstyle>\n",
    "  </mrow>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mo>+</mo>\n",
    "  <mi>&#x3B1;</mi>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mo minsize=\"1.623em\" maxsize=\"1.623em\">(</mo>\n",
    "  </mrow>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mstyle mathsize=\"0.85em\">\n",
    "      <mi>reward</mi>\n",
    "    </mstyle>\n",
    "  </mrow>\n",
    "  <mo>+</mo>\n",
    "  <mi>&#x3B3;</mi>\n",
    "  <munder>\n",
    "    <mo data-mjx-texclass=\"OP\" movablelimits=\"true\">max</mo>\n",
    "    <mrow data-mjx-texclass=\"ORD\">\n",
    "      <mi>a</mi>\n",
    "    </mrow>\n",
    "  </munder>\n",
    "  <mi>Q</mi>\n",
    "  <mo stretchy=\"false\">(</mo>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mstyle mathsize=\"0.85em\">\n",
    "      <mi>next</mi>\n",
    "      <mtext>&#xA0;</mtext>\n",
    "      <mi>state</mi>\n",
    "    </mstyle>\n",
    "  </mrow>\n",
    "  <mo>,</mo>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mstyle mathsize=\"0.85em\">\n",
    "      <mi>all</mi>\n",
    "      <mtext>&#xA0;</mtext>\n",
    "      <mi>actions</mi>\n",
    "    </mstyle>\n",
    "  </mrow>\n",
    "  <mo stretchy=\"false\">)</mo>\n",
    "  <mrow data-mjx-texclass=\"ORD\">\n",
    "    <mo minsize=\"1.623em\" maxsize=\"1.623em\">)</mo>\n",
    "  </mrow>\n",
    "</math>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3db5ace-9f1f-4284-b6ef-8ba2b4df4b00",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "-  α  (alpha) is the learning rate (0 < α ≤  1) - Just like in supervised learning settings,  is the extent to which our Q-values are being updated in every iteration.\n",
    "\n",
    "-  γ (gamma) is the discount factor (0 < γ ≤  1) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to 1) captures the long-term effective award, whereas, a discount factor of 0 makes our agent consider only immediate reward, hence making it greedy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c83d4f-0980-4d50-a509-c3ffe160298e",
   "metadata": {},
   "source": [
    "What is this saying?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9374bf0f-5bce-4d68-8afd-1696e80a4bf8",
   "metadata": {},
   "source": [
    "We are assigning (<-), or updating, the Q-value of the agent's current state and action by first taking a weight (1 - α) of the old Q-value, then adding the learned value. The learned value is a combination of the reward for taking the current action in the current state, and the discounted maximum reward from the next state we will be in once we take the current action.\n",
    "\n",
    "Basically, we are learning the proper action to take in the current state by looking at the reward for the current state/action combo, and the max rewards for the next state. This will eventually cause our taxi to consider the route with the best rewards strung together.\n",
    "\n",
    "The Q-value of a state-action pair is the sum of the instant reward and the discounted future reward (of the resulting state). The way we store the Q-values for each state and action is through a **Q-table**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2ac9b-998c-4986-9737-6c434d60bcee",
   "metadata": {},
   "source": [
    "## Q-table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ad4b7-f15a-40e9-9e0c-6e8af89a5888",
   "metadata": {},
   "source": [
    "The Q-table is a matrix where we have a row for every state (500) and a column for every action (6). It's first initialized to 0, and then values are updated after training. Note that the Q-table has the same dimensions as the reward table, but it has a completely different purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a3e23-b893-4662-8e76-331151ac0068",
   "metadata": {},
   "source": [
    "# Summing up the Q-Learning Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9968a-3d21-4ce2-a3ac-948e5cc9e7c9",
   "metadata": {},
   "source": [
    "Breaking it down into steps, we get\n",
    "\n",
    "- Initialize the Q-table by all zeros.\n",
    "- Start exploring actions: For each state, select any one among all possible actions for the current state (S).\n",
    "- Travel to the next state (S') as a result of that action (a).\n",
    "- For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "- Update Q-table values using the equation.\n",
    "- Set the next state as the current state.\n",
    "- If goal state is reached, then end and repeat the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d43492-778d-4b1d-9ec6-284cb7271a2e",
   "metadata": {},
   "source": [
    "### Exploiting learned values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a42379e-c694-47b2-a012-8c76792703ab",
   "metadata": {},
   "source": [
    "After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.\n",
    "\n",
    "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called  ε \"epsilon\" to cater to this during training.\n",
    "\n",
    "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8713728a-2fbc-4ffa-982f-4ba2f7c89702",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d94f8d7-d57f-421d-91b9-c041b7a29ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
