{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data: Flattening, Filtering, and Chunking\n",
    "\n",
    "What would you do if you were designing an algorithm to analyze the following paragraph of text?\n",
    "\n",
    "    Emma knocked on the door. No answer. She knocked again and waited. There was a large maple tree next to the house. Emma looked up the tree and saw a giant raven perched at the treetop. Under the afternoon sun, the raven gleamed magnificently. Its beak was hard and pointed, its claws sharp and strong. It looked regal and imposing. It reigned the tree it stood on. The raven was looking straight at Emma with its beady black eyes. Emma felt slightly intimidated. She took a step back from the door and tentatively said, “Hello?”\n",
    "\n",
    "The paragraph contains a lot of information. We know that it involves someone named Emma and a raven. There is a house and a tree, and Emma is trying to get into the house but sees the raven instead. The raven is magnificent and has noticed Emma, who is a little scared but is making an attempt at communication.\n",
    "\n",
    "So, which parts of this trove of information are salient features that we should extract? To start with, it seems like a good idea to extract the names of the main characters, Emma and the raven. Next, it might also be good to note the setting of a house, a door, and a tree. And what about the descriptions of the raven? What about Emma’s actions—knocking on the door, taking a step back, and saying hello?\n",
    "\n",
    "This chapter introduces the basics of feature engineering for text. We start out with bag-of-words, which is the simplest representation based on word count statistics. A very much related transformation is tf-idf, which is essentially a feature scaling technique. It is pulled out into its own chapter (the next one) for a full discussion. The current chapter first talks about text extraction features, then delves into how to filter and clean those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-X: Turning Natural Text into Flat Vectors\n",
    "\n",
    "Whether constructing machine learning models or engineering features, it’s nice when the result is simple and interpretable. Simple things are easy to try, and interpretable features and models are easier to debug than complex ones. Simple and interpretable features do not always lead to the most accurate model, but it’s a good idea to start simple and only add complexity when absolutely necessary.\n",
    "For text data, we can start with a list of word count statistics called a bag-of-words. A list of word counts makes no special effort to find the interesting entities, such as Emma or the raven. But those two words are repeatedly mentioned in our sample paragraph, and they have a higher count than a random word like “hello.” For simple tasks such as classifying a document, word count statistics often suffice. This technique can also be used in information retrieval, where the goal is to retrieve the set of documents that are relevant to an input text query. Both tasks are well served by word-level features because the presence or absence of certain words is a great indicator of the topic content of the document.\n",
    "\n",
    "Bag-of-Words\n",
    "\n",
    "In bag-of-words (BoW) featurization, a text document is converted into a vector of counts. (A vector is just a collection of n numbers.) The vector contains an entry for every possible word in the vocabulary. If the word—say, “aardvark”—appears three times in the document, then the feature vector has a count of 3 in the position corresponding to that word. If a word in the vocabulary doesn’t appear in the document, then it gets a count of 0. For example, the text “it is a puppy and it is extremely cute” has the BoW representation shown in Figure 3-1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " <img src=\"feml_0301.png\" height=\"360\" width=\"360\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words converts a text document into a flat vector. It is “flat” because it doesn’t contain any of the original textual structures. The original text is a sequence of words. But a bag-of-words has no sequence; it just remembers how many times each word appears in the text. Thus, as Figure 3-2 demonstrates, the ordering of words in the vector is not important, as long as it is consistent for all documents in the dataset. Neither does bag-of-words represent any concept of word hierarchy. For example, the concept of “animal” includes “dog,” “cat,” “raven,” etc. But in a bag-of-words representation, these words are all equal elements of the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " <img src=\"feml_0302.png\" height=\"360\" width=\"360\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is important here is the geometry of data in feature space. In a bag-of-words vector, each word becomes a dimension of the vector. If there are n words in the vocabulary, then a document becomes a point1 in n-dimensional space. It is difficult to visualize the geometry of anything beyond two or three dimensions, so we will have to use our imagination. Figure 3-3 shows what our example sentence looks like in the two-dimensional feature space corresponding to the words “puppy” and “cute.”\n",
    "\n",
    "<img src=\"feml_0303.png\" height=\"360\" width=\"360\"> \n",
    "\n",
    "Figure 3-4 shows three sentences in a 3D space corresponding to the words “puppy,” “extremely,” and “cute.”\n",
    "\n",
    "<img src=\"feml_0304.png\" height=\"360\" width=\"360\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These figures both depict data vectors in feature space. The axes denote individual words, which are features in the bag-of-words representation, and the points in space denote data points (text documents). Sometimes it is also informative to look at feature vectors in data space. A feature vector contains the value of the feature in each data point. The axes denote individual data points, and the points denote feature vectors. Figure 3-5 shows an example. With bag-of-words featurization for text documents, a feature is a word, and a feature vector contains the counts of this word in each document. In this way, a word is represented as a “bag-of-documents.”  As we shall see in Chapter 4, these bag-of-documents vectors come from the matrix transpose of the bag-of-words vectors.\n",
    "\n",
    "<img src=\"feml_0305.png\" height=\"360\" width=\"360\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag-of-words is not perfect. Breaking down a sentence into single words can destroy the semantic meaning. For instance, “not bad” semantically means “decent” or even “good” (especially if you’re British). But “not” and “bad” constitute a floating negation plus a negative sentiment. “toy dog” and “dog toy” could be very different things (unless it’s a dog toy of a toy dog), and the meaning is lost with the singleton words “toy” and “dog.” It’s easy to come up with many such examples. Bag-of-n-Grams, which we discuss next, alleviates some of the issue but is not a fundamental fix. It’s good to keep in mind that bag-of-words is a simple and useful heuristic, but it is far from a correct semantic understanding of text.\n",
    "\n",
    "## Bag-of-n-Grams\n",
    "\n",
    "Bag-of-n-Grams, or bag-of-n-grams, is a natural extension of bag-of-words. An n-gram is a sequence of n tokens. A word is essentially a 1-gram, also known as a unigram. After tokenization, the counting mechanism can collate individual tokens into word counts, or count overlapping sequences as n-grams. For example, the sentence “Emma knocked on the door” generates the n-grams “Emma knocked,” “knocked on,” “on the,” and “the door.”\n",
    "\n",
    "n-grams retain more of the original sequence structure of the text, and therefore the bag-of-n-grams representation can be more informative. However, this comes at a cost. Theoretically, with k unique words, there could be k2 unique 2-grams (also called bigrams). In practice, there are not nearly so many, because not every word can follow every other word. Nevertheless, there are usually a lot more distinct n-grams (n > 1) than words. This means that bag-of-n-grams is a much bigger and sparser feature space. It also means that n-grams are more expensive to compute, store, and model. The larger n is, the richer the information, and the greater the cost.\n",
    "\n",
    "To illustrate how the number of n-grams grows with increasing n (see Figure 3-6), let’s compute n-grams on the Yelp reviews dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_f = open('data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json')\n",
    "biz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()])\n",
    "biz_f.close()\n",
    "biz_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first 10,000 reviews\n",
    "f = open('data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json')\n",
    "js = []\n",
    "for i in range(10000):\n",
    "    js.append(json.loads(f.readline()))\n",
    "f.close()\n",
    "review_df = pd.DataFrame(js)\n",
    "review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_converter = CountVectorizer(token_pattern='(?u)\\\\b\\\\w+\\\\b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = bow_converter.fit_transform(review_df['text'])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = bow_converter.get_feature_names()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_converter = CountVectorizer(ngram_range=(2,2), token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "x2 = bigram_converter.fit_transform(review_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = bigram_converter.get_feature_names()\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_converter = CountVectorizer(ngram_range=(3,3), token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "x3 = trigram_converter.fit_transform(review_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams = trigram_converter.get_feature_names()\n",
    "len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigrams[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(words), len(bigrams), len(trigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "counts = [len(words), len(bigrams), len(trigrams)]\n",
    "plt.plot(counts, color='cornflowerblue')\n",
    "plt.plot(counts, 'bo')\n",
    "plt.margins(0.1)\n",
    "plt.xticks(range(3), ['unigram', 'bigram', 'trigram'])\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.title('Number of ngrams in the first 10,000 reviews of the Yelp dataset', {'fontsize':16})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for Cleaner Features\n",
    "\n",
    "With words, how do we cleanly separate the signal from the noise? Through filtering, techniques that use raw tokenization and counting to generate lists of simple words or n-grams become more usable. Phrase detection, which we will discuss next, can be seen as a particular bigram filter. Here are a few more ways to perform filtering.\n",
    "\n",
    "### Stopwords\n",
    "\n",
    "Classification and retrieval do not usually require an in-depth understanding of the text. For instance, in the sentence “Emma knocked on the door,” the words “on” and “the” don’t change the fact that this sentence is about a person and a door. For coarse-grained tasks such as classification, the pronouns, articles, and prepositions may not add much value. The case may be very different in sentiment analysis, which requires a fine-grained understanding of semantics.\n",
    "\n",
    "The popular Python NLP package NLTK contains a linguist-defined stopword list for many languages. (You will need to install NLTK and run nltk.download() to get all the goodies.) Various stopword lists can also be found on the web. For instance, here are some sample words from the English stopword list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords.words('spanish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the list contains apostrophes, and the words are uncapitalized. In order to use it as is, the tokenization process must not eat up apostrophes, and the words need to be converted to lowercase.\n",
    "\n",
    "## Frequency-Based Filtering\n",
    "\n",
    "Stopword lists are a way of weeding out common words that make for vacuous features. There are other, more statistical ways of getting at the concept of “common words.” In collocation extraction, we see methods that depend on manual definitions, and those that use statistics. The same idea applies to word filtering. We can use frequency statistics here as well.\n",
    "\n",
    "### Frequent words\n",
    "\n",
    "Frequency statistics are great for filtering out corpus-specific common words as well as general-purpose stopwords. For instance, the phrase “New York Times” and each of the individual words in it appear frequently in the New York Times Annotated Corpus dataset. Similarly, the word “house” appears often in the phrase “House of Commons” in the Hansard corpus of Canadian parliament debates, a dataset that is popularly used for statistical machine translation because it contains both an English and a French version of all documents. These words are meaningful in general, but not within those particular corpora. A typical stopword list will catch the general stopwords, but not corpus-specific ones.\n",
    "\n",
    "Looking at the most frequent words can reveal parsing problems and highlight normally useful words that happen to appear too many times in the corpus. For example, Table 3-1 lists the 40 most frequent words in the Yelp reviews dataset. Here, frequency is based on the number of documents (reviews) they appear in, not their count within a document. As we can see, the list includes many stopwords. It also contains some surprises. “s” and “t” are on the list because we used the apostrophe as a tokenization delimiter, and words such as “Mary’s” or “didn’t” got parsed as “Mary s” and “didn t.” Furthermore, the words “good,” “food,” and “great” each appear in around a third of the reviews, but we might want to keep them around because they are very useful for tasks such as sentiment analysis or business categorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it helps to combine frequency-based filtering with a stopword list. There is also the tricky question of where to place the cutoff. Unfortunately there is no universal answer. Most of the time the cutoff needs to be determined manually, and may need to be reexamined when the dataset changes.\n",
    "\n",
    "### Rare words\n",
    "\n",
    "Depending on the task, one might also need to filter out rare words. These might be truly obscure words, or misspellings of common words. To a statistical model, a word that appears in only one or two documents is more like noise than useful information. For example, suppose the task is to categorize businesses based on their Yelp reviews, and a single review contains the word “gobbledygook.” How would one tell, based on this one word, whether the business is a restaurant, a beauty salon, or a bar? Even if we knew that the business in this case happened to be a bar, it would probably be a mistake to classify as such for other reviews that contain the word “gobbledygook.”\n",
    "\n",
    "Not only are rare words unreliable as predictors, they also generate computational overhead. The set of 1.6 million Yelp reviews contains 357,481 unique words (tokenized by space and punctuation characters), 189,915 of which appear in only one review, and 41,162 in two reviews. Over 60% of the vocabulary occurs rarely. This is a so-called heavy-tailed distribution, and it is very common in real-world data. The training time of many statistical machine learning models scales linearly with the number of features, and some models are quadratic or worse. Rare words incur a large computation and storage cost for not much additional gain.\n",
    "\n",
    "Rare words can be easily identified and trimmed based on word count statistics. Alternatively, their counts can be aggregated into a special garbage bin, which can serve as an additional feature. Figure 3-7 demonstrates this representation on a short document that contains a bunch of usual words and two rare words, “gobbledygook” and “zylophant.” The usual words retain their own counts, which can be further filtered by stopword lists or other frequency-based methods. The rare words lose their identity and get grouped into a garbage bin feature.\n",
    "\n",
    "<img src=\"feml_0307.png\" height=\"360\" width=\"360\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since one won’t know which words are rare until the whole corpus has been counted, the garbage bin feature will need to be collected as a post-processing step.\n",
    "\n",
    "Since this book is about feature engineering, our focus is on features. But the concept of rarity also applies to data points. If a text document is very short, then it likely contains no useful information and should not be used when training a model. One must use caution when applying this rule, however. The Wikipedia dump contains many pages that are incomplete stubs, which are probably safe to filter out. Tweets, on the other hand, are inherently short, and require other featurization and modeling tricks.\n",
    "Stemming\n",
    "\n",
    "One problem with simple parsing is that different variations of the same word get counted as separate words. For instance, “flower” and “flowers” are technically different tokens, and so are “swimmer,” “swimming,” and “swim,” even though they are very close in meaning. It would be nice if all of these different variations got mapped to the same word.\n",
    "\n",
    "Stemming is an NLP task that tries to chop each word down to its basic linguistic word stem form. There are different approaches. Some are based on linguistic rules, others on observed statistics. A subclass of algorithms incorporate part-of-speech tagging and linguistic rules in a process known as lemmatization.\n",
    "\n",
    "Most stemming tools focus on the English language, though efforts are ongoing for other languages. The Porter stemmer is the most widely used free stemming tool for the English language. The original program is written in ANSI C, but many other packages have since wrapped it to provide access to other languages.\n",
    "\n",
    "Here is an example of running the Porter stemmer through the NLTK Python package. As you can see, it handles a large number of cases, but it’s not perfect. The word “goes” is mapped to “goe,” while “go” is mapped to itself:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming does have a computation cost. Whether the end benefit outweighs the cost is application-dependent. It is also worth noting that stemming could hurt more than it helps. The words “new” and “news” have very different meanings, but both would be stemmed to “new.” Similar examples abound. For this reason, stemming is not always used.\n",
    "\n",
    "\n",
    "## Atoms of Meaning: From Words to n-Grams to Phrases\n",
    "\n",
    "The concept of bag-of-words is straightforward. But how does a computer know what a word is? A text document is represented digitally as a string, which is basically a sequence of characters. One might also run into semi-structured text in the form of JSON blobs or HTML pages. But even with the added tags and structure, the basic unit is still a string. How does one turn a string into a sequence of words? This involves the tasks of parsing and tokenization, which we discuss next.\n",
    "\n",
    "### Parsing and Tokenization\n",
    "\n",
    "Parsing is necessary when the string contains more than plain text. For instance, if the raw data is a web page, an email, or a log of some sort, then it contains additional structure. One needs to decide how to handle the markup, the headers and footers, or the uninteresting sections of the log. If the document is a web page, then the parser needs to handle URLs. If it is an email, then fields like From, To, and Subject may require special handling—otherwise these headers will end up as normal words in the final count, which may not be useful.\n",
    "\n",
    "After light parsing, the plain-text portion of the document can go through tokenization. This turns the string—a sequence of characters—into a sequence of tokens. Each token can then be counted as a word. The tokenizer needs to know what characters indicate that one token has ended and another is beginning. Space characters are usually good separators, as are punctuation characters. If the text contains tweets, then hash marks (#) should not be used as separators (also known as delimiters).\n",
    "\n",
    "Sometimes, the analysis needs to operate on sentences instead of entire documents. For instance, n-grams, a generalization of the concept of a word, should not extend beyond sentence boundaries. More complex text featurization methods like word2vec also work with sentences or paragraphs. In these cases, one needs to first parse the document into sentences, then further tokenize each sentence into words.\n",
    "\n",
    "### Collocation Extraction for Phrase Detection\n",
    "\n",
    "\n",
    "A sequence of tokens immediately yields the list of words and n-grams. Semantically speaking, however, we are more used to understanding phrases, not n-grams. In computational natural language processing (NLP), the concept of a useful phrase is called a collocation. In the words of Manning and Schütze (1999: 151), “A collocation is an expression consisting of two or more words that correspond to some conventional way of saying things.”\n",
    "\n",
    "Collocations are more meaningful than the sum of their parts. For instance, “strong tea” has a different meaning beyond “great physical strength” and “tea”; therefore, it is considered a collocation. The phrase “cute puppy,” on the other hand, means exactly the sum of its parts: “cute” and “puppy.” Thus, it is not considered a collocation.\n",
    "\n",
    "Collocations do not have to be consecutive sequences. For example, the sentence “Emma knocked on the door” is considered to contain the collocation “knock door.” Hence, not every collocation is an n-gram. Conversely, not every n-gram is deemed a meaningful collocation.\n",
    "\n",
    "Because collocations are more than the sum of their parts, their meaning cannot be adequately captured by individual word counts. Bag-of-words falls short as a representation. Bag-of-n-grams is also problematic because it captures too many meaningless sequences (consider “this is” in the bag-of-n-grams example) and not enough of the meaningful ones (i.e., knock door).\n",
    "\n",
    "Collocations are useful as features. But how does one discover and extract them from text? One way is to predefine them. If we tried really hard, we could probably find comprehensive lists of idioms in various languages, and we could look through the text for any matches. It would be very expensive, but it would work. If the corpus is very domain specific and contains esoteric lingo, then this might be the preferred method. But the list would require a lot of manual curation, and it would need to be constantly updated for evolving corpora. For example, it probably wouldn’t be very realistic for analyzing tweets, or for blogs and articles.\n",
    "\n",
    "Since the advent of statistical NLP in the last two decades, people have opted more and more for statistical methods for finding phrases. Instead of establishing a fixed list of phrases and idiomatic sayings, statistical collocation extraction methods rely on the ever-evolving data to reveal the popular sayings of the day.\n",
    "\n",
    "### Frequency-based methods\n",
    "\n",
    "A simple hack is to look at the most frequently occurring n-grams. The problem with this approach is that the most frequently occurring ones may not be the most useful ones. Table 3-2 shows the most popular bigrams ( n = 2 ) in the entire Yelp reviews dataset. As we can see, the top 10 most frequently occurring bigrams by document count are very generic terms that don’t contain much meaning.\n",
    "\n",
    "### Hypothesis testing for collocation extraction\n",
    "\n",
    "Raw popularity count is too crude of a measure. We have to find more clever statistics to be able to pick out meaningful phrases easily. The key idea is to ask whether two words appear together more often than they would by chance. The statistical machinery for answering this question is called a hypothesis test.\n",
    "\n",
    "Hypothesis testing is a way to boil noisy data down to “yes” or “no” answers. It involves modeling the data as samples drawn from random distributions. The randomness means that one can never be 100% sure about the answer; there’s always the chance of an outlier. So, the answers are attached to a probability.\n",
    "\n",
    "For example, the outcome of a hypothesis test might be “these two datasets come from the same distribution with 95% probability.” For a gentle introduction to hypothesis testing, see the Khan Academy’s tutorial on Hypothesis Testing and p-Values.\n",
    "\n",
    "In the context of collocation extraction, many hypothesis tests have been proposed over the years.  One of the most successful methods is based on the likelihood ratio test (Dunning, 1993). For a given pair of words, the method tests two hypotheses on the observed dataset. Hypothesis 1 (the null hypothesis) says that word 1 appears independently from word 2. Another way of saying this is that seeing word 1 has no bearing on whether we also see word 2. Hypothesis 2 (the alternate hypothesis) says that seeing word 1 changes the likelihood of seeing word 2. We take the alternate hypothesis to imply that the two words form a common phrase. Hence, the likelihood ratio test for phrase detection (a.k.a. collocation extraction) asks the following question: are the observed word occurrences in a given text corpus more likely to have been generated from a model where the two words occur independently from one another, or a model where the probabilities of the two words are entangled?\n",
    "\n",
    "That is a mouthful. Let’s math it up a little. (Math is great at expressing things very precisely and concisely, but it does require a completely different parser than natural language.)\n",
    "\n",
    "We can express the null hypothesis Hnull (independent) as P(w2 | w1) = P(w2 | not w1), and the alternate hypothesis Halternate (not independent) as P(w2 | w1) ≠ P(w2 | not w1).\n",
    "\n",
    "The final statistic is the log of the ratio between the two:\n",
    "\n",
    "log λ = log L ( Data; H null ) L ( Data; H alternate ) . #formatear\n",
    "\n",
    "The likelihood function L(Data; H) represents the probability of seeing the word frequencies in the dataset under the independent or the not independent model for the word pair. In order to compute this probability, we have to make another assumption about how the data is generated. The simplest data generation model is the binomial model, where for each word in the dataset, we toss a coin, and we insert our special word if the coin comes up heads, and some other word otherwise. Under this strategy, the count of the number of occurrences of the special word follows a binomial distribution. The binomial distribution is completely determined by the total number of words, the number of occurrences of the word of interest, and the heads probability.\n",
    "\n",
    "The algorithm for detecting common phrases through likelihood ratio test analysis proceeds as follows:\n",
    "\n",
    "    Compute occurrence probabilities for all singleton words: P(w).\n",
    "    Compute conditional pairwise word occurrence probabilities for all unique bigrams: P(w2 | w1).\n",
    "    Compute the likelihood ratio log λ for all unique bigrams.\n",
    "    Sort the bigrams based on their likelihood ratio.\n",
    "    Take the bigrams with the smallest likelihood ratio values as features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is another statistical approach that’s based on pointwise mutual information, but it is very sensitive to rare words, which are always present in real-world text corpora. Hence, it is not commonly used and we will not be demonstrating it here.\n",
    "\n",
    "Note that all of the statistical methods for collocation extraction, whether using raw frequency, hypothesis testing, or pointwise mutual information, operate by filtering a list of candidate phrases. The easiest and cheapest way to generate such a list is by counting n-grams. It’s possible to generate nonconsecutive sequences, but they are expensive to compute. In practice, even for consecutive n-grams, people rarely go beyond bigrams or trigrams because there are too many of them, even after filtering. To generate longer phrases, there are other methods such as chunking or combining with part-of-speech (PoS) tagging.\n",
    "\n",
    "\n",
    "### Chunking and part-of-speech tagging\n",
    "\n",
    "Chunking is a bit more sophisticated than finding n-grams, in that it forms sequences of tokens based on parts of speech, using rule-based models.\n",
    "\n",
    "For example, we might be most interested in finding all of the noun phrases in a problem where the entity (in this case the subject of a text) is the most interesting to us. In order to find this, we tokenize each word with a part of speech and then examine the token’s neighborhood to look for part-of-speech groupings, or “chunks.” The models that map words to parts of speech are generally language specific. Several open source Python libraries, such as NLTK, spaCy, and TextBlob, have multiple language models available. \n",
    "\n",
    "To  illustrate how several libraries in Python make chunking using PoS tagging fairly straightforward, let’s use the Yelp reviews dataset again. In Example 3-2, we evaluate the parts of speech to find the noun phrases using both spaCy and TextBlob.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the first 10 reviews\n",
    "f = open('data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json')\n",
    "js = []\n",
    "for i in range(10):\n",
    "    js.append(json.loads(f.readline()))\n",
    "f.close()\n",
    "review_df = pd.DataFrame(js)\n",
    "review_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using spacy: [Installation instructions for spacy](https://spacy.io/docs/usage/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model meta data\n",
    "spacy.info('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preload the language model\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping it in a pandas dataframe\n",
    "doc_df = review_df['text'].apply(nlp)\n",
    "\n",
    "type(doc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(doc_df[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy gives you both fine grained (.pos_) + coarse grained (.tag_) parts of speech    \n",
    "for doc in doc_df[4]:\n",
    "    print(doc.text, doc.pos_, doc.tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spaCy also does noun chunking for us\n",
    "\n",
    "print([chunk for chunk in doc_df[4].noun_chunks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using [Textblob](https://textblob.readthedocs.io/en/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default tagger in TextBlob uses the PatternTagger, the same as [pattern](https://www.clips.uantwerpen.be/pattern), which is fine for our example. To use the NLTK tagger, we can specify the pos_tagger when we call TextBlob. More [here](http://textblob.readthedocs.io/en/dev/advanced_usage.html#advanced)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df = review_df['text'].apply(TextBlob)\n",
    "\n",
    "type(blob_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(blob_df[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_df[4].tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Effects of Feature Scaling: From Bag-of-Words to Tf-Idf\n",
    "\n",
    "A bag-of-words representation is simple to generate but far from perfect. If we count all words equally, then some words end up being emphasized more than we need. Recall our example of Emma and the raven from Chapter 3. We’d like a document representation that emphasizes the two main characters. The words “Emma” and “raven” both appear three times, but “the” appears a whopping eight times, “and” appears five times, and “it” and “was” both appear four times. The main characters do not stand out by simple frequency count alone. This is problematic.\n",
    "\n",
    "It would also be nice to pick out words such as “magnificently,” “gleamed,” “intimidated,” “tentatively,” and “reigned,” because they help to set the overall tone of the paragraph. They indicate sentiment, which can be very valuable information to a data scientist. So, ideally, we’d like a representation that highlights meaningful words.\n",
    "Tf-Idf : A Simple Twist on Bag-of-Words\n",
    "\n",
    "Tf-idf is a simple twist on the bag-of-words approach. It stands for term frequency–inverse document frequency.  Instead of looking at the raw counts of each word in each document in a dataset, tf-idf looks at a normalized count where each word count is divided by the number of documents this word appears in. That is:\n",
    "\n",
    "bow(w, d) = # times word w appears in document d\n",
    "\n",
    "tf-idf(w, d) = bow(w, d) * N / (# documents in which word w appears)\n",
    "\n",
    "N is the total number of documents in the dataset. The fraction N / (# documents ...) is what’s known as the inverse document frequency. If a word appears in many documents, then its inverse document frequency is close to 1. If a word appears in just a few documents, then the inverse document frequency is much higher.\n",
    "\n",
    "Alternatively, we can take a log transform instead using the raw inverse document frequency. Logarithm turns 1 into 0, and makes large numbers (those much greater than 1) smaller. (More on this later.)\n",
    "\n",
    "If we define tf-idf as:\n",
    "\n",
    "tf-idf(w, d) = bow(w, d) * log (N / # documents in which word w appears)\n",
    "\n",
    "then a word that appears in every single document will be effectively zeroed out, and a word that appears in very few documents will have an even larger count than before.\n",
    "\n",
    "Let’s look at some pictures to understand what it’s all about. Figure 4-1 shows a simple example that contains four sentences: “it is a puppy,” “it is a cat,” “it is a kitten,” and “that is a dog and this is a pen.” We plot these sentences in the feature space of three words: “puppy,” “cat,” and “is.”\n",
    "\n",
    " <img src=\"feml_0401.png\" height=\"360\" width=\"360\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s look at the same four sentences in tf-idf representation using the log transform for the inverse document frequency. Figure 4-2 shows the documents in feature space. Notice that the word “is” is effectively eliminated as a feature since it appears in all sentences in this dataset. Also, because they each appear in only one sentence out of the total four, the words “puppy” and “cat” are now counted higher than before (log(4) = 1.38... > 1). Thus, tf-idf makes rare words more prominent and effectively ignores common words. It is closely related to the frequency-based filtering methods in Chapter 3, but much more mathematically elegant than placing hard cutoff thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GridSearchCV function in scikit-learn runs a grid search with cross validation (see Example 4-5). Figure 4-4 shows a box-and-whiskers plot of the distribution of accuracy measurements for models trained on each of the feature sets. The middle line in the box marks the median accuracy, the box itself marks the region between the first and third quartiles, and the whiskers extend to the rest of the distribution.\n",
    "\n",
    "<img src=\"feml_0402.png\" height=\"360\" width=\"360\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It to the Test\n",
    "\n",
    "Tf-idf transforms word count features through multiplication with a constant. Hence, it is an example of feature scaling, a concept introduced in Chapter 2. How well does feature scaling work in practice? Let’s compare the performance of scaled and unscaled features in a simple text classification task. Time for some code!\n",
    "\n",
    "In Example 4-1, we revisit the Yelp reviews dataset. Round 6 of the Yelp dataset challenge contains close to 1.6 million reviews of businesses in six US cities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Classification Dataset\n",
    "\n",
    "Let’s see whether we can use the reviews to categorize a business as either a restaurant or a nightlife venue. To save on training time, we can take a subset of the reviews. In this case, there is a large difference in review count between the two categories. This is called a class-imbalanced dataset. Imbalanced datasets are problematic for modeling because the model will expend most of its effort fitting to the larger class. Since we have plenty of data in both classes, a good way to resolve the problem is to downsample the larger class (restaurants) to be roughly the same size as the smaller class (nightlife). Here is an example workflow:\n",
    "\n",
    "    Take a random sample of 10% of nightlife reviews and 2.1% of restaurant reviews (percentages chosen so the number of examples in each class is roughly equal).\n",
    "\n",
    "    Create a 70/30 train-test split of this dataset. In this example, the training set ends up with 29,264 reviews, and the test set with 12,542 reviews.\n",
    "\n",
    "    The training data contains 46,924 unique words; this is the number of features in the bag-of-words representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection as modsel\n",
    "import sklearn.preprocessing as preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep Yelp reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Yelp Business data\n",
    "biz_f = open('data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json')\n",
    "biz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()])\n",
    "biz_f.close()\n",
    "\n",
    "## Load Yelp Reviews data\n",
    "review_file = open('data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json')\n",
    "review_df = pd.DataFrame([json.loads(x) for x in review_file.readlines()])\n",
    "review_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out only Nightlife and Restaurants businesses\n",
    "two_biz = biz_df[biz_df.apply(lambda x: 'Nightlife' in x['categories'] \n",
    "                                        or 'Restaurants' in x['categories'], \n",
    "                              axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_biz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biz_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join with the reviews to get all reviews on the two types of business\n",
    "twobiz_reviews = two_biz.merge(review_df, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twobiz_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twobiz_reviews.to_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/twobiz_reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twobiz_reviews = pd.read_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/twobiz_reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim away the features we won't use\n",
    "twobiz_reviews = twobiz_reviews[['business_id', \n",
    "                                 'name', \n",
    "                                 'stars_y', \n",
    "                                 'text', \n",
    "                                 'categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the target column--True for Nightlife businesses, and False otherwise\n",
    "twobiz_reviews['target'] = twobiz_reviews.apply(lambda x: 'Nightlife' in x['categories'],\n",
    "                                                axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now pull out each class of reviews separately, \n",
    "## so we can create class-balanced samples for training\n",
    "nightlife = twobiz_reviews[twobiz_reviews.apply(lambda x: 'Nightlife' in x['categories'], axis=1)]\n",
    "restaurants = twobiz_reviews[twobiz_reviews.apply(lambda x: 'Restaurants' in x['categories'], axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nightlife.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nightlife_subset = nightlife.sample(frac=0.1, random_state=123)\n",
    "restaurant_subset = restaurants.sample(frac=0.021, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nightlife_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nightlife_subset.to_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/nightlife_subset.pkl')\n",
    "restaurant_subset.to_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/restaurant_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nightlife_subset = pd.read_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/nightlife_subset.pkl')\n",
    "restaurant_subset = pd.read_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/restaurant_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat([nightlife_subset, restaurant_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['target'] = combined.apply(lambda x: 'Nightlife' in x['categories'],\n",
    "                                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test data sets\n",
    "training_data, test_data = modsel.train_test_split(combined, \n",
    "                                                   train_size=0.7, \n",
    "                                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent the review text as a bag-of-words \n",
    "bow_transform = text.CountVectorizer()\n",
    "X_tr_bow = bow_transform.fit_transform(training_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(bow_transform.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_bow = bow_transform.transform(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tr = training_data['target']\n",
    "y_te = test_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf-idf representation using the bag-of-words matrix\n",
    "tfidf_trfm = text.TfidfTransformer(norm=None)\n",
    "X_tr_tfidf = tfidf_trfm.fit_transform(X_tr_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te_tfidf = tfidf_trfm.transform(X_te_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_l2 = preproc.normalize(X_tr_bow, axis=0)\n",
    "X_te_l2 = preproc.normalize(X_te_bow, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When we use training statistics to scale test data, the result will look a little fuzzy. Min-max scaling on the test set no longer neatly maps to 0 and 1. ℓ2 norms, mean, and variance statistics will all look a little off. This is less problematic than missing data. For instance, the test set may contain words that are not present in the training data, and we would have no document frequency to use for the new words. The common solution is to simply drop the new words in the test set. This may seem irresponsible, but the model—trained on the training set—would not know what to do with these words anyway. A slightly less hacky option would be to explicitly learn a “garbage” word and map all low-frequency words to it, even within the training set, as discussed in “Rare words”.\n",
    "\n",
    "## Classification with Logistic Regression\n",
    "\n",
    "Logistic regression is a simple, linear classifier. Due to its simplicity, it’s often a good first classifier to try. It takes a weighted combination of the input features, and passes it through a sigmoid function, which smoothly maps any real number to a number between 0 and 1. The function transforms a real number input, x, into a number between 0 and 1. It has one set of parameters, w, which represents the slope of the increase around the midpoint, 0.5. The intercept term b denotes the input value where the function output crosses the midpoint. A logistic classifier would predict the positive class if the sigmoid output is greater than 0.5, and the negative class otherwise. By varying w and b, one can control where that change in decision occurs, and how fast the decision should respond to changing input values around that point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n",
    "    ## Helper function to train a logistic classifier and score on test data\n",
    "    m = LogisticRegression(C=_C).fit(X_tr, y_tr)\n",
    "    s = m.score(X_test, y_test)\n",
    "    print ('Test score with', description, 'features:', s)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow')\n",
    "m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized')\n",
    "m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Paradoxically, the results show that the most accurate classifier is the one using BoW features. This was unexpected. As it turns out, the reason is that the classifiers are not well “tuned,” which is a common pitfall when comparing classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Logistic Regression with Regularization\n",
    "\n",
    "Logistic regression has a few bells and whistles. When the number of features is greater than the number of data points, the problem of finding the best model is said to be underdetermined. One way to fix this problem is by placing additional constraints on the training process. This is known as regularization, and its technical details are discussed here.\n",
    "\n",
    "Most implementations of logistic regression allow for regularization. In order to use this functionality, one must specify a regularization parameter. Regularization parameters are hyperparameters that are not learned automatically in the model training process. Rather, they must be tuned on the problem at hand and given to the training algorithm. This process is known as hyperparameter tuning. (For details on how to evaluate machine learning models, see, e.g., Zheng [2015].) One basic method for tuning hyperparameters is called grid search: you specify a grid of hyperparameter values and the tuner programmatically searches for the best hyperparameter setting in the grid. After finding the best hyperparameter setting, you train a model on the entire training set using that setting, and use its performance on the test set as the final evaluation of this class of models.\n",
    "\n",
    "In the following example, we manually set the search grid of the logistic regularization parameter to {1e-5, 0.001, 0.1, 1, 10, 100}. The upper and lower bounds took a couple of tries to narrow down. The optimal hyperparameter settings for each feature set are given in Table 4-1. \n",
    "\n",
    "We also want to test whether the difference in accuracy between tf-idf and BoW is due to noise. To this end, we use k-fold cross validation to simulate having multiple statistically independent datasets. It divides the dataset into k folds. The cross validation process iterates through the folds, using all but one fold for training, and validating the results on the fold that is held out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning logistic regression hyperparameters with grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\n",
    "bow_search = modsel.GridSearchCV(LogisticRegression(), cv=5, param_grid=param_grid_)\n",
    "l2_search = modsel.GridSearchCV(LogisticRegression(), cv=5,\n",
    "                               param_grid=param_grid_)\n",
    "tfidf_search = modsel.GridSearchCV(LogisticRegression(), cv=5,\n",
    "                                   param_grid=param_grid_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_search.fit(X_tr_bow, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_search.fit(X_tr_l2, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_search.fit(X_tr_tfidf, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = open('tfidf_gridcv_results.pkl', 'wb')\n",
    "pickle.dump(bow_search, results_file, -1)\n",
    "pickle.dump(tfidf_search, results_file, -1)\n",
    "pickle.dump(l2_search, results_file, -1)\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_file = open('tfidf_gridcv_results.pkl', 'rb')\n",
    "bow_search = pickle.load(pkl_file)\n",
    "tfidf_search = pickle.load(pkl_file)\n",
    "l2_search = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = pd.DataFrame.from_dict({'bow': bow_search.cv_results_['mean_test_score'],\n",
    "                               'tfidf': tfidf_search.cv_results_['mean_test_score'],\n",
    "                               'l2': l2_search.cv_results_['mean_test_score']})\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=search_results, width=0.4)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "ax.tick_params(labelsize=14)\n",
    "plt.savefig('tfidf_gridcv_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow', \n",
    "                              _C=bow_search.best_params_['C'])\n",
    "m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized', \n",
    "                              _C=l2_search.best_params_['C'])\n",
    "m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf', \n",
    "                              _C=tfidf_search.best_params_['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper tuning improved the accuracy of all the feature sets, and all three now yield similar classification accuracy under regularized logistic regression. The accuracy score for the tf-idf model is slightly higher, but the difference is likely not statistically significant. These results are completely mystifying. If feature scaling doesn’t work better than vanilla bag-of-words, then why do it at all? Why all the hoopla if tf-idf doesn’t do anything? We’ll explore the answers to those questions in the next section.\n",
    "\n",
    "### Deep Dive: What Is Happening?\n",
    "\n",
    "In order to understand the “why” behind the results, we have to look at how the features are being used by the model. For linear models like logistic regression, this happens through an intermediary object called the data matrix.\n",
    "\n",
    "The data matrix contains data points represented as fixed-length flat vectors. With bag-of-words vectors, the data matrix is also known as the document-term matrix. Figure 3-1 shows a bag-of-words vector in vector form, and Figure 4-1 illustrates four bag-of-words vectors in feature space. To form a document-term matrix, simply take the document vectors, lay them out flat, and stack them on top of one another. The columns represent all possible words in the vocabulary (see Figure 4-5). Since most documents contain only a small subset of all possible words, most of the entries in this matrix are zeros; it is a sparse matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"feml_0405.png\" height=\"360\" width=\"360\"> \n",
    " \n",
    "Feature scaling methods are essentially column operations on the data matrix. In particular, tf-idf and ℓ2 normalization both multiply the entire column (an n-gram feature, for example) by a constant.\n",
    "\n",
    "Training a linear classifier boils down to finding the best linear combination of features, which are column vectors of the data matrix. The solution space is characterized by the column space and the null space of the data matrix. The quality of the trained linear classifier directly depends upon the null space and the column space of the data matrix. A large column space means that there is little linear dependency between the features, which is generally good. The null space contains “novel” data points that cannot be formulated as linear combinations of existing data; a large null space could be problematic. (A perusal of Appendix A is highly recommended for readers who would appreciate a review on concepts such as the linear decision surface, eigen decomposition, and the fundamental subspaces of a matrix.)\n",
    "\n",
    "How do column scaling operations affect the column space and null space of the data matrix? The answer is “Not very much.” But there is a small chance that tf-idf and ℓ2 normalization could be different. We’ll look at why now.\n",
    "\n",
    "The null space of the data matrix can be large for a couple of reasons. First, many datasets contain data points that are very similar to one another. This means the effective row space is small compared to the number of data points in the dataset. Second, the number of features can be much larger than the number of data points. Bag-of-words is particularly good at creating giant feature spaces. In our Yelp example, there are 47K features in 29K reviews in the training set. Moreover, the number of distinct words usually grows with the number of documents in the dataset, so adding more documents would not necessarily decrease the feature-to-data ratio or reduce the null space.\n",
    "\n",
    "With bag-of-words, the column space is relatively small compared to the number of features. There could be words that appear roughly the same number of times in the same documents. This would lead to the corresponding column vectors being nearly linearly dependent, which leads to the column space being not as full rank as it could be (see Appendix A for the definition of full rank). This is called a rank deficiency. (Much like how animals can be deficient in vitamins and minerals, matrices can be deficient in rank, and the output space will not be as fluffy as it should.)\n",
    "\n",
    "Rank-deficient row space and column space lead to the model being overly provisioned for the problem. The linear model outfits a weight parameter for each feature in the dataset. If the row and column spaces were full rank,1 then the model would allow us to generate any target vector in the output space. When they are rank deficient, the model has more degrees of freedom than it needs. This makes it harder to pin down a solution.\n",
    "\n",
    "Can feature scaling solve the rank deficiency problem of the data matrix? Let’s take a look.\n",
    "\n",
    "The column space is defined as the linear combination of all column vectors (boldface indicates a vector): a1v1 + a2v2 + ... + anvn. Feature scaling replaces a column vector with a constant multiple, say 𝐯 ˜ 1 = c 𝐯 1 . But we can still generate the original linear combination by just replacing a1 with a ˜ 1 = a 1 / c . It appears that feature scaling does not change the rank of the column space. Similarly, feature scaling does not affect the rank of the null space, because one can counteract the scaled feature column by reverse scaling the corresponding entry in the weight vector.\n",
    "\n",
    "However, as usual, there is one catch. If the scalar is 0, then there is no way to recover the original linear combination; v1 is gone. If that vector is linearly independent from all the other columns, then we’ve effectively shrunk the column space and enlarged the null space.\n",
    "\n",
    "If that vector is not correlated with the target output, then this is effectively pruning away noisy signals, which is a good thing. This turns out to be the key difference between tf-idf and ℓ2 normalization. ℓ2 normalization would never compute a norm of zero, unless the vector contains all zeros. If the vector is close to zero, then its norm is also close to zero. Dividing by the small norm would accentuate the vector and make it longer.\n",
    "\n",
    "Tf-idf, on the other hand, can generate scaling factors that are close to zero, as shown in Figure 4-2. This happens when the word is present in a large number of documents in the training set. Such a word is likely not strongly correlated with the target vector. Pruning it away allows the solver to focus on the other directions in the column space and find better solutions (although the improvement in accuracy will probably not be huge, because there are typically few noisy directions that are prunable in this way).\n",
    "\n",
    "Where feature scaling—both ℓ2 and tf-idf—does have a telling effect is on the convergence speed of the solver. This is a sign that the data matrix now has a much smaller condition number (the ratio between the largest and smallest singular values—see Appendix A for a full discussion of these terms). In fact, ℓ2 normalization makes the condition number nearly 1. But it’s not the case that the better the condition number, the better the solution. During this experiment, ℓ2 normalization converged much faster than either BoW or tf-idf. But it is also more sensitive to overfitting: it requires much more regularization and is more sensitive to the number of iterations during optimization.\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this chapter, we used tf-idf as an entry point into a detailed analysis of how feature transformations can affect the model (or not). Tf-idf is an example of feature scaling, so we contrasted its performance with that of another feature scaling method—ℓ2 normalization.\n",
    "\n",
    "The results were not as one might have expected. Tf-idf and ℓ2 normalization do not improve the final classifier’s accuracy above plain bag-of-words. After acquiring some statistical modeling and linear algebra chops, we realize why: neither of them changes the column space of the data matrix.\n",
    "\n",
    "One small difference between the two is that tf-idf can “stretch” the word count as well as “compress” it. In other words, it makes some counts bigger, and others close to zero. Therefore, tf-idf could altogether eliminate uninformative words.\n",
    "\n",
    "Along the way, we also discovered another effect of feature scaling: it improves the condition number of the data matrix, making linear models much faster to train. Both ℓ2 normalization and tf-idf have this effect.\n",
    "\n",
    "To summarize, the lesson is: the right feature scaling can be helpful for classification. The right scaling accentuates the informative words and downweights the common words. It can also improve the condition number of the data matrix. The right scaling is not necessarily uniform column scaling.\n",
    "\n",
    "This story is a wonderful illustration of the difficulty of analyzing the effects of feature engineering in the general case. Changing the features affects the training process and the models that ensue. Linear models are the simplest models to understand, yet it still takes very careful experimentation methodology and a lot of deep mathematical knowledge to tease apart the theoretical and practical impacts. This would be mostly impossible with more complicated models or feature transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection as modsel\n",
    "import sklearn.preprocessing as preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and prep Yelp reviews data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Yelp Business data\n",
    "biz_f = open('data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_business.json')\n",
    "biz_df = pd.DataFrame([json.loads(x) for x in biz_f.readlines()])\n",
    "biz_f.close()\n",
    "\n",
    "## Load Yelp Reviews data\n",
    "review_file = open('data/yelp/v6/yelp_dataset_challenge_academic_dataset/yelp_academic_dataset_review.json')\n",
    "review_df = pd.DataFrame([json.loads(x) for x in review_file.readlines()])\n",
    "review_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "biz_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "review_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out only Nightlife and Restaurants businesses\n",
    "two_biz = biz_df[biz_df.apply(lambda x: 'Nightlife' in x['categories'] \n",
    "                                        or 'Restaurants' in x['categories'], \n",
    "                              axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24187, 15)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_biz.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(61184, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biz_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Join with the reviews to get all reviews on the two types of business\n",
    "twobiz_reviews = two_biz.merge(review_df, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1061863, 22)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twobiz_reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "twobiz_reviews.to_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/twobiz_reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "twobiz_reviews = pd.read_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/twobiz_reviews.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim away the features we won't use\n",
    "twobiz_reviews = twobiz_reviews[['business_id', \n",
    "                                 'name', \n",
    "                                 'stars_y', \n",
    "                                 'text', \n",
    "                                 'categories']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create the target column--True for Nightlife businesses, and False otherwise\n",
    "twobiz_reviews['target'] = twobiz_reviews.apply(lambda x: 'Nightlife' in x['categories'],\n",
    "                                                axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## Now pull out each class of reviews separately, \n",
    "## so we can create class-balanced samples for training\n",
    "nightlife = twobiz_reviews[twobiz_reviews.apply(lambda x: 'Nightlife' in x['categories'], axis=1)]\n",
    "restaurants = twobiz_reviews[twobiz_reviews.apply(lambda x: 'Restaurants' in x['categories'], axis=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(210028, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nightlife.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(990627, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurants.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nightlife_subset = nightlife.sample(frac=0.1, random_state=123)\n",
    "restaurant_subset = restaurants.sample(frac=0.021, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21003, 5)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nightlife_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20803, 5)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_subset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nightlife_subset.to_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/nightlife_subset.pkl')\n",
    "restaurant_subset.to_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/restaurant_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "nightlife_subset = pd.read_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/nightlife_subset.pkl')\n",
    "restaurant_subset = pd.read_pickle('data/yelp/v6/yelp_dataset_challenge_academic_dataset/restaurant_subset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "combined = pd.concat([nightlife_subset, restaurant_subset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined['target'] = combined.apply(lambda x: 'Nightlife' in x['categories'],\n",
    "                                    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>name</th>\n",
       "      <th>stars_y</th>\n",
       "      <th>text</th>\n",
       "      <th>categories</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>223202</th>\n",
       "      <td>aRkYtXfmEKYG-eTDf_qUsw</td>\n",
       "      <td>Lux</td>\n",
       "      <td>5</td>\n",
       "      <td>You have to try the velvet!!! Lux is so to die...</td>\n",
       "      <td>[Coffee &amp; Tea, Food, Bars, Nightlife, American...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365597</th>\n",
       "      <td>k-D2NUfaUbuQzPaMXniXcA</td>\n",
       "      <td>The Pub at Monte Carlo</td>\n",
       "      <td>3</td>\n",
       "      <td>This joint is hit or miss on the nite scene......</td>\n",
       "      <td>[Pubs, Bars, American (Traditional), Nightlife...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399883</th>\n",
       "      <td>adv1-kA6k2N4L-e4zULuNg</td>\n",
       "      <td>Fremont Street Experience</td>\n",
       "      <td>5</td>\n",
       "      <td>it's always fun to \"people watch\" on fremont c...</td>\n",
       "      <td>[Casinos, Bars, Arts &amp; Entertainment, Nightlife]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175018</th>\n",
       "      <td>_F2DNPjsqgkAd50zuX8dYQ</td>\n",
       "      <td>Harold's Corral</td>\n",
       "      <td>2</td>\n",
       "      <td>Went for another try at Sunday Brunch. First o...</td>\n",
       "      <td>[American (Traditional), Steakhouses, Nightlif...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>647870</th>\n",
       "      <td>tLLz1fzVjG84LF-mv9hAEw</td>\n",
       "      <td>Mandarin Bar</td>\n",
       "      <td>5</td>\n",
       "      <td>I like and probably prefer a dive bar over hig...</td>\n",
       "      <td>[Bars, Nightlife, Lounges]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058274</th>\n",
       "      <td>YYM9p7iV3A3dGtsqjemqBw</td>\n",
       "      <td>Echo 5 Sports Pub</td>\n",
       "      <td>5</td>\n",
       "      <td>What a great find! Service was awesome, food w...</td>\n",
       "      <td>[Bars, Restaurants, American (Traditional), Sp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>538477</th>\n",
       "      <td>4xqn5hGqXKcU7_pY5lKYOQ</td>\n",
       "      <td>Hawthorne's NY Pizza &amp; Bar</td>\n",
       "      <td>2</td>\n",
       "      <td>went there for \"the best in pizza in Charlotte...</td>\n",
       "      <td>[Italian, Bars, Pizza, Nightlife, Restaurants]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26030</th>\n",
       "      <td>IgaruuknYwCr9afeDYQ_yw</td>\n",
       "      <td>Dirty Drummer</td>\n",
       "      <td>4</td>\n",
       "      <td>A clean dive bar neatly disguised as a sports ...</td>\n",
       "      <td>[Nightlife, Restaurants]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606932</th>\n",
       "      <td>3n9mSKySEv3G03YjcU-YOQ</td>\n",
       "      <td>Postino Central</td>\n",
       "      <td>5</td>\n",
       "      <td>This is my new favorite restaurant in Phoenix....</td>\n",
       "      <td>[Bars, Breakfast &amp; Brunch, Wine Bars, Nightlif...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125852</th>\n",
       "      <td>LLR8iqKn7nmfwXWl1S13Lw</td>\n",
       "      <td>Hawk's Bar &amp; Grill</td>\n",
       "      <td>4</td>\n",
       "      <td>Med plate is great, price is fair. Even the sm...</td>\n",
       "      <td>[Bars, Restaurants, American (Traditional), Sp...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44114</th>\n",
       "      <td>RKOS92ptLgEU3BUNMtDHrw</td>\n",
       "      <td>Bobby C's Bar &amp; Lounge</td>\n",
       "      <td>2</td>\n",
       "      <td>This Review Brought to you by the Letter \"L\"\\n...</td>\n",
       "      <td>[Jazz &amp; Blues, Arts &amp; Entertainment, Soul Food...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582925</th>\n",
       "      <td>Es300Ys1XXPYg8aI7BKVYQ</td>\n",
       "      <td>XS Nightclub</td>\n",
       "      <td>4</td>\n",
       "      <td>My girlfriends &amp; I recently went to Vegas for ...</td>\n",
       "      <td>[Nightlife, Dance Clubs]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975188</th>\n",
       "      <td>-4bNlx03GsXzgkHZwTrMaw</td>\n",
       "      <td>Redbeard's On Sixth</td>\n",
       "      <td>2</td>\n",
       "      <td>Food was cold at all stages of the meal. Dip T...</td>\n",
       "      <td>[Bars, American (Traditional), Pubs, Nightlife...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743863</th>\n",
       "      <td>7q1FpSXbE6XtLNg518pxDA</td>\n",
       "      <td>Miller's Alehouse</td>\n",
       "      <td>5</td>\n",
       "      <td>Just had a great lunch at Ale House! Our serve...</td>\n",
       "      <td>[Nightlife, Bars, American (Traditional), Spor...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893313</th>\n",
       "      <td>xE9-TVvDM9qp_eh0oGCpTA</td>\n",
       "      <td>Oracle Nightclub</td>\n",
       "      <td>4</td>\n",
       "      <td>I'm not the one to go to clubs much, mainly be...</td>\n",
       "      <td>[Bars, Hookah Bars, Nightlife, Dance Clubs]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316312</th>\n",
       "      <td>uqisw8IXD5ZutU4MT2MQVw</td>\n",
       "      <td>Social House</td>\n",
       "      <td>3</td>\n",
       "      <td>Not really a sushi restaurant, woefully overpr...</td>\n",
       "      <td>[Bars, Japanese, Asian Fusion, Nightlife, Loun...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>606965</th>\n",
       "      <td>3n9mSKySEv3G03YjcU-YOQ</td>\n",
       "      <td>Postino Central</td>\n",
       "      <td>4</td>\n",
       "      <td>I have been to the Arcadia location quite a fe...</td>\n",
       "      <td>[Bars, Breakfast &amp; Brunch, Wine Bars, Nightlif...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257517</th>\n",
       "      <td>ES-j5yNTgmgbL4He0h6otA</td>\n",
       "      <td>Hofbrauhaus - Las Vegas</td>\n",
       "      <td>5</td>\n",
       "      <td>Hofbrauhaus used to be a hidden gem in Vegas, ...</td>\n",
       "      <td>[German, Bars, Nightlife, Restaurants]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316133</th>\n",
       "      <td>uqisw8IXD5ZutU4MT2MQVw</td>\n",
       "      <td>Social House</td>\n",
       "      <td>5</td>\n",
       "      <td>Yes! Social House was fantastic at TI. It's mo...</td>\n",
       "      <td>[Bars, Japanese, Asian Fusion, Nightlife, Loun...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31284</th>\n",
       "      <td>Pfb6VOIiroqDWOebfgWGPQ</td>\n",
       "      <td>Phoenix City Grille</td>\n",
       "      <td>5</td>\n",
       "      <td>We parked in the back and came in through a se...</td>\n",
       "      <td>[Pubs, Bars, Nightlife, Irish, Restaurants]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>640078</th>\n",
       "      <td>CDN--GWL8gm4TsFFtccpaA</td>\n",
       "      <td>Azure Luxury Pool</td>\n",
       "      <td>1</td>\n",
       "      <td>As we walked towards TAO beach, we were greete...</td>\n",
       "      <td>[Active Life, Swimming Pools, Dance Clubs, Nig...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414972</th>\n",
       "      <td>stKD-HqTxDMylPonFyVdWA</td>\n",
       "      <td>Boardwalk Billys Raw Bar &amp; Ribs</td>\n",
       "      <td>2</td>\n",
       "      <td>The best thing on this menu is the hot peel an...</td>\n",
       "      <td>[Bars, American (Traditional), Nightlife, Spor...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608519</th>\n",
       "      <td>0_J4y0aEmtuzgdsH2OfWQw</td>\n",
       "      <td>Don't Tell Mama</td>\n",
       "      <td>3</td>\n",
       "      <td>Nice little piano bar, I have heard about it's...</td>\n",
       "      <td>[Bars, Nightlife, Lounges]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998043</th>\n",
       "      <td>IOwuFFUwRljuOxatDCGm6w</td>\n",
       "      <td>The House of Brews</td>\n",
       "      <td>1</td>\n",
       "      <td>Wow...this place has really gone down. New men...</td>\n",
       "      <td>[Nightlife, Bars, American (New), Sports Bars,...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>2SwC8wqpZC4B9iFVTgYT9A</td>\n",
       "      <td>Casbah</td>\n",
       "      <td>4</td>\n",
       "      <td>So I hated this Soba place owned by the same p...</td>\n",
       "      <td>[Wine Bars, Bars, Mediterranean, Nightlife, Re...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351541</th>\n",
       "      <td>WNy1uzcmm_UHmTyR--o5IA</td>\n",
       "      <td>Cornish Pasty Company</td>\n",
       "      <td>5</td>\n",
       "      <td>Wow...we loved this place!  I had bookmarked t...</td>\n",
       "      <td>[Pubs, Bars, Nightlife, British, Restaurants]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716691</th>\n",
       "      <td>OdD1GuGNQ64ssJmMJ_D9RQ</td>\n",
       "      <td>The Duce</td>\n",
       "      <td>1</td>\n",
       "      <td>Cool place but don't try to arrange a special ...</td>\n",
       "      <td>[Bars, Nightlife, Lounges]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1125</th>\n",
       "      <td>2SwC8wqpZC4B9iFVTgYT9A</td>\n",
       "      <td>Casbah</td>\n",
       "      <td>5</td>\n",
       "      <td>I enjoy coming to Casbah because it reminds me...</td>\n",
       "      <td>[Wine Bars, Bars, Mediterranean, Nightlife, Re...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>900253</th>\n",
       "      <td>40xXIs_WIOvrhtbS7QJ1ZQ</td>\n",
       "      <td>Four Sevens Sports Bar &amp; Restaurant</td>\n",
       "      <td>4</td>\n",
       "      <td>Needed a good spot for an after work beverage ...</td>\n",
       "      <td>[Bars, Restaurants, Sports Bars, Chinese, Nigh...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430613</th>\n",
       "      <td>6gugg5XY66ukGewmOptSKw</td>\n",
       "      <td>The Steakhouse at Camelot</td>\n",
       "      <td>3</td>\n",
       "      <td>The food was great. The service sucked. The wa...</td>\n",
       "      <td>[Bars, Steakhouses, American (Traditional), Ni...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177165</th>\n",
       "      <td>ErTR62P1wNPelAbHkr_9cw</td>\n",
       "      <td>Fuego Steakhouse</td>\n",
       "      <td>5</td>\n",
       "      <td>If there's one thing I know after spending the...</td>\n",
       "      <td>[Steakhouses, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245413</th>\n",
       "      <td>4bEjOyTaDG24SY5TxsaUNQ</td>\n",
       "      <td>Mon Ami Gabi</td>\n",
       "      <td>5</td>\n",
       "      <td>We love this place. It is a must stop for us w...</td>\n",
       "      <td>[Breakfast &amp; Brunch, Steakhouses, French, Rest...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364268</th>\n",
       "      <td>Vtx0VYkybAd71fzwgaoQ8g</td>\n",
       "      <td>The Skeptical Chymist</td>\n",
       "      <td>4</td>\n",
       "      <td>Came here on a Saturday night and the band was...</td>\n",
       "      <td>[Pubs, Bars, Nightlife, Irish, Restaurants]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587462</th>\n",
       "      <td>RhqrOvy6Zr8vn5pJ9PkF1A</td>\n",
       "      <td>Wazuzu</td>\n",
       "      <td>5</td>\n",
       "      <td>Another great meal at Wazuzu.  Chef Jet is a s...</td>\n",
       "      <td>[Diners, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357168</th>\n",
       "      <td>KBG28p3lGX17hOPoHhq5PQ</td>\n",
       "      <td>Yasu Sushi Bistro</td>\n",
       "      <td>5</td>\n",
       "      <td>I want to give this place 7 stars! My only reg...</td>\n",
       "      <td>[Sushi Bars, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234987</th>\n",
       "      <td>c7pNpJMmvRXBpC_8kj76nA</td>\n",
       "      <td>Lang Van Vietnamese</td>\n",
       "      <td>5</td>\n",
       "      <td>After spending the early afternoon at the Char...</td>\n",
       "      <td>[Vietnamese, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>601625</th>\n",
       "      <td>6nfTUgP3UvQ_jhnT8kCpew</td>\n",
       "      <td>Hon Machi</td>\n",
       "      <td>4</td>\n",
       "      <td>We went here on a Monday night and reluctantly...</td>\n",
       "      <td>[Sushi Bars, Japanese, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440658</th>\n",
       "      <td>8o-NLKy_XfbJtqljX9XLCA</td>\n",
       "      <td>Mrs White's Golden Rule Cafe</td>\n",
       "      <td>5</td>\n",
       "      <td>Been coming here for a few months, but tonight...</td>\n",
       "      <td>[Soul Food, Southern, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410026</th>\n",
       "      <td>2AB_43WAw7cNvMcm-QEliw</td>\n",
       "      <td>Sushi Tower &amp; Steakhouse</td>\n",
       "      <td>4</td>\n",
       "      <td>I've been here quite a few times and I love it...</td>\n",
       "      <td>[Steakhouses, Sushi Bars, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366903</th>\n",
       "      <td>fZVZeZJkziJ7B1rIcmT3Iw</td>\n",
       "      <td>Sierra Gold</td>\n",
       "      <td>4</td>\n",
       "      <td>I haven't been to Sierra Gold in quite a while...</td>\n",
       "      <td>[Bars, Pubs, Nightlife, American (New), Sports...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>832706</th>\n",
       "      <td>IRYj1b1-mlXqABVsl9SoqQ</td>\n",
       "      <td>Tacos El Gordo</td>\n",
       "      <td>5</td>\n",
       "      <td>Hands down best taco joint I have ever been to...</td>\n",
       "      <td>[Mexican, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>481156</th>\n",
       "      <td>simnCuMX4OVwIhWaIqL4zA</td>\n",
       "      <td>Red House</td>\n",
       "      <td>5</td>\n",
       "      <td>I told myself no more five stars but I love th...</td>\n",
       "      <td>[Chinese, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209427</th>\n",
       "      <td>r70nuDpkR5-g1oShW2ANvA</td>\n",
       "      <td>Crown &amp; Anchor British Pub</td>\n",
       "      <td>5</td>\n",
       "      <td>Bloody Brilliant! Excellent choice of beers on...</td>\n",
       "      <td>[Pubs, Bars, Nightlife, British, Restaurants]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463651</th>\n",
       "      <td>CEPvTz9BU_wK45zZoQNhrw</td>\n",
       "      <td>Tequileria</td>\n",
       "      <td>2</td>\n",
       "      <td>Can I have more steak with this Fat!? Geez! It...</td>\n",
       "      <td>[Mexican, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808672</th>\n",
       "      <td>nZgYNbl1XlZyyKlISMoG6w</td>\n",
       "      <td>Sugar Factory American Brasserie</td>\n",
       "      <td>1</td>\n",
       "      <td>I went to the Sugar Factory for breakfast last...</td>\n",
       "      <td>[Breakfast &amp; Brunch, Bars, American (New), Nig...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344165</th>\n",
       "      <td>FqzgT9Y-Yu7jiWdHnGW-kQ</td>\n",
       "      <td>The Vig</td>\n",
       "      <td>5</td>\n",
       "      <td>Probably the best bowl of Posole that I have h...</td>\n",
       "      <td>[Bars, American (Traditional), Pubs, Nightlife...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980915</th>\n",
       "      <td>6O9iAmAv7vUl-Eeqb9bmpw</td>\n",
       "      <td>Wahoo's Fish Tacos</td>\n",
       "      <td>3</td>\n",
       "      <td>The food was alright, I guess.  Nothing too gr...</td>\n",
       "      <td>[Seafood, Mexican, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144580</th>\n",
       "      <td>g3WAdIH2ULX6hFI6hOfBWw</td>\n",
       "      <td>Got Sushi</td>\n",
       "      <td>1</td>\n",
       "      <td>This place made me so sick and I have an iron ...</td>\n",
       "      <td>[Japanese, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189706</th>\n",
       "      <td>LOef7o-dnn7Z5qGMZCi6pw</td>\n",
       "      <td>Circo</td>\n",
       "      <td>5</td>\n",
       "      <td>If you love eating fine cuisine, but cannot qu...</td>\n",
       "      <td>[Restaurants, Italian]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137545</th>\n",
       "      <td>NQhPkGeHiljeg7tXPTHJeA</td>\n",
       "      <td>Sweet Tomatoes</td>\n",
       "      <td>4</td>\n",
       "      <td>Every once in a while after eating poorly, I n...</td>\n",
       "      <td>[Vegetarian, Buffets, Salad, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868430</th>\n",
       "      <td>05fTEsjUPVo5jks8PpNKCQ</td>\n",
       "      <td>Rollin Smoke Barbeque</td>\n",
       "      <td>4</td>\n",
       "      <td>Great food nice clean place! No real BBQ in Ve...</td>\n",
       "      <td>[Burgers, Soul Food, Barbeque, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303238</th>\n",
       "      <td>UL3OMN_c-NXHlyb97pDifA</td>\n",
       "      <td>Yard House</td>\n",
       "      <td>4</td>\n",
       "      <td>I love this place for their fantastic martinis...</td>\n",
       "      <td>[Nightlife, Vegetarian, American (New), Bars, ...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>516017</th>\n",
       "      <td>r4AG1WXCRWkZKQctrcy9FA</td>\n",
       "      <td>Grimaldi's Pizzeria</td>\n",
       "      <td>4</td>\n",
       "      <td>I am fast becoming a pizza afficianado.    We ...</td>\n",
       "      <td>[Pizza, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>454032</th>\n",
       "      <td>SNTDhGh-AdBPKFfq6xCsRA</td>\n",
       "      <td>Kings Fish House</td>\n",
       "      <td>5</td>\n",
       "      <td>Five stars to Kings for their generosity.\\n\\nM...</td>\n",
       "      <td>[Seafood, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017805</th>\n",
       "      <td>TNpJsewe0H__8fzBgaHCzg</td>\n",
       "      <td>Angelina's Pizzeria</td>\n",
       "      <td>4</td>\n",
       "      <td>Pretty good pizza. Fast delivery. This used to...</td>\n",
       "      <td>[Italian, Pizza, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300150</th>\n",
       "      <td>KlAARyLySLdoyT8QxJFSIg</td>\n",
       "      <td>The Prime Rib Loft</td>\n",
       "      <td>4</td>\n",
       "      <td>No thanks to our server, we will be back.\\n\\nT...</td>\n",
       "      <td>[Steakhouses, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938078</th>\n",
       "      <td>vDEC5kHgtjkqyuZAZrSuMA</td>\n",
       "      <td>Pho Annie</td>\n",
       "      <td>4</td>\n",
       "      <td>Awesome place. Usually a return customer of th...</td>\n",
       "      <td>[Vietnamese, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986467</th>\n",
       "      <td>XfVGU6WGZKc867spn2TImw</td>\n",
       "      <td>Pomo Pizzeria Napoletana - Phoenix</td>\n",
       "      <td>4</td>\n",
       "      <td>Great vibe in this place. Very friendly staff....</td>\n",
       "      <td>[Italian, Pizza, Sandwiches, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54382</th>\n",
       "      <td>JNEeWjajyZPrwMb1cYi_pg</td>\n",
       "      <td>Creative Cafe</td>\n",
       "      <td>5</td>\n",
       "      <td>Just passing through town and found this great...</td>\n",
       "      <td>[American (New), Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251637</th>\n",
       "      <td>tFU2Js_nbIZOrnKfYJYBBg</td>\n",
       "      <td>Bouchon Bistro</td>\n",
       "      <td>5</td>\n",
       "      <td>No reservations and its always packed so get t...</td>\n",
       "      <td>[Breakfast &amp; Brunch, French, Restaurants]</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41806 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    business_id                                 name  stars_y  \\\n",
       "223202   aRkYtXfmEKYG-eTDf_qUsw                                  Lux        5   \n",
       "365597   k-D2NUfaUbuQzPaMXniXcA               The Pub at Monte Carlo        3   \n",
       "399883   adv1-kA6k2N4L-e4zULuNg            Fremont Street Experience        5   \n",
       "175018   _F2DNPjsqgkAd50zuX8dYQ                      Harold's Corral        2   \n",
       "647870   tLLz1fzVjG84LF-mv9hAEw                         Mandarin Bar        5   \n",
       "1058274  YYM9p7iV3A3dGtsqjemqBw                    Echo 5 Sports Pub        5   \n",
       "538477   4xqn5hGqXKcU7_pY5lKYOQ           Hawthorne's NY Pizza & Bar        2   \n",
       "26030    IgaruuknYwCr9afeDYQ_yw                        Dirty Drummer        4   \n",
       "606932   3n9mSKySEv3G03YjcU-YOQ                      Postino Central        5   \n",
       "125852   LLR8iqKn7nmfwXWl1S13Lw                   Hawk's Bar & Grill        4   \n",
       "44114    RKOS92ptLgEU3BUNMtDHrw               Bobby C's Bar & Lounge        2   \n",
       "582925   Es300Ys1XXPYg8aI7BKVYQ                         XS Nightclub        4   \n",
       "975188   -4bNlx03GsXzgkHZwTrMaw                  Redbeard's On Sixth        2   \n",
       "743863   7q1FpSXbE6XtLNg518pxDA                    Miller's Alehouse        5   \n",
       "893313   xE9-TVvDM9qp_eh0oGCpTA                     Oracle Nightclub        4   \n",
       "316312   uqisw8IXD5ZutU4MT2MQVw                         Social House        3   \n",
       "606965   3n9mSKySEv3G03YjcU-YOQ                      Postino Central        4   \n",
       "257517   ES-j5yNTgmgbL4He0h6otA              Hofbrauhaus - Las Vegas        5   \n",
       "316133   uqisw8IXD5ZutU4MT2MQVw                         Social House        5   \n",
       "31284    Pfb6VOIiroqDWOebfgWGPQ                  Phoenix City Grille        5   \n",
       "640078   CDN--GWL8gm4TsFFtccpaA                    Azure Luxury Pool        1   \n",
       "414972   stKD-HqTxDMylPonFyVdWA      Boardwalk Billys Raw Bar & Ribs        2   \n",
       "608519   0_J4y0aEmtuzgdsH2OfWQw                      Don't Tell Mama        3   \n",
       "998043   IOwuFFUwRljuOxatDCGm6w                   The House of Brews        1   \n",
       "1061     2SwC8wqpZC4B9iFVTgYT9A                               Casbah        4   \n",
       "351541   WNy1uzcmm_UHmTyR--o5IA                Cornish Pasty Company        5   \n",
       "716691   OdD1GuGNQ64ssJmMJ_D9RQ                             The Duce        1   \n",
       "1125     2SwC8wqpZC4B9iFVTgYT9A                               Casbah        5   \n",
       "900253   40xXIs_WIOvrhtbS7QJ1ZQ  Four Sevens Sports Bar & Restaurant        4   \n",
       "430613   6gugg5XY66ukGewmOptSKw            The Steakhouse at Camelot        3   \n",
       "...                         ...                                  ...      ...   \n",
       "177165   ErTR62P1wNPelAbHkr_9cw                     Fuego Steakhouse        5   \n",
       "245413   4bEjOyTaDG24SY5TxsaUNQ                         Mon Ami Gabi        5   \n",
       "364268   Vtx0VYkybAd71fzwgaoQ8g                The Skeptical Chymist        4   \n",
       "587462   RhqrOvy6Zr8vn5pJ9PkF1A                               Wazuzu        5   \n",
       "357168   KBG28p3lGX17hOPoHhq5PQ                    Yasu Sushi Bistro        5   \n",
       "234987   c7pNpJMmvRXBpC_8kj76nA                  Lang Van Vietnamese        5   \n",
       "601625   6nfTUgP3UvQ_jhnT8kCpew                            Hon Machi        4   \n",
       "440658   8o-NLKy_XfbJtqljX9XLCA         Mrs White's Golden Rule Cafe        5   \n",
       "410026   2AB_43WAw7cNvMcm-QEliw             Sushi Tower & Steakhouse        4   \n",
       "366903   fZVZeZJkziJ7B1rIcmT3Iw                          Sierra Gold        4   \n",
       "832706   IRYj1b1-mlXqABVsl9SoqQ                       Tacos El Gordo        5   \n",
       "481156   simnCuMX4OVwIhWaIqL4zA                            Red House        5   \n",
       "209427   r70nuDpkR5-g1oShW2ANvA           Crown & Anchor British Pub        5   \n",
       "463651   CEPvTz9BU_wK45zZoQNhrw                           Tequileria        2   \n",
       "808672   nZgYNbl1XlZyyKlISMoG6w     Sugar Factory American Brasserie        1   \n",
       "344165   FqzgT9Y-Yu7jiWdHnGW-kQ                              The Vig        5   \n",
       "980915   6O9iAmAv7vUl-Eeqb9bmpw                   Wahoo's Fish Tacos        3   \n",
       "144580   g3WAdIH2ULX6hFI6hOfBWw                            Got Sushi        1   \n",
       "189706   LOef7o-dnn7Z5qGMZCi6pw                                Circo        5   \n",
       "137545   NQhPkGeHiljeg7tXPTHJeA                       Sweet Tomatoes        4   \n",
       "868430   05fTEsjUPVo5jks8PpNKCQ                Rollin Smoke Barbeque        4   \n",
       "303238   UL3OMN_c-NXHlyb97pDifA                           Yard House        4   \n",
       "516017   r4AG1WXCRWkZKQctrcy9FA                  Grimaldi's Pizzeria        4   \n",
       "454032   SNTDhGh-AdBPKFfq6xCsRA                     Kings Fish House        5   \n",
       "1017805  TNpJsewe0H__8fzBgaHCzg                  Angelina's Pizzeria        4   \n",
       "300150   KlAARyLySLdoyT8QxJFSIg                   The Prime Rib Loft        4   \n",
       "938078   vDEC5kHgtjkqyuZAZrSuMA                            Pho Annie        4   \n",
       "986467   XfVGU6WGZKc867spn2TImw   Pomo Pizzeria Napoletana - Phoenix        4   \n",
       "54382    JNEeWjajyZPrwMb1cYi_pg                        Creative Cafe        5   \n",
       "251637   tFU2Js_nbIZOrnKfYJYBBg                       Bouchon Bistro        5   \n",
       "\n",
       "                                                      text  \\\n",
       "223202   You have to try the velvet!!! Lux is so to die...   \n",
       "365597   This joint is hit or miss on the nite scene......   \n",
       "399883   it's always fun to \"people watch\" on fremont c...   \n",
       "175018   Went for another try at Sunday Brunch. First o...   \n",
       "647870   I like and probably prefer a dive bar over hig...   \n",
       "1058274  What a great find! Service was awesome, food w...   \n",
       "538477   went there for \"the best in pizza in Charlotte...   \n",
       "26030    A clean dive bar neatly disguised as a sports ...   \n",
       "606932   This is my new favorite restaurant in Phoenix....   \n",
       "125852   Med plate is great, price is fair. Even the sm...   \n",
       "44114    This Review Brought to you by the Letter \"L\"\\n...   \n",
       "582925   My girlfriends & I recently went to Vegas for ...   \n",
       "975188   Food was cold at all stages of the meal. Dip T...   \n",
       "743863   Just had a great lunch at Ale House! Our serve...   \n",
       "893313   I'm not the one to go to clubs much, mainly be...   \n",
       "316312   Not really a sushi restaurant, woefully overpr...   \n",
       "606965   I have been to the Arcadia location quite a fe...   \n",
       "257517   Hofbrauhaus used to be a hidden gem in Vegas, ...   \n",
       "316133   Yes! Social House was fantastic at TI. It's mo...   \n",
       "31284    We parked in the back and came in through a se...   \n",
       "640078   As we walked towards TAO beach, we were greete...   \n",
       "414972   The best thing on this menu is the hot peel an...   \n",
       "608519   Nice little piano bar, I have heard about it's...   \n",
       "998043   Wow...this place has really gone down. New men...   \n",
       "1061     So I hated this Soba place owned by the same p...   \n",
       "351541   Wow...we loved this place!  I had bookmarked t...   \n",
       "716691   Cool place but don't try to arrange a special ...   \n",
       "1125     I enjoy coming to Casbah because it reminds me...   \n",
       "900253   Needed a good spot for an after work beverage ...   \n",
       "430613   The food was great. The service sucked. The wa...   \n",
       "...                                                    ...   \n",
       "177165   If there's one thing I know after spending the...   \n",
       "245413   We love this place. It is a must stop for us w...   \n",
       "364268   Came here on a Saturday night and the band was...   \n",
       "587462   Another great meal at Wazuzu.  Chef Jet is a s...   \n",
       "357168   I want to give this place 7 stars! My only reg...   \n",
       "234987   After spending the early afternoon at the Char...   \n",
       "601625   We went here on a Monday night and reluctantly...   \n",
       "440658   Been coming here for a few months, but tonight...   \n",
       "410026   I've been here quite a few times and I love it...   \n",
       "366903   I haven't been to Sierra Gold in quite a while...   \n",
       "832706   Hands down best taco joint I have ever been to...   \n",
       "481156   I told myself no more five stars but I love th...   \n",
       "209427   Bloody Brilliant! Excellent choice of beers on...   \n",
       "463651   Can I have more steak with this Fat!? Geez! It...   \n",
       "808672   I went to the Sugar Factory for breakfast last...   \n",
       "344165   Probably the best bowl of Posole that I have h...   \n",
       "980915   The food was alright, I guess.  Nothing too gr...   \n",
       "144580   This place made me so sick and I have an iron ...   \n",
       "189706   If you love eating fine cuisine, but cannot qu...   \n",
       "137545   Every once in a while after eating poorly, I n...   \n",
       "868430   Great food nice clean place! No real BBQ in Ve...   \n",
       "303238   I love this place for their fantastic martinis...   \n",
       "516017   I am fast becoming a pizza afficianado.    We ...   \n",
       "454032   Five stars to Kings for their generosity.\\n\\nM...   \n",
       "1017805  Pretty good pizza. Fast delivery. This used to...   \n",
       "300150   No thanks to our server, we will be back.\\n\\nT...   \n",
       "938078   Awesome place. Usually a return customer of th...   \n",
       "986467   Great vibe in this place. Very friendly staff....   \n",
       "54382    Just passing through town and found this great...   \n",
       "251637   No reservations and its always packed so get t...   \n",
       "\n",
       "                                                categories  target  \n",
       "223202   [Coffee & Tea, Food, Bars, Nightlife, American...    True  \n",
       "365597   [Pubs, Bars, American (Traditional), Nightlife...    True  \n",
       "399883    [Casinos, Bars, Arts & Entertainment, Nightlife]    True  \n",
       "175018   [American (Traditional), Steakhouses, Nightlif...    True  \n",
       "647870                          [Bars, Nightlife, Lounges]    True  \n",
       "1058274  [Bars, Restaurants, American (Traditional), Sp...    True  \n",
       "538477      [Italian, Bars, Pizza, Nightlife, Restaurants]    True  \n",
       "26030                             [Nightlife, Restaurants]    True  \n",
       "606932   [Bars, Breakfast & Brunch, Wine Bars, Nightlif...    True  \n",
       "125852   [Bars, Restaurants, American (Traditional), Sp...    True  \n",
       "44114    [Jazz & Blues, Arts & Entertainment, Soul Food...    True  \n",
       "582925                            [Nightlife, Dance Clubs]    True  \n",
       "975188   [Bars, American (Traditional), Pubs, Nightlife...    True  \n",
       "743863   [Nightlife, Bars, American (Traditional), Spor...    True  \n",
       "893313         [Bars, Hookah Bars, Nightlife, Dance Clubs]    True  \n",
       "316312   [Bars, Japanese, Asian Fusion, Nightlife, Loun...    True  \n",
       "606965   [Bars, Breakfast & Brunch, Wine Bars, Nightlif...    True  \n",
       "257517              [German, Bars, Nightlife, Restaurants]    True  \n",
       "316133   [Bars, Japanese, Asian Fusion, Nightlife, Loun...    True  \n",
       "31284          [Pubs, Bars, Nightlife, Irish, Restaurants]    True  \n",
       "640078   [Active Life, Swimming Pools, Dance Clubs, Nig...    True  \n",
       "414972   [Bars, American (Traditional), Nightlife, Spor...    True  \n",
       "608519                          [Bars, Nightlife, Lounges]    True  \n",
       "998043   [Nightlife, Bars, American (New), Sports Bars,...    True  \n",
       "1061     [Wine Bars, Bars, Mediterranean, Nightlife, Re...    True  \n",
       "351541       [Pubs, Bars, Nightlife, British, Restaurants]    True  \n",
       "716691                          [Bars, Nightlife, Lounges]    True  \n",
       "1125     [Wine Bars, Bars, Mediterranean, Nightlife, Re...    True  \n",
       "900253   [Bars, Restaurants, Sports Bars, Chinese, Nigh...    True  \n",
       "430613   [Bars, Steakhouses, American (Traditional), Ni...    True  \n",
       "...                                                    ...     ...  \n",
       "177165                          [Steakhouses, Restaurants]   False  \n",
       "245413   [Breakfast & Brunch, Steakhouses, French, Rest...   False  \n",
       "364268         [Pubs, Bars, Nightlife, Irish, Restaurants]    True  \n",
       "587462                               [Diners, Restaurants]   False  \n",
       "357168                           [Sushi Bars, Restaurants]   False  \n",
       "234987                           [Vietnamese, Restaurants]   False  \n",
       "601625                 [Sushi Bars, Japanese, Restaurants]   False  \n",
       "440658                  [Soul Food, Southern, Restaurants]   False  \n",
       "410026              [Steakhouses, Sushi Bars, Restaurants]   False  \n",
       "366903   [Bars, Pubs, Nightlife, American (New), Sports...    True  \n",
       "832706                              [Mexican, Restaurants]   False  \n",
       "481156                              [Chinese, Restaurants]   False  \n",
       "209427       [Pubs, Bars, Nightlife, British, Restaurants]    True  \n",
       "463651                              [Mexican, Restaurants]   False  \n",
       "808672   [Breakfast & Brunch, Bars, American (New), Nig...    True  \n",
       "344165   [Bars, American (Traditional), Pubs, Nightlife...    True  \n",
       "980915                     [Seafood, Mexican, Restaurants]   False  \n",
       "144580                             [Japanese, Restaurants]   False  \n",
       "189706                              [Restaurants, Italian]   False  \n",
       "137545           [Vegetarian, Buffets, Salad, Restaurants]   False  \n",
       "868430         [Burgers, Soul Food, Barbeque, Restaurants]   False  \n",
       "303238   [Nightlife, Vegetarian, American (New), Bars, ...    True  \n",
       "516017                                [Pizza, Restaurants]   False  \n",
       "454032                              [Seafood, Restaurants]   False  \n",
       "1017805                      [Italian, Pizza, Restaurants]   False  \n",
       "300150                          [Steakhouses, Restaurants]   False  \n",
       "938078                           [Vietnamese, Restaurants]   False  \n",
       "986467           [Italian, Pizza, Sandwiches, Restaurants]   False  \n",
       "54382                        [American (New), Restaurants]   False  \n",
       "251637           [Breakfast & Brunch, French, Restaurants]   False  \n",
       "\n",
       "[41806 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lemon\\Anaconda3\\envs\\mfe\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test data sets\n",
    "training_data, test_data = modsel.train_test_split(combined, \n",
    "                                                   train_size=0.7, \n",
    "                                                   random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29264, 6)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12542, 6)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Represent the review text as a bag-of-words \n",
    "bow_transform = text.CountVectorizer()\n",
    "X_tr_bow = bow_transform.fit_transform(training_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46924"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow_transform.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29264, 46924)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_te_bow = bow_transform.transform(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "y_tr = training_data['target']\n",
    "y_te = test_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tf-idf representation using the bag-of-words matrix\n",
    "tfidf_trfm = text.TfidfTransformer(norm=None)\n",
    "X_tr_tfidf = tfidf_trfm.fit_transform(X_tr_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "X_te_tfidf = tfidf_trfm.transform(X_te_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_l2 = preproc.normalize(X_tr_bow, axis=0)\n",
    "X_te_l2 = preproc.normalize(X_te_bow, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classify with logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0):\n",
    "    ## Helper function to train a logistic classifier and score on test data\n",
    "    m = LogisticRegression(C=_C).fit(X_tr, y_tr)\n",
    "    s = m.score(X_test, y_test)\n",
    "    print ('Test score with', description, 'features:', s)\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score with bow features: 0.775873066497\n",
      "Test score with l2-normalized features: 0.763514590974\n",
      "Test score with tf-idf features: 0.743182905438\n"
     ]
    }
   ],
   "source": [
    "m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow')\n",
    "m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized')\n",
    "m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune regularization parameters using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "param_grid_ = {'C': [1e-5, 1e-3, 1e-1, 1e0, 1e1, 1e2]}\n",
    "bow_search = modsel.GridSearchCV(LogisticRegression(), cv=5, param_grid=param_grid_)\n",
    "l2_search = modsel.GridSearchCV(LogisticRegression(), cv=5,\n",
    "                               param_grid=param_grid_)\n",
    "tfidf_search = modsel.GridSearchCV(LogisticRegression(), cv=5,\n",
    "                                   param_grid=param_grid_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.001, 0.1, 1.0, 10.0, 100.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_search.fit(X_tr_bow, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78283898305084743"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.001, 0.1, 1.0, 10.0, 100.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_search.fit(X_tr_l2, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77675642427556046"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.001, 0.1, 1.0, 10.0, 100.0]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_search.fit(X_tr_tfidf, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78864816839803176"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10.0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.001}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([  0.43648252,   0.94630651,   5.64090128,  15.31248307,\n",
       "         31.47010217,  42.44257565]),\n",
       " 'mean_score_time': array([ 0.00080056,  0.00392466,  0.00864897,  0.00784755,  0.01192751,\n",
       "         0.0072515 ]),\n",
       " 'mean_test_score': array([ 0.57897075,  0.7518111 ,  0.78283898,  0.77381766,  0.75515992,\n",
       "         0.73937261]),\n",
       " 'mean_train_score': array([ 0.5792185 ,  0.76731652,  0.87697341,  0.94629064,  0.98357195,\n",
       "         0.99441294]),\n",
       " 'param_C': masked_array(data = [1e-05 0.001 0.1 1.0 10.0 100.0],\n",
       "              mask = [False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'C': 1e-05},\n",
       "  {'C': 0.001},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1.0},\n",
       "  {'C': 10.0},\n",
       "  {'C': 100.0}),\n",
       " 'rank_test_score': array([6, 4, 1, 2, 3, 5]),\n",
       " 'split0_test_score': array([ 0.58028698,  0.75025624,  0.7799795 ,  0.7726341 ,  0.75247694,\n",
       "         0.74086095]),\n",
       " 'split0_train_score': array([ 0.57923964,  0.76860316,  0.87560871,  0.94434003,  0.9819308 ,\n",
       "         0.99470312]),\n",
       " 'split1_test_score': array([ 0.5786776 ,  0.74628396,  0.77669571,  0.76627371,  0.74867589,\n",
       "         0.73176149]),\n",
       " 'split1_train_score': array([ 0.57917218,  0.7684849 ,  0.87945837,  0.94822946,  0.98504976,\n",
       "         0.99538678]),\n",
       " 'split2_test_score': array([ 0.57816504,  0.75533914,  0.78472578,  0.76832394,  0.74799248,\n",
       "         0.7356911 ]),\n",
       " 'split2_train_score': array([ 0.57977019,  0.76613558,  0.87689548,  0.94566657,  0.98368288,\n",
       "         0.99397719]),\n",
       " 'split3_test_score': array([ 0.57894737,  0.75051265,  0.78332194,  0.77682843,  0.75768968,\n",
       "         0.73855092]),\n",
       " 'split3_train_score': array([ 0.57914745,  0.76678626,  0.87634546,  0.94558346,  0.98385443,\n",
       "         0.99474628]),\n",
       " 'split4_test_score': array([ 0.57877649,  0.75666439,  0.78947368,  0.78503076,  0.76896787,\n",
       "         0.75      ]),\n",
       " 'split4_train_score': array([ 0.57876303,  0.7665727 ,  0.87655903,  0.94763369,  0.98334188,\n",
       "         0.99325132]),\n",
       " 'std_fit_time': array([ 0.03874582,  0.02297261,  1.18862097,  1.83901079,  4.21516797,\n",
       "         2.93444269]),\n",
       " 'std_score_time': array([ 0.00160112,  0.00605009,  0.00623053,  0.00698687,  0.00713112,\n",
       "         0.00570195]),\n",
       " 'std_test_score': array([ 0.00070799,  0.00375907,  0.00432957,  0.00668246,  0.00771557,\n",
       "         0.00612049]),\n",
       " 'std_train_score': array([ 0.00032232,  0.00102466,  0.00131222,  0.00143229,  0.00100223,\n",
       "         0.00073252])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results_file = open('tfidf_gridcv_results.pkl', 'wb')\n",
    "pickle.dump(bow_search, results_file, -1)\n",
    "pickle.dump(tfidf_search, results_file, -1)\n",
    "pickle.dump(l2_search, results_file, -1)\n",
    "results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lemon\\Anaconda3\\envs\\mfe\\lib\\site-packages\\sklearn\\base.py:312: UserWarning: Trying to unpickle estimator LogisticRegression from version 0.18.2 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Users\\Lemon\\Anaconda3\\envs\\mfe\\lib\\site-packages\\sklearn\\base.py:312: UserWarning: Trying to unpickle estimator GridSearchCV from version 0.18.2 when using version 0.19.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "pkl_file = open('tfidf_gridcv_results.pkl', 'rb')\n",
    "bow_search = pickle.load(pkl_file)\n",
    "tfidf_search = pickle.load(pkl_file)\n",
    "l2_search = pickle.load(pkl_file)\n",
    "pkl_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bow</th>\n",
       "      <th>l2</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.578971</td>\n",
       "      <td>0.575724</td>\n",
       "      <td>0.721638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.751811</td>\n",
       "      <td>0.575724</td>\n",
       "      <td>0.788648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.782839</td>\n",
       "      <td>0.589120</td>\n",
       "      <td>0.763566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.773818</td>\n",
       "      <td>0.734247</td>\n",
       "      <td>0.741150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.755160</td>\n",
       "      <td>0.776756</td>\n",
       "      <td>0.721467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.739373</td>\n",
       "      <td>0.761106</td>\n",
       "      <td>0.712309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        bow        l2     tfidf\n",
       "0  0.578971  0.575724  0.721638\n",
       "1  0.751811  0.575724  0.788648\n",
       "2  0.782839  0.589120  0.763566\n",
       "3  0.773818  0.734247  0.741150\n",
       "4  0.755160  0.776756  0.721467\n",
       "5  0.739373  0.761106  0.712309"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_results = pd.DataFrame.from_dict({'bow': bow_search.cv_results_['mean_test_score'],\n",
    "                               'tfidf': tfidf_search.cv_results_['mean_test_score'],\n",
    "                               'l2': l2_search.cv_results_['mean_test_score']})\n",
    "search_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lemon\\Anaconda3\\envs\\mfe\\lib\\site-packages\\seaborn\\categorical.py:462: FutureWarning: remove_na is deprecated and is a private function. Do not use.\n",
      "  box_data = remove_na(group_data)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD5CAYAAADbY2myAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1f8953b9390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = sns.boxplot(data=search_results, width=0.4)\n",
    "ax.set_ylabel('Accuracy', size=14)\n",
    "ax.tick_params(labelsize=14)\n",
    "plt.savefig('tfidf_gridcv_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score with bow features: 0.78360708021\n",
      "Test score with l2-normalized features: 0.780178599904\n",
      "Test score with tf-idf features: 0.788470738319\n"
     ]
    }
   ],
   "source": [
    "m1 = simple_logistic_classify(X_tr_bow, y_tr, X_te_bow, y_te, 'bow', \n",
    "                              _C=bow_search.best_params_['C'])\n",
    "m2 = simple_logistic_classify(X_tr_l2, y_tr, X_te_l2, y_te, 'l2-normalized', \n",
    "                              _C=l2_search.best_params_['C'])\n",
    "m3 = simple_logistic_classify(X_tr_tfidf, y_tr, X_te_tfidf, y_te, 'tf-idf', \n",
    "                              _C=tfidf_search.best_params_['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.57897075,  0.7518111 ,  0.78283898,  0.77381766,  0.75515992,\n",
       "        0.73937261])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_search.cv_results_['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# blobs in TextBlob also have noun phrase extraction\n",
    "\n",
    "print([np for np in blob_df[4].noun_phrases])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
