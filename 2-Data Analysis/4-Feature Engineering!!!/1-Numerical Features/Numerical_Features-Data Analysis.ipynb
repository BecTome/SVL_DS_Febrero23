{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNRSx2dfA5bO"
   },
   "source": [
    "## Feature Scaling: Z-Score Standardization and Min-Max Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0ORIQYJA5bP"
   },
   "source": [
    "- [About standardization](#About-standardization)\n",
    "- [About Min-Max scaling / \"normalization\"](#About-Min-Max-scaling-normalization)\n",
    "- [Standardization or Min-Max scaling?](#Standardization-or-Min-Max-scaling?)\n",
    "- [Standardizing and normalizing - how it can be done using scikit-learn](#Standardizing-and-normalizing---how-it-can-be-done-using-scikit-learn)\n",
    "- [Bottom-up approaches](#Bottom-up-approaches)\n",
    "- [The effect of standardization on PCA in a pattern classification task](#The-effect-of-standardization-on-PCA-in-a-pattern-classification-task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ti17xhGNA5bP"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8eIevUaA5bQ"
   },
   "source": [
    "### About standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iuLMzyIA5bQ"
   },
   "source": [
    "The result of **standardization** (or **Z-score normalization**) is that the features will be rescaled so that they'll have the properties of a standard normal distribution with   \n",
    "\n",
    "$\\mu = 0$ and $\\sigma = 1$\n",
    "\n",
    "where $\\mu$ is the mean (average) and $\\sigma$ is the standard deviation from the mean; standard scores (also called ***z*** scores) of the samples are calculated as follows:\n",
    "\n",
    "\\begin{equation} z = \\frac{x - \\mu}{\\sigma}\\end{equation} \n",
    "\n",
    "Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. Intuitively, we can think of gradient descent as a prominent example\n",
    "(an optimization algorithm often used in logistic regression, SVMs, perceptrons, neural networks etc.); with features being on different scales, certain weights may update faster than others since the feature values $x_j$ play a role in the weight updates\n",
    "\n",
    "$$\\Delta w_j = - \\eta \\frac{\\partial J}{\\partial w_j} = \\eta \\sum_i (t^{(i)} - o^{(i)})x^{(i)}_{j},$$\n",
    "\n",
    "so that \n",
    "\n",
    "$$w_j := w_j + \\Delta w_j,$$\n",
    "where $\\eta$ is the learning rate, $t$ the target class label, and $o$ the actual output.\n",
    "Other intuitive examples include K-Nearest Neighbor algorithms and clustering algorithms that use, for example, Euclidean distance measures -- in fact, tree-based classifier are probably the only classifiers where feature scaling doesn't make a difference.\n",
    "\n",
    "\n",
    "\n",
    "To quote from the [`scikit-learn`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) documentation:\n",
    "\n",
    "*\"Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual feature do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFSDfRzDA5bR"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j5A0INeBA5bR"
   },
   "source": [
    "<a id='About-Min-Max-scaling-normalization'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nI1hkCE_A5bS"
   },
   "source": [
    "### About Min-Max scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b9Jf3rSTA5bS"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVm48vnkA5bT"
   },
   "source": [
    "An alternative approach to Z-score normalization (or standardization) is the so-called **Min-Max scaling** (often also simply called \"normalization\" - a common cause for ambiguities).  \n",
    "In this approach, the data is scaled to a fixed range - usually 0 to 1.  \n",
    "The cost of having this bounded range - in contrast to standardization - is that we will end up with smaller standard deviations, which can suppress the effect of outliers.\n",
    "\n",
    "A Min-Max scaling is typically done via the following equation:\n",
    "\n",
    "\\begin{equation} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QCXeBnBA5bT"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jxrqn_WEA5bU"
   },
   "source": [
    "### Z-score standardization or Min-Max scaling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfQWCindA5bU"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pcYyZgW_A5bV"
   },
   "source": [
    "\"Standardization or Min-Max scaling?\" - There is no obvious answer to this question: it really depends on the application. \n",
    "\n",
    "For example, in clustering analyses, standardization may be especially crucial in order to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling, since we are interested in the components that maximize the variance (depending on the question and if the PCA computes the components via the correlation matrix instead of the covariance matrix; [but more about PCA in my previous article](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)).\n",
    "\n",
    "However, this doesn't mean that Min-Max scaling is not useful at all! A popular application is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Also, typical neural network algorithm require data that on a 0-1 scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzrirBRmA5bV"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX1ITX9yA5bW"
   },
   "source": [
    "## Standardizing and normalizing - how it can be done using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQPknixmA5bW"
   },
   "source": [
    "[[back to top](#Sections)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1B5R04aA5bW"
   },
   "source": [
    "Of course, we could make use of NumPy's vectorization capabilities to calculate the z-scores for standardization and to normalize the data using the equations that were mentioned in the previous sections. However, there is an even more convenient approach using the preprocessing module from one of Python's open-source machine learning library [scikit-learn](http://scikit-learn.org )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWEN_OL5A5bX"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOakFslHA5bX"
   },
   "source": [
    "For the following examples and discussion, we will have a look at the free \"Wine\" Dataset that is deposited on the UCI machine learning repository  \n",
    "(http://archive.ics.uci.edu/ml/datasets/Wine).\n",
    "\n",
    "<br>\n",
    "\n",
    "<font size=\"1\">\n",
    "**Reference:**  \n",
    "Forina, M. et al, PARVUS - An Extendible Package for Data\n",
    "Exploration, Classification and Correlation. Institute of Pharmaceutical\n",
    "and Food Analysis and Technologies, Via Brigata Salerno, \n",
    "16147 Genoa, Italy.\n",
    "\n",
    "Bache, K. & Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91cm_5heA5bY"
   },
   "source": [
    "The Wine dataset consists of 3 different classes where each row correspond to a particular wine sample.\n",
    "\n",
    "The class labels (1, 2, 3) are listed in the first column, and the columns 2-14 correspond to 13 different attributes (features):\n",
    "\n",
    "1) Alcohol  \n",
    "2) Malic acid  \n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oF1VqrLA5bY"
   },
   "source": [
    "#### Loading the wine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "S6edsgZMA5bZ",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ead5438b-7990-4a8f-9ac2-5e1c89c3fbfb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.io.parsers.read_csv(\n",
    "    'https://raw.githubusercontent.com/rasbt/pattern_classification/master/data/wine_data.csv', \n",
    "     header=None,\n",
    "     usecols=[0,1,2]\n",
    "    )\n",
    "\n",
    "df.columns=['Class label', 'Alcohol', 'Malic acid']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SHQKwf7A5bc"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxm6pRQBA5bd"
   },
   "source": [
    "As we can see in the table above, the features **Alcohol** (percent/volumne) and **Malic acid** (g/l) are measured on different scales, so that ***Feature Scaling*** is necessary important prior to any comparison or combination of these data.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KwkhPa5A5bd"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvTYGjOrA5be"
   },
   "source": [
    "#### Standardization and Min-Max scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "fVbIgf2PA5be",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "v93uyjO_A5bh",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "6287a137-d1a3-4c99-d61d-fb5fb5cfb4d1"
   },
   "outputs": [],
   "source": [
    "print('Mean after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format(df_std[:,0].mean(), df_std[:,1].mean()))\n",
    "print('\\nStandard deviation after standardization:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format(df_std[:,0].std(), df_std[:,1].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "6ytjA_lHA5bj",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "fc867db1-5119-49e3-e5b7-130f5376445b"
   },
   "outputs": [],
   "source": [
    "print('Min-value after min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format(df_minmax[:,0].min(), df_minmax[:,1].min()))\n",
    "print('\\nMax-value after min-max scaling:\\nAlcohol={:.2f}, Malic acid={:.2f}'\n",
    "      .format(df_minmax[:,0].max(), df_minmax[:,1].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFkDQES_A5bl"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oFYVfTZNA5bm"
   },
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "77K__G6WA5bm",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "Qfm1-VqJA5br",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "9271d145-5550-43ef-fab2-b2c9180ab8cd"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot():\n",
    "    plt.figure(figsize=(8,6))\n",
    "\n",
    "    plt.scatter(df['Alcohol'], df['Malic acid'], \n",
    "            color='green', label='input scale', alpha=0.5)\n",
    "\n",
    "    plt.scatter(df_std[:,0], df_std[:,1], color='red', \n",
    "            label='Standardized [$N  (\\mu=0, \\; \\sigma=1)$]', alpha=0.3)\n",
    "\n",
    "    plt.scatter(df_minmax[:,0], df_minmax[:,1], \n",
    "            color='blue', label='min-max scaled [min=0, max=1]', alpha=0.3)\n",
    "\n",
    "    plt.title('Alcohol and Malic Acid content of the wine dataset')\n",
    "    plt.xlabel('Alcohol')\n",
    "    plt.ylabel('Malic Acid')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oG7IESkRA5bu"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EbYAU0ZoA5bu"
   },
   "source": [
    "The plot above includes the wine datapoints on all three different scales: the input scale where the alcohol content was measured in volume-percent (green), the standardized features (red), and the normalized features (blue).\n",
    "In the following plot, we will zoom in into the three different axis-scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2tqyHUpA5bv"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "aSSt2gUEA5bv",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "13379129-5f89-43fc-ebd1-301006a22971"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, figsize=(6,14))\n",
    "\n",
    "for a,d,l in zip(range(len(ax)), \n",
    "               (df[['Alcohol', 'Malic acid']].values, df_std, df_minmax),\n",
    "               ('Input scale', \n",
    "                'Standardized [$N  (\\mu=0, \\; \\sigma=1)$]', \n",
    "                'min-max scaled [min=0, max=1]')\n",
    "                ):\n",
    "    for i,c in zip(range(1,4), ('red', 'blue', 'green')):\n",
    "        ax[a].scatter(d[df['Class label'].values == i, 0], \n",
    "                  d[df['Class label'].values == i, 1],\n",
    "                  alpha=0.5,\n",
    "                  color=c,\n",
    "                  label='Class %s' %i\n",
    "                  )\n",
    "    ax[a].set_title(l)\n",
    "    ax[a].set_xlabel('Alcohol')\n",
    "    ax[a].set_ylabel('Malic Acid')\n",
    "    ax[a].legend(loc='upper left')\n",
    "    ax[a].grid()\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "igHy0_aCA5bx"
   },
   "source": [
    "### Comparing features with different scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_madrid = stats.gamma.rvs(1, size=5000)*100000\n",
    "\n",
    "plt.hist(price_madrid, 70, histtype=\"stepfilled\", alpha=.7);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precios de casas en diferentes monedas\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "price_madrid = stats.gamma.rvs(1, size=5000)*100000\n",
    "price_london = stats.gamma(5).rvs(5000)*100000*0.87\n",
    "price_spetesbourg = stats.gamma(5).rvs(5000)*100000*90.23\n",
    "\n",
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Madrid': price_madrid,\n",
    "                  'London': price_london,\n",
    "                  'Saint Petersburg': price_spetesbourg})\n",
    "\n",
    "#### CODE\n",
    "\n",
    "plt.hist(df_minmax[:, 0], 70, histtype=\"stepfilled\", alpha=.7, label='Madrid')\n",
    "plt.hist(df_minmax[:, 1], 70, histtype=\"stepfilled\", alpha=.7, label='London')\n",
    "plt.hist(df_minmax[:, 2], 70, histtype=\"stepfilled\", alpha=.7, label='Saint Petersburg')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIY4clVRA5bx"
   },
   "source": [
    "## Bottom-up approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqiB5GbhA5by"
   },
   "source": [
    "Of course, we can also code the equations for standardization and 0-1 Min-Max scaling \"manually\". However, the scikit-learn methods are still useful if you are working with test and training data sets and want to scale them equally.\n",
    "\n",
    "E.g., \n",
    "<pre>\n",
    "std_scale = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = std_scale.transform(X_train)\n",
    "X_test = std_scale.transform(X_test)\n",
    "</pre>\n",
    "\n",
    "Below, we will perform the calculations using \"pure\" Python code, and an more convenient NumPy solution, which is especially useful if we attempt to transform a whole matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lq-5HSSFA5by"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WS7Y2PoPA5by"
   },
   "source": [
    "Just to recall the equations that we are using:\n",
    "\n",
    "Standardization: \\begin{equation} z = \\frac{x - \\mu}{\\sigma} \\end{equation} \n",
    "\n",
    "with mean:  \n",
    "\n",
    "\\begin{equation}\\mu = \\frac{1}{N} \\sum_{i=1}^N (x_i)\\end{equation}\n",
    "\n",
    "and standard deviation:  \n",
    "\n",
    "\\begin{equation}\\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2}\\end{equation}\n",
    "\n",
    "\n",
    "Min-Max scaling: \\begin{equation} X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} \\end{equation}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8np6BxSWA5bz"
   },
   "source": [
    "### Pure Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "5XpINxXEA5bz",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Standardization\n",
    "\n",
    "x = [1,4,5,6,6,2,3]\n",
    "mean = sum(x)/len(x)\n",
    "std_dev = (1/len(x) * sum([ (x_i - mean)**2 for x_i in x]))**0.5\n",
    "\n",
    "z_scores = [(x_i - mean)/std_dev for x_i in x]\n",
    "\n",
    "# Min-Max scaling\n",
    "\n",
    "minmax = [(x_i - min(x)) / (max(x) - min(x)) for x_i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2O6zScLA5b1"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bbwi029aA5b1"
   },
   "source": [
    "### NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "jtpk8DKAA5b2",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Standardization\n",
    "\n",
    "x_np = np.asarray(x)\n",
    "z_scores_np = (x_np - x_np.mean()) / x_np.std()\n",
    "\n",
    "# Min-Max scaling\n",
    "\n",
    "np_minmax = (x_np - x_np.min()) / (x_np.max() - x_np.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS5sTXR9A5b4"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDx1mgDqA5b4"
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5-Nh9EKA5b4"
   },
   "source": [
    "Just to make sure that our code works correctly, let us plot the results via matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "b4j8TR7uA5b4",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "6IruhpK7A5b6",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "d443f3f0-bea5-46ab-b44d-35debc0a5624"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,5))\n",
    "\n",
    "y_pos = [0 for i in range(len(x))]\n",
    "\n",
    "ax1.scatter(z_scores, y_pos, color='g')\n",
    "ax1.set_title('Python standardization', color='g')\n",
    "\n",
    "ax2.scatter(minmax, y_pos, color='g')\n",
    "ax2.set_title('Python Min-Max scaling', color='g')\n",
    "\n",
    "ax3.scatter(z_scores_np, y_pos, color='b')\n",
    "ax3.set_title('Python NumPy standardization', color='b')\n",
    "\n",
    "ax4.scatter(np_minmax, y_pos, color='b')\n",
    "ax4.set_title('Python NumPy Min-Max scaling', color='b')\n",
    "    \n",
    "plt.tight_layout()\n",
    "\n",
    "for ax in (ax1, ax2, ax3, ax4):\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    ax.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTNz1_IkA5b8"
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWjJU2MuA5ci"
   },
   "source": [
    "# Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 360
    },
    "id": "MylAxHQjA5ci",
    "outputId": "134cc5c4-9962-4879-ae97-9114ad3069b4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "fcc_survey_df = pd.read_csv('fcc_2016_coder_survey_subset.csv', encoding='utf-8')\n",
    "fcc_survey_df[['ID.x', 'EmploymentField', 'Age', 'Income']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSTrEM6AA5cj"
   },
   "source": [
    "## Fixed-width binning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB85yDDkA5ck"
   },
   "source": [
    "### Developer age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "1GDAI6NIA5ck",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "5e603cf7-fea4-4078-f046-f4ea26e6976e"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fcc_survey_df['Age'].hist(color='#A9C5D3')\n",
    "ax.set_title('Developer Age Histogram', fontsize=12)\n",
    "ax.set_xlabel('Age', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Bi6STxGA5cl"
   },
   "source": [
    "### Binning based on rounding\n",
    "\n",
    "``` \n",
    "Age Range: Bin\n",
    "---------------\n",
    " 0 -  9  : 0\n",
    "10 - 19  : 1\n",
    "20 - 29  : 2\n",
    "30 - 39  : 3\n",
    "40 - 49  : 4\n",
    "50 - 59  : 5\n",
    "60 - 69  : 6\n",
    "  ... and so on\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "yZEV-G1pA5cm",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "f9fca4c7-ce4a-4041-eeae-4c3d8b4c2782"
   },
   "outputs": [],
   "source": [
    "fcc_survey_df['Age_bin_round'] = np.array(np.floor(np.array(fcc_survey_df['Age']) / 10.))\n",
    "fcc_survey_df[['ID.x', 'Age', 'Age_bin_round']].iloc[1071:1076]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1PL6KvuyA5cn"
   },
   "source": [
    "### Binning based on custom ranges\n",
    "\n",
    "``` \n",
    "Age Range : Bin\n",
    "---------------\n",
    " 0 -  15  : 1\n",
    "16 -  30  : 2\n",
    "31 -  45  : 3\n",
    "46 -  60  : 4\n",
    "61 -  75  : 5\n",
    "75 - 100  : 6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "7cfltQpAA5cn",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "b08a956d-a679-43c4-c278-37aa88806c88"
   },
   "outputs": [],
   "source": [
    "bin_ranges = [0, 15, 30, 45, 60, 75, 100]\n",
    "bin_names = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "#### CODE\n",
    "\n",
    "fcc_survey_df[['ID.x', 'Age', 'Age_bin_round', \n",
    "               'Age_bin_custom_range', 'Age_bin_custom_label']].iloc[1071:1076]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BshqgrYVA5cp"
   },
   "source": [
    "## Quantile based binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "sz1qddTZA5cp",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "ae6721cf-edfb-4b17-d7b5-ebb4a236f494"
   },
   "outputs": [],
   "source": [
    "fcc_survey_df[['ID.x', 'Age', 'Income']].iloc[4:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "2xbYwRQ1A5cq",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "7c1db47b-6e49-417f-aab1-cea497a95c6f"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3')\n",
    "ax.set_title('Developer Income Histogram', fontsize=12)\n",
    "ax.set_xlabel('Developer Income', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "bKxprAEQA5cs",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "2b4e492b-b2d3-4de1-84c4-cf2cd5e840fd"
   },
   "outputs": [],
   "source": [
    "#### CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "bNJrNhoKA5ct",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "9c56378e-e5c3-466d-a9e5-87a7350a8deb"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fcc_survey_df['Income'].hist(bins=30, color='#A9C5D3')\n",
    "\n",
    "for quantile in quantiles:\n",
    "    qvl = plt.axvline(quantile, color='r')\n",
    "ax.legend([qvl], ['Quantiles'], fontsize=10)\n",
    "\n",
    "ax.set_title('Developer Income Histogram with Quantiles', fontsize=12)\n",
    "ax.set_xlabel('Developer Income', fontsize=12)\n",
    "ax.set_ylabel('Frequency', fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "id": "MlsDrooFA5cv",
    "jupyter": {
     "outputs_hidden": false
    },
    "outputId": "95de9631-3523-40f4-aef2-e5e8b5bbd460"
   },
   "outputs": [],
   "source": [
    "quantile_labels = ['0-25Q', '25-50Q', '50-75Q', '75-100Q']\n",
    "fcc_survey_df['Income_quantile_range'] = pd.qcut(fcc_survey_df['Income'], \n",
    "                                                 q=quantile_list)\n",
    "fcc_survey_df['Income_quantile_label'] = pd.qcut(fcc_survey_df['Income'], \n",
    "                                                 q=quantile_list, labels=quantile_labels)\n",
    "fcc_survey_df[['ID.x', 'Age', 'Income', \n",
    "               'Income_quantile_range', 'Income_quantile_label']].iloc[4:9]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "1-Numerical_Features.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
